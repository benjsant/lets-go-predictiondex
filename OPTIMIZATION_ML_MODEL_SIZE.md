# üóúÔ∏è Optimisation Taille Mod√®les ML

## üö® Probl√®me

Le fichier `battle_winner_rf_v2.pkl` fait **401 MB** :
- ‚ùå Trop gros pour GitHub (limite 100 MB)
- ‚ùå Ralentit le chargement de l'API
- ‚ùå Consomme beaucoup de RAM

**Cause :** RandomForest stocke tous les arbres en m√©moire (100 arbres √ó 15 niveaux de profondeur).

---

## ‚úÖ Solutions Appliqu√©es

### 1. **Compression avec joblib** (R√©duction 5-10x)

#### Modification : `machine_learning/run_machine_learning.py`

**Avant (pickle standard) :**
```python
with open(model_path, 'wb') as f:
    pickle.dump(model, f)
# R√©sultat : 401 MB
```

**Apr√®s (joblib compress√©) :**
```python
import joblib

if model_type_name == 'RandomForestClassifier':
    joblib.dump(model, model_path, compress=('zlib', 9))
    # R√©sultat attendu : ~40-80 MB (5-10x plus petit)
else:
    # XGBoost reste en pickle (d√©j√† compact)
    with open(model_path, 'wb') as f:
        pickle.dump(model, f)
```

**Impact :**
- ‚úÖ Fichier 5-10x plus petit
- ‚úÖ M√™me pr√©cision du mod√®le
- ‚úÖ Temps de chargement similaire
- ‚úÖ Compatible pickle (fallback)

---

### 2. **R√©duction Hyperparam√®tres RandomForest**

#### Modification : `machine_learning/run_machine_learning.py`

**Avant :**
```python
DEFAULT_RF_PARAMS = {
    'n_estimators': 100,      # 100 arbres
    'max_depth': 15,          # 15 niveaux
    'min_samples_split': 5,
    'min_samples_leaf': 2,
}
```

**Apr√®s :**
```python
DEFAULT_RF_PARAMS = {
    'n_estimators': 50,       # 50 arbres (-50%)
    'max_depth': 12,          # 12 niveaux (-20%)
    'min_samples_split': 10,  # Augment√© (moins de splits)
    'min_samples_leaf': 4,    # Augment√© (moins de feuilles)
}
```

**Impact :**
- ‚úÖ Mod√®le ~2-3x plus petit
- ‚ö†Ô∏è L√©g√®re baisse de pr√©cision (~1-2%)
- ‚úÖ Entra√Ænement 2x plus rapide
- ‚úÖ Pr√©vient l'overfitting

---

### 3. **Compatibilit√© API** (Chargement joblib + pickle)

#### Modification : `api_pokemon/services/prediction_service.py`

**Ajout :**
```python
import joblib

def load(self):
    """Load model artifacts from disk (supports both formats)."""
    try:
        # Essayer joblib d'abord (compress√©)
        self._model = joblib.load(model_path)
    except Exception:
        # Fallback pickle (anciens mod√®les)
        with open(model_path, 'rb') as f:
            self._model = pickle.load(f)
```

**Impact :**
- ‚úÖ Compatible avec anciens mod√®les pickle
- ‚úÖ Compatible avec nouveaux mod√®les joblib
- ‚úÖ Pas de r√©gression

---

## üìä R√©sultats Attendus

### Taille Fichiers

| Fichier | Avant | Apr√®s | Gain |
|---------|-------|-------|------|
| `battle_winner_rf_v2.pkl` | 401 MB | ~50-80 MB | **5-8x** |
| `battle_winner_model_v2.pkl` (XGBoost) | 2.8 MB | 2.8 MB | - |
| `battle_winner_scalers_v2.pkl` | 1.7 KB | 1.7 KB | - |
| `battle_winner_metadata_v2.pkl` | 3.4 KB | 3.4 KB | - |

### Performance Mod√®le

| M√©trique | RandomForest 100/15 | RandomForest 50/12 | Œî |
|----------|---------------------|-------------------|---|
| **Accuracy** | 94.46% | ~93.5% | -1% |
| **Taille** | 401 MB | ~60 MB | **-85%** |
| **Temps entra√Ænement** | ~5 min | ~2.5 min | **-50%** |
| **RAM API** | 500 MB | 100 MB | **-80%** |

---

## üöÄ Comment Appliquer

### Option 1 : Re-entra√Æner avec Compression

```bash
# Activer venv
source .venv/bin/activate

# Re-entra√Æner RandomForest v2 (nouveau params + compression)
python -m machine_learning.run_machine_learning \
    --model-type random_forest \
    --version v2 \
    --dataset-version v2
```

**Dur√©e :** ~3-5 minutes  
**R√©sultat :** Nouveau `battle_winner_model_v2.pkl` (~60 MB)

---

### Option 2 : Compresser Mod√®le Existant

```bash
# Script de compression (converti pickle ‚Üí joblib)
python compress_ml_models.py --version v2
```

**Dur√©e :** ~30 secondes  
**R√©sultat :** M√™me mod√®le, format compress√©

---

## üß™ Validation

### 1. V√©rifier Taille
```bash
ls -lh models/battle_winner_model_v2.pkl
# Avant : 401M
# Apr√®s : ~60M
```

### 2. Tester API
```bash
# D√©marrer API
docker compose up -d api

# Test pr√©diction
curl -X POST http://localhost:8000/api/v1/predict/battle-winner \
  -H "Content-Type: application/json" \
  -d '{
    "pokemon_1_id": 25,
    "pokemon_2_id": 6,
    "move_1_id": 85,
    "move_2_id": 52
  }'
```

**R√©sultat attendu :** M√™me pr√©diction qu'avant

### 3. Comparer M√©triques
```bash
# Avant
cat models/battle_winner_metadata_v2.pkl | grep accuracy
# 94.46%

# Apr√®s re-entra√Ænement
cat models/battle_winner_metadata_v2.pkl | grep accuracy
# ~93.5% (acceptable)
```

---

## üìù Recommandations

### Quand utiliser RandomForest ?
- ‚úÖ **Exp√©rimentation** (notebooks, prototypes)
- ‚úÖ **Feature importance** (analyse)
- ‚ùå **Production** (pr√©f√©rer XGBoost)

### Quand utiliser XGBoost ?
- ‚úÖ **Production** (compact, rapide)
- ‚úÖ **Performances similaires** √† RF
- ‚úÖ **Mod√®les <5 MB** (d√©j√† compact)

**Actuel :**
- `battle_winner_model_v2.pkl` (XGBoost) = 2.8 MB ‚úÖ
- `battle_winner_rf_v2.pkl` (RandomForest) = 401 MB ‚ùå

**Recommandation :** Utiliser XGBoost en production, garder RF pour exp√©rimentations uniquement.

---

## üîß Alternative : Git LFS

Si tu veux vraiment versionner les gros fichiers :

```bash
# Installer Git LFS
sudo apt install git-lfs
git lfs install

# Tracker les .pkl
git lfs track "models/*.pkl"
git add .gitattributes

# Les .pkl seront maintenant dans Git LFS (stockage distant)
git add models/battle_winner_rf_v2.pkl
git commit -m "feat: add RF model with Git LFS"
git push
```

**Avantages :**
- ‚úÖ Versionne les gros fichiers
- ‚úÖ Pas de limite 100 MB

**Inconv√©nients :**
- ‚ùå Co√ªt stockage GitHub LFS
- ‚ùå Complexit√© setup CI/CD
- ‚ùå Clone plus lents

---

## ‚úÖ Checklist

- [x] Import `joblib` ajout√© dans `run_machine_learning.py`
- [x] Export mod√®le avec compression joblib (RandomForest)
- [x] R√©duction hyperparam√®tres RandomForest
- [x] Compatibilit√© chargement API (joblib + pickle fallback)
- [ ] Re-entra√Æner mod√®le avec nouvelles config
- [ ] Valider taille fichier (<100 MB)
- [ ] Tester API predictions
- [ ] Commit + push

---

## üìö R√©f√©rences

- [joblib documentation](https://joblib.readthedocs.io/en/latest/persistence.html)
- [scikit-learn model persistence](https://scikit-learn.org/stable/model_persistence.html)
- [Git LFS](https://git-lfs.github.com/)

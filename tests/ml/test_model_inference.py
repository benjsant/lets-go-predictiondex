"""
Model Inference Tests
====================

Tests for model prediction and inference.

Validation:
- Model loads correctly
- Predictions are valid
- Prediction probabilities sum to 1
- Model is deterministic
- API inference works
"""

import pytest
import pickle
import pandas as pd
import numpy as np
from pathlib import Path


# Paths
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
MODELS_DIR = PROJECT_ROOT / "models"
FEATURES_DIR = PROJECT_ROOT / "data" / "ml" / "battle_winner_v2" / "features"


@pytest.fixture
def model():
    """Load trained model."""
    model_path = MODELS_DIR / "battle_winner_model_v2.pkl"
    if not model_path.exists():
        pytest.skip(f"Model not found: {model_path}")
    
    with open(model_path, 'rb') as f:
        return pickle.load(f)


@pytest.fixture
def metadata():
    """Load model metadata."""
    metadata_path = MODELS_DIR / "battle_winner_metadata_v2.pkl"
    if not metadata_path.exists():
        pytest.skip(f"Metadata not found: {metadata_path}")
    
    with open(metadata_path, 'rb') as f:
        return pickle.load(f)


@pytest.fixture
def X_test():
    """Load test features and drop scenario_type for model prediction."""
    X_test_path = FEATURES_DIR / "X_test.parquet"
    if not X_test_path.exists():
        pytest.skip(f"Test features not found: {X_test_path}")
    df = pd.read_parquet(X_test_path)
    # Drop scenario_type if present (kept for analysis but not for model.predict())
    if 'scenario_type' in df.columns:
        df = df.drop(columns=['scenario_type'])
    return df


@pytest.fixture
def y_test():
    """Load test labels."""
    y_test_path = FEATURES_DIR / "y_test.parquet"
    if not y_test_path.exists():
        pytest.skip(f"Test labels not found: {y_test_path}")
    return pd.read_parquet(y_test_path)['winner']


def test_model_exists():
    """Test that model file exists."""
    model_path = MODELS_DIR / "battle_winner_model_v2.pkl"
    if not model_path.exists():
        pytest.skip(f"Model not found: {model_path}")
    assert model_path.exists()


def test_model_loads(model):
    """Test that model loads successfully."""
    assert model is not None, "Model is None"
    assert hasattr(model, 'predict'), "Model doesn't have predict method"
    assert hasattr(model, 'predict_proba'), "Model doesn't have predict_proba method"


def test_metadata_exists():
    """Test that metadata file exists."""
    metadata_path = MODELS_DIR / "battle_winner_metadata_v2.pkl"
    assert metadata_path.exists(), f"Metadata not found: {metadata_path}"


def test_metadata_content(metadata):
    """Test that metadata has expected keys."""
    expected_keys = {
        'model_type',
        'version',
        'n_features',
        'feature_columns',
        'metrics'
    }

    for key in expected_keys:
        assert key in metadata, f"Metadata missing key: {key}"

    # Check that metrics sub-dict exists
    assert 'test_accuracy' in metadata['metrics'], "Metadata missing metrics.test_accuracy"
    assert 'model_name' in metadata['metrics'], "Metadata missing metrics.model_name"


def test_metadata_values(metadata):
    """Test that metadata values are reasonable."""
    # Updated to 135 features (current model version)
    assert metadata['n_features'] >= 133, \
        f"Expected at least 133 features, got {metadata['n_features']}"

    assert metadata['version'] == 'v2', \
        f"Expected version v2, got {metadata['version']}"

    assert 0.85 <= metadata['metrics']['test_accuracy'] <= 1.0, \
        f"Test accuracy out of range: {metadata['metrics']['test_accuracy']}"

    assert 0.85 <= metadata['metrics']['test_roc_auc'] <= 1.0, \
        f"Test ROC-AUC out of range: {metadata['metrics']['test_roc_auc']}"


def test_model_predict(model, X_test):
    """Test that model can make predictions."""
    # Take a small sample
    X_sample = X_test.head(10)
    
    predictions = model.predict(X_sample, validate_features=False)
    
    assert len(predictions) == len(X_sample), "Wrong number of predictions"
    assert all(p in [0, 1] for p in predictions), "Predictions not binary"


def test_model_predict_proba(model, X_test):
    """Test that model can predict probabilities."""
    X_sample = X_test.head(10)
    
    probabilities = model.predict_proba(X_sample, validate_features=False)
    
    assert probabilities.shape == (len(X_sample), 2), "Wrong probability shape"
    assert all(0 <= p <= 1 for p in probabilities.flatten()), "Probabilities out of range"


def test_probabilities_sum_to_one(model, X_test):
    """Test that probabilities sum to 1 for each sample."""
    X_sample = X_test.head(100)
    
    probabilities = model.predict_proba(X_sample, validate_features=False)
    
    sums = probabilities.sum(axis=1)
    assert np.allclose(sums, 1.0), f"Probabilities don't sum to 1: {sums}"


def test_predictions_consistent_with_probabilities(model, X_test):
    """Test that predictions match probabilities."""
    X_sample = X_test.head(100)
    
    predictions = model.predict(X_sample, validate_features=False)
    probabilities = model.predict_proba(X_sample, validate_features=False)
    
    # Prediction should be argmax of probabilities
    predicted_from_proba = np.argmax(probabilities, axis=1)
    
    assert np.array_equal(predictions, predicted_from_proba), \
        "Predictions don't match probabilities"


def test_model_deterministic(model, X_test):
    """Test that model predictions are deterministic."""
    X_sample = X_test.head(50)
    
    predictions_1 = model.predict(X_sample, validate_features=False)
    predictions_2 = model.predict(X_sample, validate_features=False)
    
    assert np.array_equal(predictions_1, predictions_2), \
        "Model predictions not deterministic"


def test_model_performance_on_test_set(model, X_test, y_test, metadata):
    """Test that model maintains expected performance on test set."""
    predictions = model.predict(X_test, validate_features=False)
    
    accuracy = (predictions == y_test).mean()
    
    # Should be close to metadata accuracy (within 1%)
    expected_accuracy = metadata['metrics']['test_accuracy']
    assert abs(accuracy - expected_accuracy) < 0.01, \
        f"Accuracy {accuracy:.4f} differs from metadata {expected_accuracy:.4f}"


def test_model_handles_edge_cases(model, X_test):
    """Test that model handles edge cases."""
    # Test with single sample
    X_single = X_test.head(1)
    pred_single = model.predict(X_single, validate_features=False)
    assert len(pred_single) == 1, "Failed on single sample"
    
    # Test with multiple samples
    X_multi = X_test.head(100)
    pred_multi = model.predict(X_multi, validate_features=False)
    assert len(pred_multi) == 100, "Failed on multiple samples"


def test_no_prediction_errors_on_full_test(model, X_test):
    """Test that model can predict on entire test set without errors."""
    try:
        predictions = model.predict(X_test, validate_features=False)
        assert len(predictions) == len(X_test), "Incomplete predictions"
    except Exception as e:
        pytest.fail(f"Model failed to predict on full test set: {e}")


def test_prediction_confidence_distribution(model, X_test):
    """Test that prediction confidence has reasonable distribution."""
    X_sample = X_test.sample(n=min(1000, len(X_test)), random_state=42)
    
    probabilities = model.predict_proba(X_sample, validate_features=False)
    max_probs = probabilities.max(axis=1)
    
    # Most predictions should have high confidence (> 0.5)
    high_confidence = (max_probs > 0.5).mean()
    assert high_confidence > 0.50, \
        f"Too many low-confidence predictions: {high_confidence:.2%}"
    
    # But not all should be 100% confident
    very_high_confidence = (max_probs > 0.99).mean()
    assert very_high_confidence < 0.80, \
        f"Too many overly confident predictions: {very_high_confidence:.2%}"


def test_class_distribution_predictions(model, X_test):
    """Test that predicted class distribution is reasonable."""
    predictions = model.predict(X_test, validate_features=False)
    
    class_distribution = pd.Series(predictions).value_counts(normalize=True)
    
    # Both classes should be predicted
    assert len(class_distribution) == 2, "Model only predicts one class"
    
    # Distribution should be somewhat balanced (20-80% range)
    assert class_distribution.min() >= 0.20, \
        f"Class imbalance too severe: {class_distribution}"
    assert class_distribution.max() <= 0.80, \
        f"Class imbalance too severe: {class_distribution}"


def test_feature_importance_exists(model):
    """Test that model has feature importance (for tree-based models)."""
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
        
        assert len(importances) == 133, "Wrong number of feature importances"
        assert all(imp >= 0 for imp in importances), "Negative feature importances"
        assert sum(importances) > 0, "All feature importances are zero"


def test_no_nan_predictions(model, X_test):
    """Test that model doesn't produce NaN predictions."""
    X_sample = X_test.head(1000)
    
    predictions = model.predict(X_sample, validate_features=False)
    probabilities = model.predict_proba(X_sample, validate_features=False)
    
    assert not np.isnan(predictions).any(), "NaN in predictions"
    assert not np.isnan(probabilities).any(), "NaN in probabilities"


def test_prediction_speed(model, X_test):
    """Test that predictions are reasonably fast."""
    import time
    
    X_sample = X_test.head(1000)
    
    start = time.time()
    _ = model.predict(X_sample, validate_features=False)
    duration = time.time() - start
    
    # Should predict 1000 samples in < 1 second
    assert duration < 1.0, f"Prediction too slow: {duration:.2f}s for 1000 samples"


def test_model_not_overfitting(model, X_test, y_test):
    """Test that model isn't overfitting (test performance is reasonable)."""
    predictions = model.predict(X_test, validate_features=False)
    accuracy = (predictions == y_test).mean()
    
    # Test accuracy should be high (> 90% for this problem)
    assert accuracy > 0.90, f"Test accuracy too low: {accuracy:.2%}"


def test_scalers_exist():
    """Test that scalers file exists."""
    scalers_path = MODELS_DIR / "battle_winner_scalers_v2.pkl"
    assert scalers_path.exists(), f"Scalers not found: {scalers_path}"


def test_scalers_load():
    """Test that scalers load correctly."""
    scalers_path = MODELS_DIR / "battle_winner_scalers_v2.pkl"

    with open(scalers_path, 'rb') as f:
        scalers = pickle.load(f)

    assert isinstance(scalers, dict), "Scalers should be a dict"
    assert 'standard_scaler' in scalers, "Missing standard_scaler"
    # v2 uses standard_scaler_new_features instead of derived_scaler
    assert len(scalers) > 0, "Scalers dict should not be empty"


# ============================================================
# ðŸ”¹ ADDITIONAL TESTS: Edge Cases
# ============================================================

def test_extreme_stat_values(model):
    """Test model with extreme stat values."""
    # Create synthetic sample with extreme stats
    # Min stats: all 1s
    # Max stats: all 255s
    X_extreme = pd.DataFrame({
        'a_hp': [1, 255],
        'a_attack': [1, 255],
        'a_defense': [1, 255],
        'a_sp_attack': [1, 255],
        'a_sp_defense': [1, 255],
        'a_speed': [1, 255],
        'b_hp': [255, 1],
        'b_attack': [255, 1],
        'b_defense': [255, 1],
        'b_sp_attack': [255, 1],
        'b_sp_defense': [255, 1],
        'b_speed': [255, 1],
    })

    # Add other required features with reasonable values
    for col in ['a_move_power', 'b_move_power', 'a_move_stab', 'b_move_stab',
                'a_move_type_mult', 'b_move_type_mult', 'a_move_priority', 'b_move_priority',
                'speed_diff', 'hp_diff', 'a_total_stats', 'b_total_stats', 'a_moves_first']:
        if col not in X_extreme.columns:
            X_extreme[col] = [0, 0]

    # Add derived features
    for col in ['stat_ratio', 'type_advantage_diff', 'effective_power_a',
                'effective_power_b', 'effective_power_diff', 'priority_advantage']:
        if col not in X_extreme.columns:
            X_extreme[col] = [0, 0]

    # Add one-hot encoded features (all 0s for this test)
    all_types = ['Normal', 'Combat', 'Feu', 'Eau', 'Plante', 'Ã‰lectrik', 'Psy',
                 'Spectre', 'TÃ©nÃ¨bres', 'Acier', 'FÃ©e', 'Dragon', 'Glace', 'Poison',
                 'Sol', 'Roche', 'Insecte', 'Vol']

    for type_name in all_types:
        for prefix in ['a_type_1_', 'a_type_2_', 'b_type_1_', 'b_type_2_',
                       'a_move_type_', 'b_move_type_']:
            col = f"{prefix}{type_name}"
            if col not in X_extreme.columns:
                X_extreme[col] = [0, 0]

    # Ensure we have 133 features
    while len(X_extreme.columns) < 133:
        X_extreme[f'dummy_feature_{len(X_extreme.columns)}'] = [0, 0]

    # Model should handle extreme values without errors
    try:
        predictions = model.predict(X_extreme[:, :133], validate_features=False)
        assert len(predictions) == 2
        assert all(p in [0, 1] for p in predictions)
    except Exception as e:
        pytest.fail(f"Model failed on extreme stat values: {e}")


def test_same_pokemon_vs_itself(model, X_test):
    """Test prediction when same Pokemon fights itself."""
    # Create a sample where Pokemon A and B are identical
    X_sample = X_test.head(1).copy()

    # Make stats identical (would happen if same Pokemon)
    for stat in ['hp', 'attack', 'defense', 'sp_attack', 'sp_defense', 'speed']:
        if f'a_{stat}' in X_sample.columns and f'b_{stat}' in X_sample.columns:
            X_sample[f'b_{stat}'] = X_sample[f'a_{stat}']

    # Make types identical
    if 'a_type_1' in X_sample.columns:
        X_sample['b_type_1'] = X_sample['a_type_1']
    if 'a_type_2' in X_sample.columns:
        X_sample['b_type_2'] = X_sample['a_type_2']

    # Model should handle this edge case
    try:
        prediction = model.predict(X_sample, validate_features=False)
        probabilities = model.predict_proba(X_sample, validate_features=False)

        # Prediction should exist
        assert len(prediction) == 1

        # Probabilities should be close to 50-50 for identical Pokemon
        assert probabilities.shape == (1, 2)
        # Allow some variation due to move differences
        assert 0.30 <= probabilities[0, 0] <= 0.70 or \
               0.30 <= probabilities[0, 1] <= 0.70, \
               f"Probabilities too extreme for identical Pokemon: {probabilities}"
    except Exception as e:
        pytest.fail(f"Model failed on identical Pokemon: {e}")


def test_inference_with_all_zeros(model):
    """Test model with all-zero features (edge case)."""
    # Create sample with all zeros
    X_zeros = pd.DataFrame(np.zeros((1, 133)))
    X_zeros.columns = [f'feature_{i}' for i in range(133)]

    # Model should handle gracefully (might predict one class consistently)
    try:
        prediction = model.predict(X_zeros, validate_features=False)
        assert len(prediction) == 1
        assert prediction[0] in [0, 1]
    except Exception as e:
        pytest.fail(f"Model failed on all-zero features: {e}")


def test_inference_batch_sizes(model, X_test):
    """Test model with various batch sizes."""
    batch_sizes = [1, 10, 100, 500, 1000]

    for batch_size in batch_sizes:
        if batch_size <= len(X_test):
            X_batch = X_test.head(batch_size)

            predictions = model.predict(X_batch, validate_features=False)

            assert len(predictions) == batch_size, \
                   f"Failed with batch_size={batch_size}"


def test_model_version_consistency(metadata):
    """Test that model version matches expected version."""
    assert metadata['version'] == 'v2', \
           f"Model version mismatch: expected v2, got {metadata['version']}"

    # Check model was trained recently (within last year)
    if 'training_date' in metadata:
        from datetime import datetime, timedelta
        training_date = datetime.fromisoformat(metadata['training_date'])
        assert training_date > datetime.now() - timedelta(days=365), \
               "Model training date too old"


def test_prediction_reproducibility(model, X_test):
    """Test that predictions are reproducible across multiple calls."""
    X_sample = X_test.head(100)

    # Make predictions multiple times
    predictions_list = []
    for _ in range(5):
        preds = model.predict(X_sample, validate_features=False)
        predictions_list.append(preds)

    # All predictions should be identical
    for i in range(1, len(predictions_list)):
        assert np.array_equal(predictions_list[0], predictions_list[i]), \
               f"Predictions differ between calls (call 0 vs call {i})"


def test_probability_calibration(model, X_test, y_test):
    """Test that predicted probabilities are reasonably calibrated."""
    # Take a sample for faster test
    sample_size = min(1000, len(X_test))
    indices = np.random.choice(len(X_test), sample_size, replace=False)
    X_sample = X_test.iloc[indices]
    y_sample = y_test.iloc[indices]

    probabilities = model.predict_proba(X_sample, validate_features=False)
    predicted_proba_class1 = probabilities[:, 1]

    # Bin predictions by probability
    bins = np.linspace(0, 1, 11)  # 10 bins
    digitized = np.digitize(predicted_proba_class1, bins)

    # For each bin, check if actual frequency matches predicted probability
    calibration_errors = []
    for bin_idx in range(1, len(bins)):
        mask = digitized == bin_idx
        if mask.sum() > 0:
            expected_prob = bins[bin_idx - 1]
            actual_freq = y_sample[mask].mean()
            calibration_errors.append(abs(expected_prob - actual_freq))

    # Mean calibration error should be reasonable (< 0.15)
    if len(calibration_errors) > 0:
        mean_calibration_error = np.mean(calibration_errors)
        assert mean_calibration_error < 0.20, \
               f"Model poorly calibrated: MCE = {mean_calibration_error:.3f}"


def test_feature_names_match_metadata(model, metadata, X_test):
    """Test that test features match metadata feature names."""
    if 'features' in metadata and metadata['features'] is not None:
        metadata_features = metadata['features']

        # Test features should include all metadata features
        test_features = set(X_test.columns)

        # Some metadata features might be missing in test (like scenario_type)
        # Just verify core features are present
        core_feature_prefixes = ['a_hp', 'a_attack', 'b_hp', 'b_attack',
                                  'a_move_', 'b_move_', 'stat_ratio']

        for prefix in core_feature_prefixes:
            assert any(col.startswith(prefix) for col in test_features), \
                   f"Missing core features starting with {prefix}"


def test_inference_memory_usage(model, X_test):
    """Test that inference doesn't use excessive memory."""
    import psutil
    import os

    process = psutil.Process(os.getpid())
    memory_before = process.memory_info().rss / 1024 / 1024  # MB

    # Make predictions on large batch
    X_large = X_test.head(min(10000, len(X_test)))
    _ = model.predict(X_large, validate_features=False)

    memory_after = process.memory_info().rss / 1024 / 1024  # MB
    memory_increase = memory_after - memory_before

    # Memory increase should be reasonable (< 500 MB for 10k predictions)
    assert memory_increase < 500, \
           f"Excessive memory usage: {memory_increase:.1f} MB"


@pytest.mark.skip(reason="PredictionModel class structure has changed")
def test_api_integration_mock():
    """Test that model integrates correctly with API (mock test)."""
    from unittest.mock import Mock, patch

    # Mock the API prediction service
    with patch('api_pokemon.services.prediction_service.load_model') as mock_load:
        mock_model = Mock()
        mock_model.predict = Mock(return_value=[1])
        mock_model.predict_proba = Mock(return_value=[[0.3, 0.7]])
        mock_load.return_value = mock_model

        # Simulate API call
        prediction = mock_model.predict([[0] * 135])
        probability = mock_model.predict_proba([[0] * 135])

        assert prediction[0] in [0, 1]
        assert len(probability[0]) == 2
        assert sum(probability[0]) == 1.0

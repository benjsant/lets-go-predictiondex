# Documentation D√©taill√©e du Mod√®le ML - Battle Winner Predictor

> **Mod√®le XGBoost pour la Pr√©diction de Victoire dans les Combats Pok√©mon Let's Go**
>
> **Accuracy: 94.24%** | **ROC-AUC: 98.96%** | **Version: v1** | **Date: 2026-01-21**

---

## üìã Table des Mati√®res

1. [Vue d'Ensemble](#vue-densemble)
2. [Probl√®me ML & M√©thodologie](#probl√®me-ml--m√©thodologie)
3. [Dataset de Combats](#dataset-de-combats)
4. [Features Engineering](#features-engineering)
5. [S√©lection du Mod√®le](#s√©lection-du-mod√®le)
6. [Hyperparam√®tres & Training](#hyperparam√®tres--training)
7. [√âvaluation & M√©triques](#√©valuation--m√©triques)
8. [Feature Importance](#feature-importance)
9. [Inference en Production](#inference-en-production)
10. [Limites & Am√©liorations](#limites--am√©liorations)
11. [Reproductibilit√©](#reproductibilit√©)

---

## üéØ Vue d'Ensemble

### Probl√®me Business

**Question:** "Quelle capacit√© mon Pok√©mon doit-il utiliser pour maximiser ses chances de gagner contre un adversaire?"

**Cas d'Usage:** Aider un enfant jouant √† Pok√©mon Let's Go √† choisir la meilleure capacit√© lors d'un combat.

### Solution ML

**Type de probl√®me:** Classification binaire supervis√©e

**Variable cible:**
```python
winner = 1 if pokemon_a_wins else 0
```

**Input:**
- Pok√©mon A (stats, types, capacit√© choisie)
- Pok√©mon B (stats, types, capacit√© choisie)

**Output:**
- Pr√©diction: A gagne (1) ou B gagne (0)
- Probabilit√© de victoire pour A: P(winner=1)

---

## üß† Probl√®me ML & M√©thodologie

### Formulation du Probl√®me

Soit un combat entre deux Pok√©mon:
- **Pok√©mon A** utilisant la capacit√© `move_A`
- **Pok√©mon B** utilisant la capacit√© `move_B`

Nous voulons pr√©dire:
```
P(A wins | stats_A, types_A, move_A, stats_B, types_B, move_B)
```

### Hypoth√®ses du Mod√®le

1. **Combat 1v1**: Un seul tour d√©cisif
2. **Pas d'al√©atoire**: Coups critiques et miss exclus (pour d√©terminisme)
3. **Pas de contexte**: M√©t√©o, statuts, stat changes ignor√©s
4. **Capacit√©s exclus**: Certaines capacit√©s sp√©ciales exclues:
   - Bluff (d√©pend du premier tour)
   - Croc Fatal (KO instantan√©)
   - Balayage (d√©pend du poids)
   - Moves r√©actifs (Riposte, Voile Miroir, Protection)

### M√©triques de Succ√®s

- **Accuracy ‚â• 90%**: Pr√©dire correctement le gagnant dans 90% des cas
- **ROC-AUC ‚â• 95%**: Bonne discrimination entre victoire et d√©faite
- **Overfitting < 5%**: Gap train-test minimal
- **Latence < 500ms**: Pr√©diction en temps r√©el

---

## üìä Dataset de Combats

### G√©n√©ration du Dataset

**Script:** `machine_learning/build_battle_winner_dataset.py`

#### √âtape 1: G√©n√©ration des Matchups

```python
# Tous les matchups possibles
all_pokemons = 188 (Let's Go dex)
possible_matchups = 188 √ó 188 = 35,344
```

#### √âtape 2: S√©lection des Capacit√©s

Pour chaque matchup (Pokemon A vs Pokemon B):

1. **Capacit√© de A:** S√©lectionne la meilleure capacit√© offensive de A contre les types de B
   ```python
   score_A = power √ó stab √ó type_mult √ó (accuracy/100) + priority √ó 50
   ```

2. **Capacit√© de B:** S√©lectionne la meilleure capacit√© offensive de B contre les types de A
   ```python
   score_B = power √ó stab √ó type_mult √ó (accuracy/100) + priority √ó 50
   ```

**Composants du Score:**
- `power`: Puissance de base de la capacit√©
- `stab`: Same Type Attack Bonus (1.5 si type capacit√© = type Pok√©mon, sinon 1.0)
- `type_mult`: Efficacit√© du type (0.25, 0.5, 1.0, 2.0, ou 4.0)
- `accuracy`: Pr√©cision de la capacit√© (0-100)
- `priority`: Priorit√© de la capacit√© (-5 √† +2)

#### √âtape 3: Calcul du Damage

**Formule simplifi√©e (bas√©e sur Pok√©mon Let's Go):**

```python
# Damage de base
base_damage = ((2 * level / 5 + 2) * power * attack / defense / 50 + 2)

# Multiplicateurs
damage = base_damage √ó stab √ó type_effectiveness √ó random(0.85, 1.0)

# Dans notre cas: random = 1.0 (d√©terministe)
```

**Simplifications:**
- Level = 50 (standard competitive)
- Attack = `attack` stat si move physique, `sp_attack` si move sp√©cial
- Defense = `defense` stat si move physique, `sp_defense` si move sp√©cial

#### √âtape 4: D√©termination du Gagnant

```python
# Qui attaque en premier?
if move_A.priority > move_B.priority:
    first_attacker = A
elif move_A.priority < move_B.priority:
    first_attacker = B
else:  # M√™me priorit√©
    if pokemon_A.speed > pokemon_B.speed:
        first_attacker = A
    elif pokemon_A.speed < pokemon_B.speed:
        first_attacker = B
    else:
        first_attacker = random.choice([A, B])

# Combat
if first_attacker == A:
    damage_to_B = calculate_damage(A, move_A, B)
    if damage_to_B >= pokemon_B.hp:
        winner = A  # KO en premier
    else:
        damage_to_A = calculate_damage(B, move_B, A)
        winner = A if damage_to_A < damage_to_B else B
else:
    # Inverse...
```

### Dataset Final

**Fichiers Parquet:**
- `data/ml/battle_winner/raw/matchups.parquet` - 34,040 samples
- `data/ml/battle_winner/processed/train.parquet` - 27,232 samples (80%)
- `data/ml/battle_winner/processed/test.parquet` - 6,808 samples (20%)

**Split:** 80% train / 20% test (stratified par winner)

**Balance:**
```
Class distribution:
‚îú‚îÄ A wins: 50.04% (17,026 samples)
‚îî‚îÄ B wins: 49.96% (17,014 samples)

Presque parfaitement balanc√©!
```

**Colonnes (38 features brutes):**
```
a_hp, a_attack, a_defense, a_sp_attack, a_sp_defense, a_speed
b_hp, b_attack, b_defense, b_sp_attack, b_sp_defense, b_speed
a_type_1, a_type_2
b_type_1, b_type_2
a_move_power, a_move_type, a_move_priority, a_move_stab, a_move_type_mult
b_move_power, b_move_type, b_move_priority, b_move_stab, b_move_type_mult
speed_diff, hp_diff
a_total_stats, b_total_stats
a_moves_first
winner (target)
```

---

## üîß Features Engineering

### Pipeline de Transformation

**Notebook:** `notebooks/02_feature_engineering.ipynb`
**Script Production:** `machine_learning/train_model.py` (fonction `engineer_features()`)

#### √âtape 1: One-Hot Encoding (38 ‚Üí 107 colonnes)

**Features cat√©gorielles:**
- `a_type_1`, `a_type_2` (18 types possibles + "none")
- `b_type_1`, `b_type_2`
- `a_move_type`, `b_move_type`

**Exemple:**
```python
# Avant
a_type_1 = "Eau"

# Apr√®s one-hot
a_type_1_Eau = 1
a_type_1_Feu = 0
a_type_1_Plante = 0
... (17 autres colonnes)
```

**R√©sultat:** ~102 colonnes binaires cr√©√©es (6 features √ó 17 types moyens)

#### √âtape 2: Normalisation (StandardScaler #1)

**Features √† normaliser (18 colonnes):**
```python
features_to_scale = [
    'a_hp', 'a_attack', 'a_defense', 'a_sp_attack', 'a_sp_defense', 'a_speed',
    'b_hp', 'b_attack', 'b_defense', 'b_sp_attack', 'b_sp_defense', 'b_speed',
    'a_move_power', 'b_move_power',
    'a_total_stats', 'b_total_stats',
    'speed_diff', 'hp_diff'
]
```

**StandardScaler:**
```python
scaler = StandardScaler()
X_train[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])
X_test[features_to_scale] = scaler.transform(X_test[features_to_scale])

# Formule: z = (x - Œº) / œÉ
```

**Raison:** Mettre toutes les features num√©riques sur la m√™me √©chelle (moyenne=0, √©cart-type=1)

#### √âtape 3: Cr√©ation de Features D√©riv√©es (+6 colonnes)

**IMPORTANT:** Les features d√©riv√©es sont cr√©√©es √† partir des **valeurs originales** (non normalis√©es) pour pr√©server les relations.

```python
# 1. Ratio des stats totales (qui est globalement plus fort?)
stat_ratio = a_total_stats / (b_total_stats + 1)

# 2. Diff√©rence d'avantage de type
type_advantage_diff = a_move_type_mult - b_move_type_mult

# 3. Puissance effective de A (power √ó stab √ó type_mult)
effective_power_a = a_move_power √ó a_move_stab √ó a_move_type_mult

# 4. Puissance effective de B
effective_power_b = b_move_power √ó b_move_stab √ó b_move_type_mult

# 5. Diff√©rence de puissance effective
effective_power_diff = effective_power_a - effective_power_b

# 6. Avantage de priorit√© (qui attaque en premier?)
priority_advantage = a_move_priority - b_move_priority
```

**Intuition:**
- `stat_ratio > 1`: A plus fort statistiquement
- `type_advantage_diff > 0`: A a avantage de type
- `effective_power_diff > 0`: A frappe plus fort
- `priority_advantage > 0`: A attaque en premier

#### √âtape 4: Normalisation (StandardScaler #2)

```python
scaler_new = StandardScaler()
X_train[new_features] = scaler_new.fit_transform(X_train[new_features])
X_test[new_features] = scaler_new.transform(X_test[new_features])
```

**Raison:** Les features d√©riv√©es ont des √©chelles diff√©rentes ‚Üí normalisation s√©par√©e

### Features Finales: 133 Colonnes

**Composition:**
- **107 colonnes** one-hot encod√©es (6 features cat√©gorielles)
- **20 colonnes** num√©riques normalis√©es (scaler #1)
- **6 colonnes** d√©riv√©es normalis√©es (scaler #2)

**Total:** 133 features

### Export des Scalers

Les deux scalers sont sauvegard√©s pour inference:
```python
scalers = {
    'standard_scaler': scaler,          # Pour stats/powers
    'standard_scaler_new_features': scaler_new  # Pour features d√©riv√©es
}

with open('models/battle_winner_scalers_v1.pkl', 'wb') as f:
    pickle.dump(scalers, f)
```

---

## üèÜ S√©lection du Mod√®le

### Mod√®les Test√©s

**Notebook:** `notebooks/03_training_evaluation.ipynb`

#### 1. Logistic Regression (Baseline)

**Hyperparam√®tres:**
```python
LogisticRegression(
    max_iter=1000,
    random_state=42
)
```

**R√©sultats:**
- Test Accuracy: **90.88%**
- Test ROC-AUC: **97.13%**
- Training Time: ~2s

**Analyse:** Bon mod√®le de base, mais relations non-lin√©aires manqu√©es.

#### 2. Random Forest

**Hyperparam√®tres:**
```python
RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)
```

**R√©sultats:**
- Test Accuracy: **93.48%**
- Test ROC-AUC: **98.59%**
- Training Time: ~15s

**Analyse:** Excellente feature importance, mais l√©ger overfitting (gap train-test = 5%).

#### 3. XGBoost (Choisi) ‚úÖ

**Hyperparam√®tres:**
```python
XGBClassifier(
    n_estimators=100,
    max_depth=8,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1,
    eval_metric='logloss'
)
```

**R√©sultats:**
- Test Accuracy: **94.24%** ‚úÖ
- Test ROC-AUC: **98.96%** ‚úÖ
- Training Time: ~5s
- Overfitting: 4.63% (minimal)

**Pourquoi XGBoost?**
- ‚úÖ Meilleure accuracy (94.24% vs 93.48% vs 90.88%)
- ‚úÖ Meilleur ROC-AUC (98.96%)
- ‚úÖ Moins d'overfitting que Random Forest
- ‚úÖ Plus rapide que Random Forest
- ‚úÖ Robuste aux outliers
- ‚úÖ G√®re bien les features cat√©gorielles encod√©es

### Comparaison Finale

| M√©trique | Logistic Regression | Random Forest | **XGBoost** |
|----------|-------------------|---------------|-------------|
| Test Accuracy | 90.88% | 93.48% | **94.24%** |
| Test Precision | 90.83% | 93.46% | **94.22%** |
| Test Recall | 90.93% | 93.51% | **94.26%** |
| Test F1 | 90.88% | 93.48% | **94.24%** |
| Test ROC-AUC | 97.13% | 98.59% | **98.96%** |
| Train Accuracy | 91.12% | 98.52% | **98.87%** |
| Overfitting | 0.24% | 5.04% | **4.63%** |
| Training Time | 2s | 15s | **5s** |
| Model Size | 2 KB | 28 MB | **983 KB** |

**Gagnant:** XGBoost (meilleur compromis accuracy/overfitting/vitesse)

---

## ‚öôÔ∏è Hyperparam√®tres & Training

### Hyperparam√®tres XGBoost

```python
XGBOOST_PARAMS = {
    'n_estimators': 100,      # Nombre d'arbres
    'max_depth': 8,           # Profondeur maximale (contr√¥le overfitting)
    'learning_rate': 0.1,     # Taux d'apprentissage (eta)
    'subsample': 0.8,         # Proportion de samples par arbre (80%)
    'colsample_bytree': 0.8,  # Proportion de features par arbre (80%)
    'random_state': 42,       # Seed pour reproductibilit√©
    'n_jobs': -1,             # Utilise tous les CPU
    'eval_metric': 'logloss'  # M√©trique d'√©valuation
}
```

**Choix des Hyperparam√®tres:**

| Param√®tre | Valeur | Justification |
|-----------|--------|---------------|
| `n_estimators` | 100 | Suffisant pour converger (tests avec 200/300 n'am√©liorent pas) |
| `max_depth` | 8 | √âquilibre: assez profond pour capturer interactions, pas trop pour √©viter overfitting |
| `learning_rate` | 0.1 | Standard, bon compromis vitesse/accuracy |
| `subsample` | 0.8 | R√©duit overfitting en cr√©ant diversit√© entre arbres |
| `colsample_bytree` | 0.8 | √âvite que certaines features dominent |

### Courbe d'Apprentissage

**Training:**
```python
model = xgb.XGBClassifier(**XGBOOST_PARAMS)
model.fit(X_train, y_train, verbose=False)
```

**√âvolution de l'Accuracy:**
```
Iteration   Train Accuracy   Test Accuracy
----------------------------------------------
10          85.23%          84.91%
20          90.12%          89.87%
30          93.45%          92.98%
50          96.78%          93.87%
75          98.12%          94.15%
100         98.87%          94.24%  ‚Üê Convergence
```

**Convergence:** Le mod√®le converge vers 100 arbres. Au-del√†, pas d'am√©lioration significative.

### Validation Crois√©e

**5-Fold Cross-Validation:**
```python
cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')

CV Scores: [93.89%, 94.12%, 93.76%, 94.31%, 93.95%]
Mean CV Accuracy: 94.01% ¬± 0.19%
```

**Analyse:** Variance faible (¬± 0.19%) ‚Üí Mod√®le stable et robuste.

---

## üìà √âvaluation & M√©triques

### M√©triques de Performance

#### Train Set (27,232 samples)

```
Accuracy:  98.87%
Precision: 98.89%
Recall:    98.85%
F1-Score:  98.87%
```

#### Test Set (6,808 samples)

```
Accuracy:  94.24%
Precision: 94.22%
Recall:    94.26%
F1-Score:  94.24%
ROC-AUC:   98.96%
```

**Gap Train-Test:** 4.63% (overfitting minimal, acceptable)

### Matrice de Confusion (Test Set)

```
                 Predicted
                 A wins    B wins
Actual  A wins    3215      193     (94.34% correct)
        B wins     199     3201     (94.14% correct)

Total Test Samples: 6,808
Correct Predictions: 6,416 (94.24%)
Incorrect Predictions: 392 (5.76%)
```

**Analyse:**
- **Faux Positifs:** 193 cas o√π le mod√®le pr√©dit victoire de A mais B gagne
- **Faux N√©gatifs:** 199 cas o√π le mod√®le pr√©dit victoire de B mais A gagne
- **Balance:** Erreurs presque √©gales (193 vs 199) ‚Üí pas de biais vers une classe

### Classification Report

```
              precision    recall  f1-score   support

      B wins     0.9418    0.9415    0.9416      3400
      A wins     0.9427    0.9430    0.9429      3408

    accuracy                         0.9424      6808
   macro avg     0.9422    0.9422    0.9422      6808
weighted avg     0.9422    0.9424    0.9424      6808
```

### Courbe ROC

**ROC-AUC: 98.96%**

```
           1.0 |                    ******
               |                ****
               |             ***
    True       |          ***
    Positive   |       ***
    Rate       |     ***
               |   ***
               | ***
           0.0 |***_________________
               0.0                1.0
                 False Positive Rate
```

**Interpr√©tation:** Le mod√®le discrimine tr√®s bien entre victoire et d√©faite (AUC proche de 1.0).

### Distribution des Probabilit√©s

**Histogramme des probabilit√©s pr√©dites:**

```
Classe A wins:
0.0-0.1: ‚ñà‚ñà
0.1-0.2: ‚ñà‚ñà‚ñà‚ñà
0.2-0.3: ‚ñà‚ñà‚ñà‚ñà
0.3-0.4: ‚ñà‚ñà‚ñà‚ñà
0.4-0.5: ‚ñà‚ñà‚ñà‚ñà
0.5-0.6: ‚ñà‚ñà‚ñà‚ñà
0.6-0.7: ‚ñà‚ñà‚ñà‚ñà
0.7-0.8: ‚ñà‚ñà‚ñà‚ñà
0.8-0.9: ‚ñà‚ñà‚ñà‚ñà
0.9-1.0: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  (Distribution concentr√©e aux extr√™mes)
```

**Observation:** Le mod√®le est **confiant** dans ses pr√©dictions (beaucoup de probabilit√©s proches de 0 ou 1).

### Calibration

**Brier Score:** 0.0523 (bon, < 0.10)

**Analyse:** Le mod√®le est bien calibr√© - quand il pr√©dit 90% de chances de victoire, A gagne effectivement ~90% du temps.

---

## üîç Feature Importance

### Top 15 Features (Random Forest)

**Note:** XGBoost et Random Forest donnent des importances similaires.

| Rank | Feature | Importance | Type |
|------|---------|-----------|------|
| 1 | `stat_ratio` | 15.0% | D√©riv√©e |
| 2 | `effective_power_diff` | 9.0% | D√©riv√©e |
| 3 | `hp_diff` | 8.8% | Brute |
| 4 | `a_total_stats` | 5.3% | Brute |
| 5 | `b_total_stats` | 4.6% | Brute |
| 6 | `a_hp` | 3.9% | Brute |
| 7 | `b_hp` | 3.7% | Brute |
| 8 | `effective_power_a` | 3.5% | D√©riv√©e |
| 9 | `effective_power_b` | 3.4% | D√©riv√©e |
| 10 | `priority_advantage` | 3.2% | D√©riv√©e |
| 11 | `type_advantage_diff` | 3.0% | D√©riv√©e |
| 12 | `a_attack` | 2.8% | Brute |
| 13 | `b_attack` | 2.7% | Brute |
| 14 | `speed_diff` | 2.5% | Brute |
| 15 | `a_speed` | 2.3% | Brute |

### Insights

**Features d√©riv√©es dominent le top 5:**
- `stat_ratio` (#1): Ratio des stats totales est la feature la plus importante
- `effective_power_diff` (#2): Diff√©rence de puissance effective tr√®s importante

**HP est critique:**
- `hp_diff` (#3), `a_hp` (#6), `b_hp` (#7): Les HP influencent grandement l'issue

**Features cat√©gorielles (types):**
- One-hot encoded types contribuent ~20% cumul√©s
- `a_move_type_Eau`, `b_type_1_Feu`, etc. apparaissent dans le top 30

**Priorit√© compte:**
- `priority_advantage` (#10): Qui attaque en premier est important

### Visualisation

```
Feature Importance (Top 10)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

stat_ratio             ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 15.0%
effective_power_diff   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 9.0%
hp_diff                ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 8.8%
a_total_stats          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 5.3%
b_total_stats          ‚ñà‚ñà‚ñà‚ñà‚ñà 4.6%
a_hp                   ‚ñà‚ñà‚ñà‚ñà 3.9%
b_hp                   ‚ñà‚ñà‚ñà‚ñà 3.7%
effective_power_a      ‚ñà‚ñà‚ñà‚ñà 3.5%
effective_power_b      ‚ñà‚ñà‚ñà‚ñà 3.4%
priority_advantage     ‚ñà‚ñà‚ñà 3.2%
```

---

## üöÄ Inference en Production

### Chargement du Mod√®le

**Fichier:** `api_pokemon/services/prediction_service.py`

```python
class PredictionModel:
    """Singleton pour charger le mod√®le une fois."""
    _instance = None

    def load(self):
        """Charge les artifacts ML."""
        # Mod√®le XGBoost
        with open(MODELS_DIR / "battle_winner_model_v1.pkl", 'rb') as f:
            self._model = pickle.load(f)

        # 2 StandardScalers
        with open(MODELS_DIR / "battle_winner_scalers_v1.pkl", 'rb') as f:
            self._scalers = pickle.load(f)

        # M√©tadonn√©es (feature_columns, metrics, etc.)
        with open(MODELS_DIR / "battle_winner_metadata.pkl", 'rb') as f:
            self._metadata = pickle.load(f)

# Singleton global
prediction_model = PredictionModel()
prediction_model.load()  # Charg√© une fois au d√©marrage de l'API
```

**Dur√©e de chargement:** ~100ms (une fois)

### Pipeline de Pr√©diction

**Fonction principale:** `predict_best_move(pokemon_a_id, pokemon_b_id, available_moves_a)`

```python
def predict_best_move(db, pokemon_a_id, pokemon_b_id, available_moves_a):
    """
    Pr√©dit la meilleure capacit√© pour Pokemon A contre Pokemon B.

    Returns:
        {
            'recommended_move': str,
            'win_probability': float,
            'all_moves': List[dict]  # Toutes les moves class√©es
        }
    """
    move_results = []

    for move_a_name in available_moves_a:
        # 1. S√©lectionner meilleure capacit√© pour B (contre-attaque)
        move_b_info = select_best_move_for_matchup(pokemon_b, pokemon_a, ...)

        # 2. Pr√©parer features (38 colonnes brutes)
        features_raw = prepare_features_for_prediction(
            pokemon_a, pokemon_b, move_a_info, move_b_info
        )

        # 3. Feature engineering (38 ‚Üí 133 colonnes)
        features_final = apply_feature_engineering(features_raw)

        # 4. Pr√©dire
        prediction = model.predict(features_final, validate_features=False)[0]
        probability = model.predict_proba(features_final, validate_features=False)[0]
        win_prob = probability[1]  # P(A wins)

        move_results.append({
            'move_name': move_a_name,
            'win_probability': float(win_prob),
            'predicted_winner': 'A' if prediction == 1 else 'B',
            ...
        })

    # 5. Classer par probabilit√© d√©croissante
    move_results.sort(key=lambda x: x['win_probability'], reverse=True)

    return {
        'recommended_move': move_results[0]['move_name'],
        'win_probability': move_results[0]['win_probability'],
        'all_moves': move_results
    }
```

### Pr√©paration des Features (Runtime)

```python
def prepare_features_for_prediction(pokemon_a, pokemon_b, move_a_info, move_b_info):
    """Construit les 38 features brutes."""
    features = {
        # Stats A
        'a_hp': pokemon_a.stats.hp,
        'a_attack': pokemon_a.stats.attack,
        'a_defense': pokemon_a.stats.defense,
        'a_sp_attack': pokemon_a.stats.sp_attack,
        'a_sp_defense': pokemon_a.stats.sp_defense,
        'a_speed': pokemon_a.stats.speed,

        # Stats B
        'b_hp': pokemon_b.stats.hp,
        ...

        # Types A
        'a_type_1': pokemon_a.pokemon_types[0].type.name,
        'a_type_2': pokemon_a.pokemon_types[1].type.name if len(...) > 1 else 'none',

        # Move A
        'a_move_power': move_a_info['move_power'],
        'a_move_type': move_a_info['move_type_name'],
        'a_move_priority': move_a_info['priority'],
        'a_move_stab': move_a_info['stab'],
        'a_move_type_mult': move_a_info['type_multiplier'],

        # Derived
        'speed_diff': pokemon_a.stats.speed - pokemon_b.stats.speed,
        'hp_diff': pokemon_a.stats.hp - pokemon_b.stats.hp,
        'a_total_stats': sum(pokemon_a.stats),
        'b_total_stats': sum(pokemon_b.stats),
        'a_moves_first': 1 if (priority_a > priority_b or
                              (priority_a == priority_b and speed_a > speed_b)) else 0
    }

    return pd.DataFrame([features])
```

### Feature Engineering (Runtime)

```python
def apply_feature_engineering(df_raw):
    """Applique le m√™me pipeline que le training."""
    # Charger scalers depuis le singleton
    scalers = prediction_model.scalers
    feature_columns = prediction_model.metadata['feature_columns']

    # √âtape 1: One-hot encode
    X_encoded = df_raw.copy()
    for feature in ['a_type_1', 'a_type_2', 'b_type_1', 'b_type_2',
                   'a_move_type', 'b_move_type']:
        dummies = pd.get_dummies(X_encoded[feature], prefix=feature)
        X_encoded = pd.concat([X_encoded, dummies], axis=1)
    X_encoded = X_encoded.drop(columns=categorical_features)

    # √âtape 2: Normaliser avec scaler #1
    scaler = scalers['standard_scaler']
    X_encoded[features_to_scale] = scaler.transform(X_encoded[features_to_scale])

    # √âtape 3: Cr√©er features d√©riv√©es (valeurs originales)
    X_encoded['stat_ratio'] = df_raw['a_total_stats'] / (df_raw['b_total_stats'] + 1)
    X_encoded['type_advantage_diff'] = df_raw['a_move_type_mult'] - df_raw['b_move_type_mult']
    X_encoded['effective_power_a'] = df_raw['a_move_power'] * df_raw['a_move_stab'] * df_raw['a_move_type_mult']
    X_encoded['effective_power_b'] = df_raw['b_move_power'] * df_raw['b_move_stab'] * df_raw['b_move_type_mult']
    X_encoded['effective_power_diff'] = X_encoded['effective_power_a'] - X_encoded['effective_power_b']
    X_encoded['priority_advantage'] = df_raw['a_move_priority'] - df_raw['b_move_priority']

    # √âtape 4: Normaliser features d√©riv√©es avec scaler #2
    scaler_new = scalers['standard_scaler_new_features']
    X_encoded[new_features] = scaler_new.transform(X_encoded[new_features])

    # √âtape 5: Ajouter colonnes manquantes (one-hot) avec 0
    for col in feature_columns:
        if col not in X_encoded.columns:
            X_encoded[col] = 0

    # √âtape 6: R√©ordonner pour matcher training
    X_encoded = X_encoded[feature_columns]

    return X_encoded
```

### Latence de Pr√©diction

**Benchmark (API running):**

| Op√©ration | Dur√©e | % Total |
|-----------|-------|---------|
| Chargement DB (Pok√©mon A) | 10ms | 12% |
| Chargement DB (Pok√©mon B) | 10ms | 12% |
| S√©lection best move B | 5ms | 6% |
| Pr√©paration features | 3ms | 4% |
| Feature engineering | 10ms | 12% |
| **Pr√©diction XGBoost** | **2ms** | **2%** |
| **Total par move** | **~40ms** | **~50ms avec overhead** |

**Pour 4 capacit√©s test√©es:** ~200ms total

**Goulot d'√©tranglement:** DB queries (60% du temps) ‚Üí Opportunit√© d'optimisation avec cache.

---

## ‚ö†Ô∏è Limites & Am√©liorations

### Limites Actuelles

#### 1. Simplifications du Combat

**Exclusions:**
- ‚ùå Coups critiques (chance 1/16)
- ‚ùå Miss (pr√©cision < 100%)
- ‚ùå Stat changes (Danse-Lames, Rugissement, etc.)
- ‚ùå Statuts (poison, paralysie, br√ªlure, etc.)
- ‚ùå M√©t√©o (soleil, pluie, temp√™te de sable, etc.)
- ‚ùå Objets tenus (Lunettes Choix, Ceinture Force, etc.)
- ‚ùå Abilities (Torrent, Brasier, Engrais, etc.)
- ‚ùå Combats multi-tours (HP restants apr√®s premier tour)

**Impact:** Le mod√®le pr√©dit le r√©sultat **d√©terministe** d'un combat simplifi√©.

#### 2. Capacit√©s Exclues

**Non support√©es:**
- Bluff (d√©pend du premier tour)
- Croc Fatal (KO instantan√©)
- Balayage (d√©pend du poids)
- Moves r√©actifs (Riposte, Voile Miroir, Protection, Abri)
- Moves multi-tours (Lance-Soleil, Ultralaser avec recharge)

**Impact:** ~15% des capacit√©s Let's Go non prises en compte.

#### 3. Scope du Dataset

- ‚úÖ 188 Pok√©mon Let's Go (complet)
- ‚ùå Pas de Mega-√âvolutions (stats diff√©rentes)
- ‚ùå Pas de formes Alola avec moveset diff√©rent

### Am√©liorations Futures

#### 1. Pr√©diction de D√©g√¢ts (Regression)

**Objectif:** Pr√©dire les d√©g√¢ts exacts au lieu de juste le gagnant.

**Target:** `damage_dealt` (0-100+)

**Mod√®le:** XGBoost Regressor

**B√©n√©fices:**
- Plus de granularit√© (savoir si c'est un KO de peu ou large)
- Peut aider √† la strat√©gie (jouer d√©fensif si close)

#### 2. Simulation Multi-Tours

**Objectif:** Simuler un combat complet avec plusieurs tours.

**Approche:**
- R√©seau de neurones r√©current (LSTM/GRU)
- Input: √âtat du combat √† chaque tour (HP restants, statuts, etc.)
- Output: Probabilit√© de victoire apr√®s N tours

**Complexit√©:** +++

#### 3. Support des Al√©as

**Objectif:** Mod√©liser les coups critiques et miss.

**Approche:**
- Monte Carlo: Simuler 1000 combats avec al√©a
- Pr√©dire probabilit√© moyenne de victoire

**Formule:**
```python
P(A wins) = (Nombre de victoires A sur 1000 simulations) / 1000
```

#### 4. Context Features

**Ajouter:**
- M√©t√©o active
- Terrain (Champ √âlectrifi√©, etc.)
- Statuts des Pok√©mon
- Objets tenus
- Abilities activ√©es

**Impact attendu:** +2-3% accuracy

#### 5. Model Drift Detection

**Probl√®me:** Si le meta-game change (nouveaux Pok√©mon, buffs/nerfs), le mod√®le peut d√©grader.

**Solution:**
- Monitorer les pr√©dictions en production (Evidently, WhyLabs)
- Alerter si distribution features change
- Re-entra√Æner p√©riodiquement

#### 6. Explainability

**Ajouter:**
- SHAP values pour expliquer chaque pr√©diction
- Feature contributions (pourquoi ce move est recommand√©?)

**UI:**
```
Hydrocanon recommand√© (99.75%)
‚îú‚îÄ Type advantage: +45% (Eau super efficace contre Feu)
‚îú‚îÄ STAB bonus: +20% (Carapuce est type Eau)
‚îú‚îÄ High power: +25% (110 power vs 40)
‚îî‚îÄ Speed advantage: +10% (Carapuce attaque en premier)
```

---

## üî¨ Reproductibilit√©

### Seeds Fix√©s

```python
RANDOM_SEED = 42

# NumPy
np.random.seed(RANDOM_SEED)

# scikit-learn
from sklearn.model_selection import train_test_split
train_test_split(..., random_state=RANDOM_SEED)

# XGBoost
XGBClassifier(random_state=RANDOM_SEED)
```

### Versions des Librairies

```
python==3.11
xgboost==3.1.3
scikit-learn==1.8.0
pandas==3.0.0
numpy==2.4.1
```

### Commandes pour Reproduire

```bash
# 1. Cloner le repo
git clone <repo_url>
cd lets-go-predictiondex

# 2. Setup environment
python -m venv .venv
source .venv/bin/activate
pip install -r machine_learning/requirements.txt

# 3. D√©marrer PostgreSQL
docker compose up -d db

# 4. Charger les donn√©es
POSTGRES_HOST=localhost python etl_pokemon/scripts/etl_load_csv.py

# 5. G√©n√©rer le dataset
POSTGRES_HOST=localhost python machine_learning/build_battle_winner_dataset.py

# 6. Entra√Æner le mod√®le
python machine_learning/train_model.py

# R√©sultat attendu: Test Accuracy = 94.24% (¬± 0.1% due to randomness)
```

### Checksums des Fichiers

```bash
# Dataset train.parquet
md5sum data/ml/battle_winner/processed/train.parquet
# Attendu: <md5_hash_train>

# Mod√®le
md5sum models/battle_winner_model_v1.pkl
# Attendu: <md5_hash_model>
```

---

## üìö R√©f√©rences

### Documentation Pok√©mon

- **Pok√©mon Damage Calculator:** https://calc.pokemonshowdown.com/
- **Type Effectiveness Chart:** https://pokemondb.net/type
- **Pok√©API:** https://pokeapi.co/docs/v2
- **Pok√©p√©dia (FR):** https://www.pokepedia.fr/

### Machine Learning

- **XGBoost Documentation:** https://xgboost.readthedocs.io/
- **scikit-learn StandardScaler:** https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html
- **Binary Classification Metrics:** https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics

### Notebooks

- `notebooks/01_exploration.ipynb` - Exploration du dataset
- `notebooks/02_feature_engineering.ipynb` - Pipeline de features
- `notebooks/03_training_evaluation.ipynb` - Training et √©valuation

---

**Document g√©n√©r√© le:** 2026-01-21
**Version du mod√®le:** battle_winner_v1
**Auteur:** Projet Let's Go PredictionDex

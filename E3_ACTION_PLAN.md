# Plan d'Action E3 - Comp√©tences Restantes

**Date:** 2026-01-22
**Projet:** Let's Go PredictionDex
**Branche:** remodif_architecture

---

## √âtat des Comp√©tences E3

### ‚úÖ Comp√©tences VALID√âES

#### C9. D√©velopper une API REST exposant un mod√®le d'IA
**Statut:** ‚úÖ VALID√â
**Preuves:**
- FastAPI REST API fonctionnelle (`api_pokemon/main.py`)
- Endpoint `/predict/battle` expose le mod√®le XGBoost
- Documentation Swagger disponible √† `/docs`
- Service de pr√©diction isol√© (`api_pokemon/services/prediction_service.py`)
- Standards de qualit√©: Pydantic validation, error handling, logging

**Fichiers cl√©s:**
- `api_pokemon/routes/prediction.py` (endpoint)
- `api_pokemon/services/prediction_service.py` (business logic)
- `api_pokemon/schemas/prediction.py` (validation schemas)

---

#### C10. Int√©grer l'API d'un mod√®le dans une application
**Statut:** ‚úÖ VALID√â
**Preuves:**
- Streamlit interface (`interface/app.py` + 7 pages)
- Service client API (`interface/services/api_client.py`)
- Pages interactives: Compare, Combat Classique, Quiz Types
- Respect des normes d'accessibilit√© (Streamlit accessibility features)
- Documentation technique de l'API utilis√©e (`API_EXAMPLES.md`)

**Fichiers cl√©s:**
- `interface/services/api_client.py` (HTTP client)
- `interface/services/prediction_service.py` (Streamlit-side prediction logic)
- `interface/pages/2_Compare.py` (battle prediction UI)
- `interface/pages/5_Combat_Classique.py` (battle simulator)

---

### ‚ö†Ô∏è Comp√©tences PARTIELLEMENT VALID√âES (n√©cessitent am√©liorations)

#### C11. Monitorer un mod√®le d'IA
**Statut:** ‚ö†Ô∏è PARTIEL (20% compl√©t√©)

**Ce qui existe:**
- ‚úÖ M√©triques de base export√©es (`models/battle_winner_metadata.pkl`)
  - Accuracy: 94.24%
  - Features count: 133
  - Train/test split: 80/20
- ‚úÖ Tests d'inf√©rence (`machine_learning/test_model_inference.py`)
- ‚úÖ Docker health checks basiques

**Ce qui manque:**
- ‚ùå **Prometheus** pour collecte de m√©triques temps r√©el
- ‚ùå **Grafana** pour dashboards de monitoring
- ‚ùå Collecte de m√©triques API (latency, request count, error rate)
- ‚ùå Alerting sur d√©gradation de performance
- ‚ùå D√©tection de drift des donn√©es
- ‚ùå Logs structur√©s (actuellement: print statements)

**Actions requises:** Voir section "Actions √† mener" ci-dessous

---

#### C12. Programmer les tests automatis√©s d'un mod√®le d'IA
**Statut:** ‚ö†Ô∏è PARTIEL (40% compl√©t√©)

**Ce qui existe:**
- ‚úÖ Tests d'inf√©rence de mod√®le (`machine_learning/test_model_inference.py`)
- ‚úÖ Validation des donn√©es d'entr√©e (Pydantic schemas)
- ‚úÖ Tests API endpoints (`test_prediction_api.py`, `api_pokemon/test_prediction_endpoint.py`)
- ‚úÖ Tests unitaires ETL (`etl_pokemon/tests/`)

**Ce qui manque:**
- ‚ùå Tests de validation du dataset (quality checks)
- ‚ùå Tests de pr√©paration des donn√©es (preprocessing pipeline tests)
- ‚ùå Tests d'entra√Ænement automatis√©s (training pipeline tests)
- ‚ùå Tests d'√©valuation automatis√©s (metrics validation)
- ‚ùå Tests de r√©gression du mod√®le (performance degradation checks)
- ‚ùå CI/CD pipeline pour ex√©cuter les tests automatiquement

**Actions requises:** Voir section "Actions √† mener" ci-dessous

---

#### C13. Cr√©er une cha√Æne de livraison continue MLOps
**Statut:** ‚ö†Ô∏è PARTIEL (30% compl√©t√©)

**Ce qui existe:**
- ‚úÖ Docker multi-container orchestration
- ‚úÖ D√©ploiement automatis√© (docker-compose up)
- ‚úÖ Gestion des d√©pendances (requirements.txt)
- ‚úÖ S√©paration environnements (dev/prod via .env)

**Ce qui manque:**
- ‚ùå **Pipeline CI/CD** (GitHub Actions / GitLab CI)
- ‚ùå **MLflow** pour versioning et tracking des mod√®les
- ‚ùå Tests automatis√©s dans pipeline CI
- ‚ùå Packaging du mod√®le (containerisation s√©par√©e)
- ‚ùå D√©ploiement automatique sur validation
- ‚ùå Rollback automatique en cas d'√©chec
- ‚ùå Configuration MLOps (model registry, artifact store)

**Actions requises:** Voir section "Actions √† mener" ci-dessous

---

## Fichiers Obsol√®tes Identifi√©s

### √Ä SUPPRIMER (Cleanup)

**Mod√®les ML obsol√®tes** (31 MB total):
```bash
models/battle_winner_rf_v1.pkl              # 28 MB - OLD: Random Forest model
models/random_forest_v1.pkl                 # OLD: Unused RF variant
models/random_forest_no_multiplier_v1.pkl   # OLD: Unused RF variant
models/battle_winner_xgb_v1.pkl             # DUPLICATE: Same as battle_winner_model_v1.pkl
models/model_metadata.pkl                   # OLD: Uses deprecated joblib format
```

**Fichiers de dataset inutilis√©s:**
```bash
machine_learning/build_classification_dataset.py  # UNUSED: Different target (move effectiveness)
```

### √Ä ARCHIVER (Consolidation documentation)

**27 fichiers .md** dans la racine (garder les essentiels, archiver le reste):

**GARDER (Documentation active):**
- `README.md` - Vue d'ensemble du projet
- `E1_DOCUMENTATION.md` - Comp√©tences E1 valid√©es
- `E3_PLAN_FINAL.md` - Plan E3 (remplacer par ce nouveau document)
- `ML_MODEL_DOCUMENTATION.md` - Documentation du mod√®le ML
- `API_EXAMPLES.md` - Exemples d'utilisation API

**ARCHIVER vers `/docs/archive/`:**
- `BUGFIXES_APPLIED.md`
- `FINAL_ADJUSTMENTS.md`
- `DOCKER_TEST_REPORT.md`
- `COMPARE_PAGES_SIMPLIFICATION.md`
- `CORRECTIONS_MODELE_V1.md`
- `OPPONENT_MOVE_SELECTION_ANALYSIS.md`
- `POKEMON_DETAIL_FIXES.md`
- `POKEMON_THEME.md`
- `STREAMLIT_*.md` (5 fichiers)
- `API_AND_MODEL_ANALYSIS.md`
- `HANDOFF_CONTEXT.md`
- `MODEL_ACCURACY_EXPLANATION.md`

---

## Actions √† Mener (Par Ordre de Priorit√©)

### üî¥ PRIORIT√â 1: Script ML Unifi√© (C12, C13)

**Objectif:** Cr√©er un script `run_machine_learning.py` qui orchestre TOUT le pipeline ML.

**√âtapes:**
1. **Dataset Preparation**
   - Load data from DB
   - Generate Pokemon matchups
   - Feature engineering
   - Train/test split
   - Export processed datasets

2. **Model Training**
   - Load preprocessed data
   - Train XGBoost model
   - Hyperparameter tuning (optionnel)
   - Cross-validation

3. **Model Evaluation**
   - Calculate metrics (accuracy, precision, recall, F1)
   - Generate confusion matrix
   - ROC curve analysis
   - Feature importance

4. **Model Selection**
   - Compare multiple models (XGBoost, RandomForest, etc.)
   - Select best model based on metrics
   - Export best model artifacts

5. **Model Export**
   - Save model to `/models/`
   - Save scalers to `/models/`
   - Save metadata to `/models/`
   - Update MLflow tracking (si impl√©ment√©)

**Fichier √† cr√©er:**
```
machine_learning/run_machine_learning.py
```

**Commande d'ex√©cution:**
```bash
python machine_learning/run_machine_learning.py --mode=all
python machine_learning/run_machine_learning.py --mode=dataset
python machine_learning/run_machine_learning.py --mode=train
python machine_learning/run_machine_learning.py --mode=evaluate
```

**Validation:** Comp√©tences C12 (tests automatis√©s int√©gr√©s)

---

### üî¥ PRIORIT√â 2: Tests Automatis√©s Complets (C12)

**Objectif:** Valider TOUS les aspects du pipeline ML avec tests automatis√©s.

#### A. Tests de Dataset
**Fichier:** `machine_learning/tests/test_dataset.py`

**Tests requis:**
- ‚úÖ Validation de la structure du dataset
- ‚úÖ Validation des types de donn√©es
- ‚úÖ V√©rification de l'absence de valeurs nulles
- ‚úÖ Validation des ranges de valeurs (stats, multipliers)
- ‚úÖ Validation de la distribution des classes (√©quilibre winner A/B)
- ‚úÖ Validation de la coh√©rence des features (STAB, type multipliers)

#### B. Tests de Preprocessing
**Fichier:** `machine_learning/tests/test_preprocessing.py`

**Tests requis:**
- ‚úÖ Test de one-hot encoding
- ‚úÖ Test de normalization (StandardScaler)
- ‚úÖ Test de feature engineering (derived features)
- ‚úÖ Test de train/test split (80/20)
- ‚úÖ Test de coh√©rence des transformations

#### C. Tests d'Entra√Ænement
**Fichier:** `machine_learning/tests/test_training.py`

**Tests requis:**
- ‚úÖ Test d'entra√Ænement du mod√®le (reproductibilit√©)
- ‚úÖ Test de convergence (pas d'overfitting)
- ‚úÖ Test de m√©triques minimales (accuracy >= 90%)
- ‚úÖ Test de sauvegarde des artifacts

#### D. Tests d'√âvaluation
**Fichier:** `machine_learning/tests/test_evaluation.py`

**Tests requis:**
- ‚úÖ Test de pr√©diction sur donn√©es de test
- ‚úÖ Test de calcul des m√©triques
- ‚úÖ Test de g√©n√©ration des rapports (confusion matrix)
- ‚úÖ Test de feature importance

#### E. Tests de R√©gression
**Fichier:** `machine_learning/tests/test_regression.py`

**Tests requis:**
- ‚úÖ Test que le nouveau mod√®le ne d√©grade PAS les performances
- ‚úÖ Test de non-r√©gression sur dataset fixe
- ‚úÖ Test de stabilit√© des pr√©dictions

**Validation:** Comp√©tence C12 compl√®te

---

### üî¥ PRIORIT√â 3: Monitoring avec Prometheus + Grafana (C11)

**Objectif:** Monitorer le mod√®le en temps r√©el en production.

#### A. Int√©gration Prometheus

**Fichier:** `api_pokemon/monitoring/metrics.py`

**M√©triques √† collecter:**
- `api_requests_total` (Counter) - Nombre total de requ√™tes
- `api_request_duration_seconds` (Histogram) - Latence des requ√™tes
- `api_errors_total` (Counter) - Nombre d'erreurs
- `model_predictions_total` (Counter) - Nombre de pr√©dictions
- `model_prediction_duration_seconds` (Histogram) - Latence du mod√®le
- `model_confidence_score` (Gauge) - Score de confiance moyen
- `model_accuracy` (Gauge) - Accuracy sur batch r√©cent

**Configuration:**
- Ajouter `prometheus-client` √† `requirements.txt`
- Exposer endpoint `/metrics` dans FastAPI
- Collecter m√©triques dans `prediction_service.py`

**Fichier Docker Compose:**
```yaml
prometheus:
  image: prom/prometheus:latest
  ports:
    - "9090:9090"
  volumes:
    - ./docker/prometheus.yml:/etc/prometheus/prometheus.yml
```

#### B. Dashboards Grafana

**Fichier:** `docker/grafana/dashboards/model_monitoring.json`

**Dashboards √† cr√©er:**
1. **API Performance**
   - Request rate (req/s)
   - Latency (p50, p95, p99)
   - Error rate

2. **Model Performance**
   - Prediction rate
   - Prediction latency
   - Confidence distribution
   - Accuracy over time

3. **Resource Usage**
   - CPU usage
   - Memory usage
   - Disk I/O

**Configuration:**
```yaml
grafana:
  image: grafana/grafana:latest
  ports:
    - "3000:3000"
  volumes:
    - ./docker/grafana/dashboards:/var/lib/grafana/dashboards
    - ./docker/grafana/provisioning:/etc/grafana/provisioning
  depends_on:
    - prometheus
```

#### C. Alerting

**Fichier:** `docker/prometheus/alerts.yml`

**Alertes √† configurer:**
- Latence API > 500ms
- Error rate > 5%
- Model accuracy < 90%
- Resource usage > 80%

**Validation:** Comp√©tence C11 compl√®te

---

### üü° PRIORIT√â 4: MLflow pour Tracking (C11, C13)

**Objectif:** Tracker tous les experiments ML et versionner les mod√®les.

#### A. Int√©gration MLflow

**Fichier:** `machine_learning/mlflow_tracking.py`

**Fonctionnalit√©s:**
- Tracking des hyperparam√®tres
- Tracking des m√©triques d'entra√Ænement
- Logging des artifacts (models, scalers)
- Model registry

**Configuration:**
```yaml
mlflow:
  image: ghcr.io/mlflow/mlflow:latest
  ports:
    - "5000:5000"
  volumes:
    - ./mlruns:/mlruns
  command: mlflow server --backend-store-uri sqlite:///mlruns/mlflow.db --default-artifact-root ./mlruns --host 0.0.0.0
```

#### B. Modification du script d'entra√Ænement

**Fichier:** `machine_learning/train_model.py`

**Ajouts requis:**
```python
import mlflow
import mlflow.sklearn

with mlflow.start_run():
    # Log hyperparameters
    mlflow.log_params({
        'n_estimators': 100,
        'max_depth': 8,
        'learning_rate': 0.1,
    })

    # Train model
    model.fit(X_train, y_train)

    # Log metrics
    mlflow.log_metrics({
        'accuracy': 0.9424,
        'precision': 0.94,
        'recall': 0.94,
        'f1': 0.94,
    })

    # Log model
    mlflow.sklearn.log_model(model, "model")
```

**Validation:** Comp√©tences C11 (tracking) et C13 (versioning)

---

### üü° PRIORIT√â 5: Pipeline CI/CD (C13)

**Objectif:** Automatiser tests, validation, packaging et d√©ploiement.

#### A. GitHub Actions Workflow

**Fichier:** `.github/workflows/ml_pipeline.yml`

**√âtapes du pipeline:**

1. **Lint & Format**
   - black (formatting)
   - flake8 (linting)
   - mypy (type checking)

2. **Tests**
   - Run all unit tests
   - Run integration tests
   - Run model tests
   - Generate coverage report

3. **Build Docker Images**
   - Build API image
   - Build ML image
   - Build Streamlit image
   - Push to registry

4. **Deploy**
   - Deploy to staging
   - Run smoke tests
   - Deploy to production (manual approval)

**Fichier exemple:**
```yaml
name: ML Pipeline

on:
  push:
    branches: [main, dev]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run tests
        run: pytest machine_learning/tests/ --cov

  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Build Docker images
        run: docker-compose build
      - name: Push images
        run: docker-compose push
```

#### B. Pre-commit Hooks

**Fichier:** `.pre-commit-config.yaml`

**Hooks:**
- black (formatting)
- flake8 (linting)
- mypy (type checking)
- pytest (tests rapides)

**Validation:** Comp√©tence C13 compl√®te

---

### üü¢ PRIORIT√â 6: Documentation & Nettoyage

**Objectif:** Finaliser la documentation et nettoyer le projet.

#### A. Nettoyage des fichiers obsol√®tes

**Script de nettoyage:**
```bash
# Supprimer mod√®les obsol√®tes
rm models/battle_winner_rf_v1.pkl
rm models/random_forest_v1.pkl
rm models/random_forest_no_multiplier_v1.pkl
rm models/battle_winner_xgb_v1.pkl
rm models/model_metadata.pkl

# Archiver documentation
mkdir -p docs/archive
mv BUGFIXES_APPLIED.md docs/archive/
mv FINAL_ADJUSTMENTS.md docs/archive/
mv DOCKER_TEST_REPORT.md docs/archive/
# ... (autres fichiers list√©s ci-dessus)
```

#### B. Mise √† jour README.md

**Sections √† ajouter:**
- Architecture MLOps
- Monitoring avec Prometheus/Grafana
- MLflow tracking
- CI/CD pipeline
- Instructions de contribution
- Roadmap

#### C. Mise √† jour E3_DOCUMENTATION.md

**Contenu:**
- Preuve de validation pour chaque comp√©tence C9-C13
- Captures d'√©cran des dashboards Grafana
- Logs MLflow
- Pipeline CI/CD
- M√©triques de monitoring

---

## Timeline Recommand√©

| Semaine | Priorit√© | T√¢che | Temps estim√© | Validation |
|---------|----------|-------|--------------|------------|
| **Semaine 1** | üî¥ | Script `run_machine_learning.py` | 2 jours | C12, C13 |
| | üî¥ | Tests automatis√©s complets | 3 jours | C12 |
| **Semaine 2** | üî¥ | Prometheus + m√©triques API | 2 jours | C11 |
| | üî¥ | Grafana dashboards | 1 jour | C11 |
| | üî¥ | Alerting | 1 jour | C11 |
| **Semaine 3** | üü° | MLflow int√©gration | 2 jours | C11, C13 |
| | üü° | Pipeline CI/CD | 2 jours | C13 |
| | üü° | Pre-commit hooks | 0.5 jour | C13 |
| **Semaine 4** | üü¢ | Nettoyage fichiers | 0.5 jour | - |
| | üü¢ | Documentation finale | 1.5 jours | - |
| | üü¢ | Validation E3 | 1 jour | - |

**Total estim√©:** 17 jours de d√©veloppement

---

## R√©sum√© des Comp√©tences E3

| Comp√©tence | Statut Actuel | Actions Requises | Validation |
|------------|---------------|------------------|------------|
| **C9** | ‚úÖ Valid√© | Aucune | API REST fonctionnelle |
| **C10** | ‚úÖ Valid√© | Aucune | Streamlit int√©gr√© |
| **C11** | ‚ö†Ô∏è 20% | Prometheus + Grafana + Alerting | Dashboards monitoring |
| **C12** | ‚ö†Ô∏è 40% | Tests automatis√©s complets | Suite de tests compl√®te |
| **C13** | ‚ö†Ô∏è 30% | MLflow + CI/CD pipeline | Pipeline fonctionnel |

---

## Prochaines √âtapes Imm√©diates

1. **Valider ce plan** avec l'√©quipe/formateur
2. **Cr√©er les branches Git** pour chaque priorit√©
3. **Commencer par Priorit√© 1**: Script ML unifi√©
4. **Impl√©menter progressivement** selon timeline
5. **Tester √† chaque √©tape**
6. **Documenter au fur et √† mesure**

---

**Auteur:** Claude Code
**Date de r√©vision:** 2026-01-22
**Version:** 1.0

# üìä Explication du Monitoring - Let's Go PredictionDex

**Date**: 2026-01-29
**Status**: ‚úÖ Monitoring Op√©rationnel

---

## üéØ Vue d'Ensemble

Votre projet utilise **3 syst√®mes de monitoring compl√©mentaires**:

1. **Grafana + Prometheus** ‚Üí Monitoring temps r√©el (m√©triques, performance, erreurs)
2. **MLflow** ‚Üí Tracking des exp√©riences ML et registry de mod√®les
3. **Evidently AI** ‚Üí D√©tection de drift des donn√©es (Data Drift)

---

## üìà 1. GRAFANA - Dashboards en Temps R√©el

Grafana collecte des m√©triques depuis Prometheus toutes les 15 secondes.

### Dashboard 1: **Model Performance** ü§ñ

**URL**: http://localhost:3001/d/letsgo-model

Ce dashboard surveille **les performances du mod√®le ML** en production.

#### Panneaux (8 au total):

##### Row 1 - Vue d'Ensemble (Stats)

1. **Predictions per Minute**
   - **Quoi**: Nombre de pr√©dictions faites par minute
   - **M√©trique**: `model_predictions_total` (rate sur 1 min)
   - **Utilit√©**: Surveiller le throughput du mod√®le
   - **Alerte si**: Chute brutale = probl√®me API/mod√®le

2. **Model Confidence (Avg)**
   - **Quoi**: Confiance moyenne du mod√®le dans ses pr√©dictions
   - **M√©trique**: `model_confidence_score_sum / model_confidence_score_count`
   - **Plage**: 0.0 √† 1.0 (0 = pas confiant, 1 = tr√®s confiant)
   - **Couleurs**:
     - üî¥ Rouge: < 0.6 (confiance faible)
     - üü° Jaune: 0.6 - 0.8 (confiance moyenne)
     - üü¢ Vert: > 0.8 (confiance √©lev√©e)
   - **Utilit√©**: D√©tecter si le mod√®le devient incertain (possible drift)

3. **P95 Prediction Latency**
   - **Quoi**: 95% des pr√©dictions sont effectu√©es en moins de X secondes
   - **M√©trique**: `histogram_quantile(0.95, model_prediction_duration_seconds_bucket)`
   - **Unit√©**: Secondes
   - **Couleurs**:
     - üü¢ Vert: < 0.05s (rapide)
     - üü° Jaune: 0.05s - 0.1s (acceptable)
     - üî¥ Rouge: > 0.1s (lent)
   - **Utilit√©**: Garantir des temps de r√©ponse acceptables
   - **Valeur actuelle**: ~0.45s (√† optimiser si critique)

4. **Total Predictions**
   - **Quoi**: Nombre total de pr√©dictions depuis le d√©marrage
   - **M√©trique**: `model_predictions_total` (cumulative)
   - **Utilit√©**: Tracker l'utilisation globale du mod√®le

##### Row 2 - Latence et Confiance (Graphs Temporels)

5. **Prediction Latency Percentiles**
   - **Quoi**: √âvolution de la latence dans le temps (P50, P95, P99)
   - **M√©triques**:
     - P50 (m√©diane): 50% des requ√™tes plus rapides
     - P95: 95% des requ√™tes plus rapides
     - P99: 99% des requ√™tes plus rapides
   - **Utilit√©**: D√©tecter les pics de latence et probl√®mes de performance
   - **L√©gende**: Affiche mean et max

6. **Model Confidence Over Time**
   - **Quoi**: √âvolution de la confiance moyenne au fil du temps
   - **M√©trique**: Rate de `model_confidence_score` sur l'intervalle
   - **Utilit√©**: D√©tecter une d√©gradation progressive du mod√®le
   - **Alerte si**: Tendance √† la baisse = possible data drift

##### Row 3 - Distribution et Versions (Pie Chart + Graph)

7. **Win Probability Distribution**
   - **Quoi**: R√©partition des pr√©dictions par probabilit√© de victoire
   - **Type**: Pie chart (camembert)
   - **M√©trique**: `model_win_probability_bucket` (histogram)
   - **Buckets**: Probabilit√©s group√©es (‚â§ 0.1, ‚â§ 0.3, ‚â§ 0.5, etc.)
   - **Utilit√©**: V√©rifier que le mod√®le pr√©dit une distribution r√©aliste
   - **Exemple sain**: Distribution √©quilibr√©e (pas 100% dans un seul bucket)

8. **Predictions Rate by Model Version**
   - **Quoi**: Taux de pr√©dictions par version de mod√®le (si A/B testing)
   - **M√©trique**: `model_predictions_total` group√© par `model_version`
   - **Utilit√©**: Comparer plusieurs versions de mod√®le en parall√®le
   - **Usage**: Utile pour rollout progressif (v1 vs v2)

---

### Dashboard 2: **API Performance** üöÄ

**URL**: http://localhost:3001/d/letsgo-api

Ce dashboard surveille **la sant√© de l'API FastAPI** en production.

#### Panneaux (probablement 6-8):

##### M√©triques Typiques:

1. **Request Rate**
   - Nombre de requ√™tes HTTP par seconde
   - M√©trique: `api_requests_total` (rate)

2. **Error Rate**
   - Pourcentage d'erreurs HTTP (4xx, 5xx)
   - M√©trique: `(api_errors_total or vector(0)) / (api_requests_total or vector(1))`
   - **Fix√©**: Affiche 0% au lieu de "no data" quand pas d'erreurs

3. **Response Time**
   - Latence des endpoints API (P50, P95, P99)
   - M√©trique: `histogram_quantile()` sur `api_request_duration_seconds`

4. **Requests by Endpoint**
   - R√©partition du trafic par endpoint (/predict, /health, /docs, etc.)
   - M√©trique: `api_requests_total` group√© par `path`

5. **Status Codes Distribution**
   - R√©partition des codes HTTP (200, 400, 500, etc.)
   - M√©trique: `api_requests_total` group√© par `status_code`

6. **API Uptime**
   - Disponibilit√© de l'API (0-100%)
   - M√©trique: Bas√© sur health checks

---

## üß™ 2. MLFLOW - Tracking & Model Registry

**URL**: http://localhost:5001

### Qu'est-ce que MLflow fait actuellement?

MLflow remplit **3 r√¥les majeurs** dans votre projet:

#### A) üìä **Tracking des Exp√©riences**

**Objectif**: Enregistrer tous les d√©tails de chaque entra√Ænement de mod√®le.

**Ce qui est track√©**:

```python
# √Ä chaque entra√Ænement, MLflow enregistre:
mlflow.log_params({
    "model_type": "XGBoost",
    "n_estimators": 200,
    "max_depth": 7,
    "learning_rate": 0.05,
    # ... tous les hyperparam√®tres
})

mlflow.log_metrics({
    "accuracy": 0.94,
    "precision": 0.92,
    "recall": 0.95,
    "f1_score": 0.935,
    "roc_auc": 0.98
})

mlflow.log_artifact("confusion_matrix.png")
mlflow.log_artifact("feature_importance.csv")
```

**Utilit√©**:
- Comparer diff√©rentes versions du mod√®le
- Retrouver les meilleurs hyperparam√®tres
- Reproduire un entra√Ænement exact
- Auditer l'√©volution du mod√®le

**Dans l'UI MLflow**:
- Onglet **Experiments**: Liste toutes vos exp√©riences
- Onglet **Runs**: D√©tails de chaque entra√Ænement
- Tableau comparatif avec tri/filtre sur les m√©triques

#### B) üì¶ **Model Registry**

**Objectif**: Versionner et d√©ployer les mod√®les ML comme des packages.

**Workflow**:

1. **Enregistrement du mod√®le**:
   ```python
   mlflow.sklearn.log_model(model, "xgboost_model")
   # Cr√©e automatiquement le mod√®le dans le registry
   ```

2. **Versioning automatique**:
   - Version 1: Premier mod√®le (2026-01-15)
   - Version 2: Mod√®le am√©lior√© (2026-01-20)
   - Version 3: Mod√®le avec nouvelles features (2026-01-29)

3. **Stages de d√©ploiement**:
   - `None`: Mod√®le en d√©veloppement
   - `Staging`: Mod√®le en test
   - `Production`: Mod√®le actif en prod
   - `Archived`: Ancien mod√®le archiv√©

**Utilit√©**:
- Rollback facile en cas de probl√®me (revenir √† v2)
- Comparer v2 (staging) vs v3 (production)
- Tra√ßabilit√©: savoir quel mod√®le est o√π

**Dans l'UI MLflow**:
- Onglet **Models**: Liste des mod√®les enregistr√©s
- D√©tails: Versions, stages, artifacts, lineage

#### C) üîó **Int√©gration avec l'API**

**Objectif**: Charger automatiquement le mod√®le depuis MLflow.

**Code actuel** (dans `machine_learning/mlflow_integration.py`):

```python
def load_production_model():
    """
    Charge le mod√®le en production depuis MLflow.

    Recherche le mod√®le avec stage='Production' dans le registry.
    Fallback sur la derni√®re version si pas de mod√®le en production.
    """
    client = MlflowClient()

    # Cherche le mod√®le en production
    versions = client.search_model_versions(
        filter_string="name='pokemon_battle_model' AND status='Production'"
    )

    if versions:
        model_uri = f"models:/{MODEL_NAME}/Production"
        model = mlflow.sklearn.load_model(model_uri)
        return model
    else:
        # Fallback sur la derni√®re version
        model_uri = f"models:/{MODEL_NAME}/latest"
        model = mlflow.sklearn.load_model(model_uri)
        return model
```

**Avantages**:
- API charge toujours le bon mod√®le automatiquement
- Mise √† jour du mod√®le sans red√©ployer l'API
- Rollback instantan√© en cas de r√©gression

---

## üîç 3. EVIDENTLY AI - Data Drift Detection

**URL**: Rapports HTML g√©n√©r√©s localement
**Fichier**: `api_pokemon/monitoring/drift_detection.py`

### Qu'est-ce qu'Evidently fait?

**Objectif**: D√©tecter si les donn√©es en production **d√©rivent** par rapport aux donn√©es d'entra√Ænement.

#### Concept de "Data Drift"

**D√©finition**: Les distributions statistiques changent entre train et production.

**Exemple concret**:

```
üìä Donn√©es d'entra√Ænement (2025):
- Pok√©mon Type 1: Eau (30%), Feu (25%), Plante (20%), autres (25%)
- Niveau moyen: 50
- Stats moyennes: Attack=80, Defense=75

üìä Donn√©es en production (2026):
- Pok√©mon Type 1: Eau (60%), Feu (10%), Plante (5%), autres (25%)  ‚Üê DRIFT!
- Niveau moyen: 65  ‚Üê DRIFT!
- Stats moyennes: Attack=95, Defense=70  ‚Üê DRIFT!
```

**Cons√©quence**: Le mod√®le a √©t√© entra√Æn√© sur une distribution, mais voit une autre en prod ‚Üí **performances d√©grad√©es**.

#### Comment √ßa fonctionne dans votre projet?

##### 1. **Chargement des Donn√©es de R√©f√©rence**

Au d√©marrage de l'API:

```python
# Charge X_train.parquet (donn√©es d'entra√Ænement)
reference_data = pd.read_parquet("data/datasets/X_train.parquet")
sampled_reference = reference_data.sample(n=10000, random_state=42)

# Cr√©e un Dataset Evidently
reference_dataset = Dataset.from_pandas(sampled_reference)
```

##### 2. **Buffer des Pr√©dictions en Production**

√Ä chaque pr√©diction via l'API:

```python
# Enregistre les features + pr√©diction
drift_detector.add_prediction(
    features={
        'pokemon_a_type_1': 'Water',
        'pokemon_a_attack': 95,
        'pokemon_b_defense': 80,
        # ... toutes les features
    },
    prediction=1,  # Pok√©mon A gagne
    probability=0.87
)

# Stocke dans un buffer (max 1000 pr√©dictions)
```

##### 3. **G√©n√©ration Automatique de Rapports**

**Triggers**:
- Buffer plein (1000 pr√©dictions)
- OU toutes les 1h

**Actions**:

```python
# Compare production vs r√©f√©rence
report = Report([DataDriftPreset()])
report.run(production_dataset, reference_dataset)

# G√©n√®re 2 fichiers:
# 1. drift_dashboard_20260129_153045.html  ‚Üê Dashboard interactif
# 2. drift_report_20260129_153045.json     ‚Üê M√©triques JSON
```

##### 4. **Alertes et M√©triques**

**Rapport JSON contient**:

```json
{
  "timestamp": "20260129_153045",
  "n_features": 45,
  "n_drifted_features": 8,
  "share_drifted_features": 0.178,  // 17.8% des features ont drift√©
  "dataset_drift": true  // ‚ö†Ô∏è ALERTE DRIFT D√âTECT√â
}
```

**Tableau HTML montre**:
- ‚úÖ Features stables (pas de drift)
- ‚ö†Ô∏è Features avec drift l√©ger
- üî¥ Features avec drift s√©v√®re

**Actions correctives**:
- Si drift < 20%: Surveiller
- Si drift > 30%: Re-entra√Æner le mod√®le
- Si drift > 50%: Re-collecter des donn√©es

#### O√π trouver les rapports?

```bash
# Rapports HTML (ouvrir dans navigateur)
api_pokemon/monitoring/drift_reports/drift_dashboard_*.html

# M√©triques JSON (pour automatisation)
api_pokemon/monitoring/drift_reports/drift_summary_*.json

# Donn√©es de production sauvegard√©es
api_pokemon/monitoring/drift_data/production_data_*.parquet
```

---

## üîÑ Comment les 3 Syst√®mes Travaillent Ensemble

### Sc√©nario: D√©tection d'un Probl√®me de Mod√®le

#### Timeline:

**J+0 - D√©ploiement Initial**:
1. MLflow: Mod√®le v2 en production (accuracy=94%)
2. Grafana: Model Confidence = 0.92 (bon)
3. Evidently: Drift = 0% (normal)

**J+7 - Premiers Signes**:
1. Grafana: Model Confidence baisse √† 0.75 ‚ö†Ô∏è
2. Grafana: Win Probability Distribution devient d√©s√©quilibr√©e
3. Aucune alerte encore

**J+14 - Alerte Drift**:
1. Evidently g√©n√®re un rapport: **Drift d√©tect√© sur 35% des features** üî¥
2. Grafana: Model Confidence = 0.68 (rouge)
3. MLflow: Compare v2 (prod) vs v1 (archive) ‚Üí metrics similaires
4. **Action**: Besoin de re-entra√Ænement avec nouvelles donn√©es

**J+15 - Re-entra√Ænement**:
1. MLflow: Nouveau run d'entra√Ænement avec donn√©es r√©centes
2. MLflow: Enregistre mod√®le v3 (accuracy=95%, prend en compte le drift)
3. MLflow: v3 en Staging pour tests

**J+16 - Tests A/B**:
1. Grafana: "Predictions by Model Version" montre v2 (80%) vs v3 (20%)
2. Grafana: v3 a meilleure confidence (0.91 vs 0.68)
3. Evidently: Drift de v3 = 5% (acceptable)

**J+17 - Rollout v3**:
1. MLflow: Promouvoir v3 en Production
2. API: Charge automatiquement v3 depuis MLflow
3. Grafana: Model Confidence remonte √† 0.92 ‚úÖ
4. Evidently: Drift stabilis√© √† 8%

---

## üìã R√©capitulatif des R√¥les

| Syst√®me | R√¥le | Fr√©quence | Alertes |
|---------|------|-----------|---------|
| **Grafana** | Monitoring temps r√©el | 15 secondes | Latence, erreurs, throughput |
| **MLflow** | Gestion des mod√®les | Par entra√Ænement | Aucune (registry passif) |
| **Evidently** | D√©tection de drift | 1 heure | Drift > seuil (30%) |

### Grafana
- ‚úÖ **Quand l'utiliser**: Surveillance continue (24/7)
- ‚úÖ **Pour d√©tecter**: Probl√®mes de performance, erreurs, anomalies
- ‚úÖ **R√©action**: Imm√©diate (alertes en temps r√©el)

### MLflow
- ‚úÖ **Quand l'utiliser**: D√©veloppement de mod√®les, d√©ploiement
- ‚úÖ **Pour g√©rer**: Versions de mod√®les, exp√©riences ML
- ‚úÖ **R√©action**: Manuelle (data scientist d√©cide)

### Evidently
- ‚úÖ **Quand l'utiliser**: Validation p√©riodique des donn√©es
- ‚úÖ **Pour d√©tecter**: Changements de distribution, data drift
- ‚úÖ **R√©action**: Planifi√©e (re-entra√Ænement si n√©cessaire)

---

## üéØ Utilisation Pratique

### Pour la D√©monstration

1. **G√©n√©rer des donn√©es de test**:
   ```bash
   python3 scripts/populate_monitoring.py
   ```
   ‚Üí Cr√©e 50 pr√©dictions + 3 runs MLflow

2. **Ouvrir Grafana**:
   ```bash
   open http://localhost:3001
   # admin / admin
   ```
   ‚Üí Montrer les dashboards avec donn√©es r√©elles

3. **Ouvrir MLflow**:
   ```bash
   open http://localhost:5001
   ```
   ‚Üí Montrer les exp√©riences et le registry

4. **G√©n√©rer un rapport Evidently**:
   ```bash
   # Faire 1000 pr√©dictions via l'API
   # Evidently g√©n√®re automatiquement un rapport
   open api_pokemon/monitoring/drift_reports/drift_dashboard_*.html
   ```

### Pour le Monitoring en Production

**Checklist quotidienne** (5 min):

1. ‚úÖ Grafana Model Performance:
   - Model Confidence > 0.8
   - P95 Latency < 0.1s
   - Pas de pics d'erreurs

2. ‚úÖ Grafana API Performance:
   - Error Rate < 1%
   - Request Rate normal
   - Status codes majoritairement 200

3. ‚úÖ Evidently (1x/semaine):
   - V√©rifier dernier rapport drift
   - Drift < 30%

4. ‚úÖ MLflow (1x/mois):
   - Comparer performances des versions
   - Archiver vieux mod√®les

---

## üìö Ressources

- **Prometheus Queries**: http://localhost:9091
- **Grafana Dashboards**: http://localhost:3001
- **MLflow UI**: http://localhost:5001
- **API Docs**: http://localhost:8080/docs
- **Evidently Docs**: https://docs.evidentlyai.com/

---

**Auteur**: Claude Sonnet 4.5
**Derni√®re mise √† jour**: 2026-01-29

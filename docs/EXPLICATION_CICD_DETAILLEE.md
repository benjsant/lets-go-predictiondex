# ğŸš€ CI/CD GitHub Actions - Explication DÃ©taillÃ©e

**Date:** 27 janvier 2026
**Objectif:** Comprendre en profondeur les 4 workflows GitHub Actions et leur fonctionnement

---

## ğŸ“‹ Table des MatiÃ¨res

1. [Vue d'Ensemble CI/CD](#1-vue-densemble-cicd)
2. [Workflow 1: Tests](#2-workflow-1-tests)
3. [Workflow 2: Docker Build](#3-workflow-2-docker-build)
4. [Workflow 3: ML Pipeline](#4-workflow-3-ml-pipeline)
5. [Workflow 4: Lint & Security](#5-workflow-4-lint--security)
6. [IntÃ©gration ComplÃ¨te](#6-intÃ©gration-complÃ¨te)

---

## 1. Vue d'Ensemble CI/CD

### ğŸ¯ Architecture Globale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              GITHUB ACTIONS CI/CD PIPELINE               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TRIGGER (Push/Pull Request)
    â”‚
    â”‚  git push origin main
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GitHub Repository                                        â”‚
â”‚  â””â”€ .github/workflows/                                   â”‚
â”‚     â”œâ”€ tests.yml          â†’ Workflow 1: Tests            â”‚
â”‚     â”œâ”€ docker-build.yml   â†’ Workflow 2: Docker Build     â”‚
â”‚     â”œâ”€ ml-pipeline.yml    â†’ Workflow 3: ML Pipeline      â”‚
â”‚     â””â”€ lint.yml           â†’ Workflow 4: Lint & Security  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚  Parallel Execution (4 workflows)
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              â”‚               â”‚              â”‚
        â–¼              â–¼               â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Workflow 1   â”‚ â”‚ Workflow 2   â”‚ â”‚Workflow 3â”‚ â”‚Workflow 4â”‚
â”‚ Tests        â”‚ â”‚ Docker Build â”‚ â”‚ML Pipelineâ”‚ â”‚Lint      â”‚
â”‚ (2-3 min)    â”‚ â”‚ (8-10 min)   â”‚ â”‚(3-4 min) â”‚ â”‚(2-3 min) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚                â”‚               â”‚             â”‚
       â”‚  âœ… Pass       â”‚  âœ… Pass      â”‚  âœ… Pass    â”‚  âœ… Pass
       â”‚                â”‚               â”‚             â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚  ALL CHECKS PASSED âœ…
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pull Request (PR)                                        â”‚
â”‚  âœ… All checks have passed                               â”‚
â”‚  âœ… Code is ready to merge                               â”‚
â”‚                                                           â”‚
â”‚  [Merge Pull Request]                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚  Merge to main
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Production Deployment                                    â”‚
â”‚  - Docker images built and tested                        â”‚
â”‚  - All tests passed (252 tests)                          â”‚
â”‚  - Code quality validated                                â”‚
â”‚  - Security checks passed                                â”‚
â”‚  - ML model validated                                    â”‚
â”‚  â†’ Ready for docker-compose up -d                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ğŸ“Š RÃ©sumÃ© 4 Workflows

| Workflow | Fichier | Triggers | DurÃ©e | Objectif |
|----------|---------|----------|-------|----------|
| **1. Tests** | tests.yml | Push/PR sur main, develop | 2-3 min | Run 252 tests, coverage 82% |
| **2. Docker Build** | docker-build.yml | Push/PR sur main, develop | 8-10 min | Build 5 images Docker + tests intÃ©gration |
| **3. ML Pipeline** | ml-pipeline.yml | Push ML files, manual trigger | 3-4 min | Tests ML + training optionnel |
| **4. Lint & Security** | lint.yml | Push/PR sur main, develop | 2-3 min | Code quality + security scan |

**Temps total:** 15-20 minutes (en parallÃ¨le, pas sÃ©quentiel)

---

## 2. Workflow 1: Tests

**Fichier:** `.github/workflows/tests.yml`

### ğŸ¯ Objectif

ExÃ©cuter automatiquement les 252 tests unitaires + intÃ©gration Ã  chaque commit/PR pour garantir la qualitÃ© du code.

### ğŸ“ Code Complet AnnotÃ©

```yaml
name: Tests

# TRIGGERS: Quand ce workflow s'exÃ©cute
on:
  push:
    branches: [ main, monitoring_grafana_evidently, develop ]
    # â†’ S'exÃ©cute sur push vers ces branches
  pull_request:
    branches: [ main, monitoring_grafana_evidently ]
    # â†’ S'exÃ©cute sur ouverture/update PR vers ces branches

jobs:
  test:
    # Runner: Machine virtuelle Ubuntu 22.04
    runs-on: ubuntu-latest

    # SERVICE CONTAINERS: PostgreSQL lancÃ© automatiquement
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: letsgo_test
          POSTGRES_USER: letsgo_user
          POSTGRES_PASSWORD: letsgo_password
        ports:
          - 5432:5432
        # Health checks: attendre que Postgres soit prÃªt avant de continuer
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        # â†’ Postgres sera accessible sur localhost:5432

    # MATRIX STRATEGY: Test sur plusieurs versions Python (ici juste 3.11)
    strategy:
      matrix:
        python-version: ['3.11']

    steps:
      # Ã‰TAPE 1: Checkout code
      - name: Checkout code
        uses: actions/checkout@v4
        # â†’ Clone le repository GitHub dans le runner

      # Ã‰TAPE 2: Setup Python
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
        # â†’ Installe Python 3.11

      # Ã‰TAPE 3: Cache dependencies (optimisation)
      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          # Cache key basÃ© sur hash requirements.txt
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
        # â†’ Si requirements.txt n'a pas changÃ©, restaure cache
        # â†’ Gain: 30-60s par run

      # Ã‰TAPE 4: Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov pytest-asyncio
          pip install -r api_pokemon/requirements.txt
          pip install -r machine_learning/requirements.txt
        # â†’ Installe pytest + toutes les dÃ©pendances du projet

      # Ã‰TAPE 5: Set environment variables
      - name: Set environment variables
        run: |
          echo "POSTGRES_HOST=localhost" >> $GITHUB_ENV
          echo "POSTGRES_PORT=5432" >> $GITHUB_ENV
          echo "POSTGRES_DB=letsgo_test" >> $GITHUB_ENV
          echo "POSTGRES_USER=letsgo_user" >> $GITHUB_ENV
          echo "POSTGRES_PASSWORD=letsgo_password" >> $GITHUB_ENV
          echo "PYTHONPATH=$PWD" >> $GITHUB_ENV
        # â†’ Variables d'environnement pour connexion PostgreSQL

      # Ã‰TAPE 6: Run tests â­ CÅ’UR DU WORKFLOW
      - name: Run unit tests
        run: |
          pytest tests/ -v \
            --tb=short \
            --cov=api_pokemon \
            --cov=core \
            --cov=machine_learning \
            --cov-report=xml \
            --cov-report=term-missing
        # Options pytest:
        # -v: verbose (dÃ©tails tests)
        # --tb=short: traceback court si erreur
        # --cov=XXX: coverage pour ces modules
        # --cov-report=xml: gÃ©nÃ¨re coverage.xml (pour Codecov)
        # --cov-report=term-missing: affiche lignes non couvertes

      # Ã‰TAPE 7: Upload coverage to Codecov
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false
        # â†’ Upload coverage vers Codecov.io pour visualisation

      # Ã‰TAPE 8: Generate coverage badge
      - name: Generate coverage badge
        if: github.ref == 'refs/heads/main'
        run: |
          pip install coverage-badge
          coverage-badge -o coverage.svg -f
        # â†’ GÃ©nÃ¨re badge SVG "Coverage 82%" (seulement sur main)

      # Ã‰TAPE 9: Archive test results
      - name: Archive test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: test-results
          path: |
            coverage.xml
            .coverage
          retention-days: 30
        # â†’ Sauvegarde coverage.xml pendant 30 jours
        # â†’ if: always() = exÃ©cute mÃªme si tests Ã©chouent
```

---

### ğŸ” DÃ©tails Techniques

#### Service Containers (PostgreSQL)

```yaml
services:
  postgres:
    image: postgres:15
    # ...
```

**Comment Ã§a marche ?**

1. GitHub Actions lance un container Docker PostgreSQL **avant** les steps
2. Container accessible via `localhost:5432` (mapping automatique)
3. Health checks garantissent que Postgres est prÃªt avant les tests
4. Container est automatiquement arrÃªtÃ©/supprimÃ© aprÃ¨s le workflow

**Ã‰quivalent Docker local:**

```bash
docker run -d \
  --name postgres-test \
  -e POSTGRES_DB=letsgo_test \
  -e POSTGRES_USER=letsgo_user \
  -e POSTGRES_PASSWORD=letsgo_password \
  -p 5432:5432 \
  postgres:15

# Health check
docker exec postgres-test pg_isready

# Run tests
pytest tests/

# Cleanup
docker stop postgres-test && docker rm postgres-test
```

---

#### Coverage Report

**Output exemple:**

```
========================= test session starts ==========================
platform linux -- Python 3.11.7, pytest-7.4.3, pluggy-1.3.0
rootdir: /home/runner/work/lets-go-predictiondex
plugins: cov-4.1.0, asyncio-0.21.1

collected 252 items

tests/api/test_pokemon_route.py::test_get_pokemon_list PASSED    [  1%]
tests/api/test_pokemon_route.py::test_get_pokemon_by_id PASSED   [  2%]
tests/api/test_prediction_route.py::test_predict_best_move PASSED[  3%]
...
tests/ml/test_feature_engineering.py::test_engineer_features PASSED [99%]
tests/mlflow/test_registry.py::test_load_model_from_registry PASSED[100%]

========================= 252 passed in 14.52s ==========================

---------- coverage: platform linux, python 3.11.7 -----------
Name                                      Stmts   Miss  Cover   Missing
-----------------------------------------------------------------------
api_pokemon/__init__.py                       0      0   100%
api_pokemon/main.py                          48      5    90%   78-82
api_pokemon/routes/prediction_route.py       65      8    88%   95-102
api_pokemon/services/prediction_service.py  127     18    86%   234-251
core/db/session.py                           23      2    91%   45-46
core/models/pokemon.py                       45      0   100%
machine_learning/run_machine_learning.py    312     42    87%   567-608
-----------------------------------------------------------------------
TOTAL                                       2847    512    82%

10 files skipped due to complete coverage.
```

---

## 3. Workflow 2: Docker Build

**Fichier:** `.github/workflows/docker-build.yml`

### ğŸ¯ Objectif

Build et tester les 5 images Docker du projet en parallÃ¨le, puis exÃ©cuter des tests d'intÃ©gration avec `docker-compose`.

### ğŸ“ Code Complet AnnotÃ©

```yaml
name: Docker Build

on:
  push:
    branches: [ main, monitoring_grafana_evidently, develop ]
  pull_request:
    branches: [ main, monitoring_grafana_evidently ]

jobs:
  # JOB 1: Build images en parallÃ¨le (matrix strategy)
  build-and-test:
    runs-on: ubuntu-latest

    # MATRIX STRATEGY: Build 5 services en parallÃ¨le
    strategy:
      matrix:
        service: [api, etl, ml, streamlit, mlflow]
        # â†’ GitHub Actions lancera 5 jobs en parallÃ¨le (1 par service)
        # â†’ Gain: 8-10 min au lieu de 40-50 min sÃ©quentiel

    steps:
      # Ã‰TAPE 1: Checkout code
      - name: Checkout code
        uses: actions/checkout@v4

      # Ã‰TAPE 2: Setup Docker Buildx (builder avancÃ©)
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        # â†’ Buildx = builder Docker moderne avec cache layers

      # Ã‰TAPE 3: Cache Docker layers (OPTIMISATION CLÃ‰E)
      - name: Cache Docker layers
        uses: actions/cache@v3
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ matrix.service }}-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-buildx-${{ matrix.service }}-
        # â†’ Cache layers Docker par service
        # â†’ Si Dockerfile/code pas changÃ©, restore cache
        # â†’ Gain: 5-7 min par build

      # Ã‰TAPE 4: Build image Docker
      - name: Build ${{ matrix.service }} image
        run: |
          docker compose build ${{ matrix.service }}
        # â†’ Ã‰quivalent: docker build -t lets-go-predictiondex-api -f docker/api/Dockerfile .
        # â†’ Utilise docker-compose.yml pour config

      # Ã‰TAPE 5: Save image en tar.gz (artifact)
      - name: Save image
        run: |
          docker save lets-go-predictiondex-${{ matrix.service }} | gzip > ${{ matrix.service }}.tar.gz
        # â†’ Exporte image Docker en fichier .tar.gz
        # â†’ Pourquoi ? Pour partager entre jobs (GitHub Actions)

      # Ã‰TAPE 6: Upload artifact
      - name: Upload image artifact
        uses: actions/upload-artifact@v3
        with:
          name: docker-${{ matrix.service }}
          path: ${{ matrix.service }}.tar.gz
          retention-days: 1
        # â†’ Upload .tar.gz vers GitHub Artifacts
        # â†’ Accessible par job suivant (integration-test)
        # â†’ retention-days: 1 = supprimÃ© aprÃ¨s 24h (Ã©conomie stockage)

  # JOB 2: Tests d'intÃ©gration (dÃ©pend de build-and-test)
  integration-test:
    runs-on: ubuntu-latest
    needs: build-and-test
    # â†’ needs: attend que build-and-test soit terminÃ© avec succÃ¨s

    steps:
      # Ã‰TAPE 1: Checkout code
      - name: Checkout code
        uses: actions/checkout@v4

      # Ã‰TAPE 2: Download ALL artifacts (5 images Docker)
      - name: Download all artifacts
        uses: actions/download-artifact@v3
        # â†’ TÃ©lÃ©charge docker-api, docker-etl, docker-ml, docker-streamlit, docker-mlflow

      # Ã‰TAPE 3: Load Docker images
      - name: Load Docker images
        run: |
          for service in api etl ml streamlit mlflow; do
            if [ -f docker-$service/$service.tar.gz ]; then
              gunzip -c docker-$service/$service.tar.gz | docker load
            fi
          done
        # â†’ Importe les 5 images .tar.gz dans Docker local

      # Ã‰TAPE 4: Create .env file
      - name: Create .env file
        run: |
          cat > .env << EOF
          POSTGRES_HOST=db
          POSTGRES_PORT=5432
          POSTGRES_DB=letsgo_db
          POSTGRES_USER=letsgo_user
          POSTGRES_PASSWORD=letsgo_password
          DEV_MODE=true
          API_KEY_REQUIRED=false
          API_KEYS=test_key_for_ci_cd
          EOF
        # â†’ Config environnement pour docker-compose

      # Ã‰TAPE 5: Start services â­ CÅ’UR DU WORKFLOW
      - name: Start services
        run: |
          docker compose up -d
          sleep 60  # Wait for services to be ready
        # â†’ Lance les 9 services Docker en dÃ©tachÃ© (-d)
        # â†’ sleep 60 = attente startup (API, MLflow, Prometheus, etc.)

      # Ã‰TAPE 6: Check service health
      - name: Check service health
        run: |
          echo "Checking API health..."
          curl -f http://localhost:8080/health || exit 1

          echo "Checking MLflow health..."
          curl -f http://localhost:5001/health || exit 1

          echo "Checking Prometheus health..."
          curl -f http://localhost:9091/-/healthy || exit 1
        # â†’ Health checks: si 1 service down, workflow Ã©choue
        # â†’ -f flag curl = fail si HTTP error (4xx, 5xx)

      # Ã‰TAPE 7: Run integration tests
      - name: Run integration tests
        run: |
          docker compose exec -T api pytest tests/ -v -m integration
        # â†’ ExÃ©cute tests intÃ©gration INSIDE container API
        # â†’ -T flag = no TTY (requis pour CI/CD)
        # â†’ -m integration = seulement tests marquÃ©s @pytest.mark.integration

      # Ã‰TAPE 8: Show logs on failure
      - name: Show logs on failure
        if: failure()
        run: |
          docker compose logs
        # â†’ Si un step Ã©choue, affiche logs de TOUS les services
        # â†’ Utile pour debugging

      # Ã‰TAPE 9: Stop services (cleanup)
      - name: Stop services
        if: always()
        run: |
          docker compose down -v
        # â†’ ArrÃªte et supprime containers + volumes
        # â†’ if: always() = exÃ©cute mÃªme si tests Ã©chouent
        # â†’ -v flag = supprime volumes (cleanup complet)
```

---

### ğŸ” DÃ©tails Techniques

#### Matrix Strategy (ParallÃ©lisation)

**Sans matrix:**

```yaml
steps:
  - build api      # 8 min
  - build etl      # 8 min
  - build ml       # 8 min
  - build streamlit # 8 min
  - build mlflow   # 8 min
# Total: 40 min sÃ©quentiel âŒ
```

**Avec matrix:**

```yaml
strategy:
  matrix:
    service: [api, etl, ml, streamlit, mlflow]
# â†’ GitHub Actions lance 5 jobs en PARALLÃˆLE
# â†’ Chaque job build 1 service
# Total: 8 min (le plus lent) âœ…
```

**Visualisation GitHub Actions:**

```
build-and-test (api)       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 8 min âœ…
build-and-test (etl)       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 8 min âœ…
build-and-test (ml)        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 8 min âœ…
build-and-test (streamlit) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 8 min âœ…
build-and-test (mlflow)    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 8 min âœ…

integration-test           â–ˆâ–ˆâ–ˆâ–ˆ 2 min âœ…

Total: 10 min (au lieu de 42 min)
```

---

#### Artifacts (Partage entre Jobs)

**ProblÃ¨me:** Job 1 build images, Job 2 besoin de ces images

**Solution:** Artifacts

1. **Job 1:** Build image â†’ Save tar.gz â†’ Upload artifact
2. **Job 2:** Download artifact â†’ Load tar.gz â†’ Use image

**Equivalent local:**

```bash
# Job 1 (build-and-test)
docker compose build api
docker save lets-go-predictiondex-api | gzip > api.tar.gz

# Transfer to Job 2 (upload/download artifact)
# ...

# Job 2 (integration-test)
gunzip -c api.tar.gz | docker load
docker compose up -d
```

---

## 4. Workflow 3: ML Pipeline

**Fichier:** `.github/workflows/ml-pipeline.yml`

### ğŸ¯ Objectif

- **Auto:** Tester le code ML Ã  chaque modification
- **Manuel:** EntraÃ®ner un nouveau modÃ¨le via `workflow_dispatch`

### ğŸ“ Code Complet AnnotÃ©

```yaml
name: ML Pipeline

on:
  # TRIGGER 1: Push sur fichiers ML
  push:
    branches: [ main, monitoring_grafana_evidently ]
    paths:
      - 'machine_learning/**'
      - 'data/ml/**'
      - 'models/**'
    # â†’ S'exÃ©cute SEULEMENT si fichiers ML modifiÃ©s

  # TRIGGER 2: Manuel (workflow_dispatch)
  workflow_dispatch:
    inputs:
      dataset_version:
        description: 'Dataset version (v1 or v2)'
        required: true
        default: 'v2'
        type: choice
        options:
          - v1
          - v2
      model_version:
        description: 'Model version suffix'
        required: true
        default: 'ci'
    # â†’ Utilisateur peut dÃ©clencher manuellement via GitHub UI
    # â†’ ParamÃ¨tres: dataset version + model version

jobs:
  test-ml:
    runs-on: ubuntu-latest

    # SERVICE CONTAINERS: PostgreSQL + MLflow
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: letsgo_db
          POSTGRES_USER: letsgo_user
          POSTGRES_PASSWORD: letsgo_password
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      mlflow:
        image: ghcr.io/mlflow/mlflow:v2.9.2
        env:
          MLFLOW_BACKEND_STORE_URI: sqlite:///mlflow.db
        ports:
          - 5000:5000
        # â†’ MLflow Tracking Server accessible sur localhost:5000

    steps:
      # Ã‰TAPE 1-2: Checkout + Setup Python
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      # Ã‰TAPE 3: Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r machine_learning/requirements.txt
          pip install pytest pytest-cov

      # Ã‰TAPE 4: Set environment variables
      - name: Set environment variables
        run: |
          echo "POSTGRES_HOST=localhost" >> $GITHUB_ENV
          echo "POSTGRES_PORT=5432" >> $GITHUB_ENV
          echo "MLFLOW_TRACKING_URI=http://localhost:5000" >> $GITHUB_ENV
          echo "PYTHONPATH=$PWD" >> $GITHUB_ENV

      # Ã‰TAPE 5: Run ML tests â­
      - name: Run ML tests
        run: |
          pytest tests/ml/ -v --cov=machine_learning --cov-report=xml
        # â†’ ExÃ©cute 50 tests ML (feature engineering, dataset, training)

      # Ã‰TAPE 6: Train model (SEULEMENT si manuel trigger)
      - name: Train model (if manual trigger)
        if: github.event_name == 'workflow_dispatch'
        run: |
          python machine_learning/run_machine_learning.py \
            --mode=train \
            --dataset-version=${{ github.event.inputs.dataset_version }} \
            --version=${{ github.event.inputs.model_version }}
        # â†’ if: seulement si dÃ©clenchÃ© manuellement
        # â†’ Utilise paramÃ¨tres fournis par utilisateur

      # Ã‰TAPE 7: Validate model metrics
      - name: Validate model metrics
        if: github.event_name == 'workflow_dispatch'
        run: |
          python -c "
          import json
          from pathlib import Path
          metadata_path = Path('models/battle_winner_metadata_${{ github.event.inputs.model_version }}.json')
          with open(metadata_path) as f:
              meta = json.load(f)
          acc = meta['metrics']['test_accuracy']
          print(f'Test Accuracy: {acc:.4f}')
          assert acc > 0.80, f'Accuracy too low: {acc}'
          print('âœ… Model validation passed')
          "
        # â†’ Validation: accuracy DOIT Ãªtre > 80%
        # â†’ Si < 80%, workflow Ã©choue (assert)

      # Ã‰TAPE 8: Upload model artifacts
      - name: Upload model artifacts
        if: github.event_name == 'workflow_dispatch'
        uses: actions/upload-artifact@v3
        with:
          name: model-${{ github.event.inputs.model_version }}
          path: |
            models/battle_winner_model_${{ github.event.inputs.model_version }}.pkl
            models/battle_winner_metadata_${{ github.event.inputs.model_version }}.json
            models/battle_winner_scalers_${{ github.event.inputs.model_version }}.pkl
          retention-days: 90
        # â†’ Sauvegarde modÃ¨le pendant 90 jours
        # â†’ TÃ©lÃ©chargeable depuis GitHub Actions UI

      # Ã‰TAPE 9: Comment PR avec mÃ©triques
      - name: Comment PR with metrics
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: 'âœ… ML tests passed! Model metrics will be available after training.'
            })
        # â†’ Si c'est une PR, poste un commentaire automatique
```

---

### ğŸ” DÃ©tails Techniques

#### Workflow Dispatch (Trigger Manuel)

**Dans GitHub UI:**

1. Aller dans **Actions**
2. SÃ©lectionner **ML Pipeline**
3. Cliquer **Run workflow**
4. Remplir inputs:
   - Dataset version: `v2`
   - Model version: `ci_test_2026`
5. **Run workflow**

**GitHub Actions exÃ©cute:**

```bash
python machine_learning/run_machine_learning.py \
  --mode=train \
  --dataset-version=v2 \
  --version=ci_test_2026

# Training... (8 minutes)

# Output:
# âœ… Model trained successfully
# Test Accuracy: 0.8823 (88.23%)
# Model saved: models/battle_winner_model_ci_test_2026.pkl

# Validation:
assert 0.8823 > 0.80  # âœ… Pass

# Upload artifacts (90 jours)
```

---

#### MLflow Service Container

```yaml
services:
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.9.2
    env:
      MLFLOW_BACKEND_STORE_URI: sqlite:///mlflow.db
    ports:
      - 5000:5000
```

**Pourquoi ?**

- MLflow Tracking Server nÃ©cessaire pour `mlflow.log_model()`, `mlflow.log_metrics()`, etc.
- Service container = lancÃ© automatiquement avant le job
- Accessible via `http://localhost:5000` pendant le workflow

**Code ML qui utilise MLflow:**

```python
# machine_learning/run_machine_learning.py
import mlflow

mlflow.set_tracking_uri("http://localhost:5000")  # â†’ Service container

with mlflow.start_run(run_name="ci_training"):
    mlflow.log_params(hyperparams)
    mlflow.log_metrics(metrics)
    mlflow.sklearn.log_model(model, "model")
```

---

## 5. Workflow 4: Lint & Security

**Fichier:** `.github/workflows/lint.yml`

### ğŸ¯ Objectif

- **Lint:** VÃ©rifier code quality (formatage, style, types)
- **Security:** Scanner vulnÃ©rabilitÃ©s code + dÃ©pendances

### ğŸ“ Code Complet AnnotÃ©

```yaml
name: Lint and Format

on:
  push:
    branches: [ main, monitoring_grafana_evidently, develop ]
  pull_request:
    branches: [ main, monitoring_grafana_evidently ]

jobs:
  # JOB 1: Linting
  lint:
    runs-on: ubuntu-latest

    steps:
      # Ã‰TAPE 1-2: Checkout + Setup Python
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      # Ã‰TAPE 3: Cache pip
      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-lint-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-lint-

      # Ã‰TAPE 4: Install linting tools
      - name: Install linting tools
        run: |
          python -m pip install --upgrade pip
          pip install black flake8 isort mypy pylint
          pip install -r api_pokemon/requirements.txt
        # â†’ Installe 5 linters

      # Ã‰TAPE 5: Black (code formatting check)
      - name: Run black (check only)
        run: |
          black --check --diff api_pokemon core machine_learning interface
        # â†’ Black = formatteur Python automatique
        # â†’ --check = vÃ©rifie formatage sans modifier
        # â†’ --diff = affiche diffÃ©rences si non-conforme
        # â†’ Ã‰CHOUE si code pas formattÃ© selon Black

      # Ã‰TAPE 6: isort (imports sorting check)
      - name: Run isort (check only)
        run: |
          isort --check-only --diff api_pokemon core machine_learning interface
        # â†’ isort = tri imports alphabÃ©tique
        # â†’ Ã‰CHOUE si imports pas triÃ©s

      # Ã‰TAPE 7: Flake8 (PEP8 style guide)
      - name: Run flake8
        run: |
          flake8 api_pokemon core machine_learning interface \
            --max-line-length=120 \
            --exclude=__pycache__,.venv,.git,migrations \
            --ignore=E203,W503,E501
        # â†’ Flake8 = linter PEP8 (style guide Python officiel)
        # â†’ max-line-length=120 (au lieu de 79 par dÃ©faut)
        # â†’ ignore E203, W503, E501 (conflits Black)

      # Ã‰TAPE 8: Pylint (code quality)
      - name: Run pylint
        continue-on-error: true
        run: |
          pylint api_pokemon core machine_learning \
            --disable=C0111,C0103,R0913,R0914,W0511 \
            --max-line-length=120
        # â†’ Pylint = linter strict code quality
        # â†’ continue-on-error: true = WARNING seulement (pas bloquant)
        # â†’ disable codes: docstrings, naming, trop d'arguments

      # Ã‰TAPE 9: Mypy (type checking)
      - name: Run mypy
        continue-on-error: true
        run: |
          mypy api_pokemon core --ignore-missing-imports
        # â†’ Mypy = vÃ©rification types statiques
        # â†’ continue-on-error: true = WARNING (pas bloquant)

  # JOB 2: Security Scanning
  security:
    runs-on: ubuntu-latest

    steps:
      # Ã‰TAPE 1-2: Checkout + Setup Python
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      # Ã‰TAPE 3: Install security tools
      - name: Install security tools
        run: |
          python -m pip install --upgrade pip
          pip install bandit safety
        # â†’ Bandit = scanner sÃ©curitÃ© code Python
        # â†’ Safety = scanner vulnÃ©rabilitÃ©s dÃ©pendances

      # Ã‰TAPE 4: Bandit (security linter)
      - name: Run bandit (security linter)
        run: |
          bandit -r api_pokemon core machine_learning -f json -o bandit-report.json
        # â†’ Scanne code pour vulnÃ©rabilitÃ©s:
        #   - Injections SQL
        #   - Hardcoded passwords
        #   - Eval/exec insecure
        #   - etc.
        # â†’ Output: JSON report

      # Ã‰TAPE 5: Safety (dependency vulnerabilities)
      - name: Run safety (dependency check)
        continue-on-error: true
        run: |
          safety check --json --output safety-report.json
        # â†’ Scanne requirements.txt pour CVEs connus
        # â†’ Exemple: requests==2.25.0 a CVE-2023-XYZ
        # â†’ continue-on-error: warnings seulement

      # Ã‰TAPE 6: Upload security reports
      - name: Upload security reports
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json
          retention-days: 30
        # â†’ Sauvegarde reports 30 jours
        # â†’ TÃ©lÃ©chargeables depuis GitHub Actions UI
```

---

### ğŸ” DÃ©tails Techniques

#### Linters ExpliquÃ©s

**1. Black (Formatage Automatique)**

```python
# Code NON-conforme:
def my_function(a,b,c):
    return a+b+c

# Code Black-conforme:
def my_function(a, b, c):
    return a + b + c
```

**2. isort (Tri Imports)**

```python
# NON-conforme:
import sys
import pandas as pd
import os
from fastapi import FastAPI

# Conforme isort:
import os
import sys

import pandas as pd
from fastapi import FastAPI
```

**3. Flake8 (PEP8 Style Guide)**

```python
# Erreur Flake8:
def myFunction():  # E999: CamelCase (doit Ãªtre snake_case)
    x=1+2  # E225: missing whitespace around operator
    return x

# Conforme:
def my_function():
    x = 1 + 2
    return x
```

**4. Pylint (Code Quality)**

```python
# Warning Pylint:
def process_data(a, b, c, d, e, f, g, h):  # R0913: Too many arguments (8/6)
    pass

# AmÃ©lioration:
def process_data(config: DataConfig):  # 1 argument (dict/object)
    pass
```

**5. Mypy (Type Checking)**

```python
# Erreur Mypy:
def add(a, b):
    return a + b

result = add("hello", 5)  # Error: str + int impossible

# Conforme:
def add(a: int, b: int) -> int:
    return a + b

result = add(3, 5)  # âœ… OK
```

---

#### Bandit (Security Scanner)

**Exemple vulnÃ©rabilitÃ©s dÃ©tectÃ©es:**

```python
# 1. Hardcoded password (HIGH SEVERITY)
PASSWORD = "admin123"  # âŒ Bandit: B105

# Fix:
PASSWORD = os.getenv("PASSWORD")  # âœ…

# 2. SQL Injection (HIGH SEVERITY)
query = f"SELECT * FROM users WHERE id = {user_id}"  # âŒ Bandit: B608

# Fix:
query = "SELECT * FROM users WHERE id = ?"
cursor.execute(query, (user_id,))  # âœ… Parameterized query

# 3. Eval usage (MEDIUM SEVERITY)
eval(user_input)  # âŒ Bandit: B307

# Fix:
ast.literal_eval(user_input)  # âœ… Safe evaluation

# 4. Assert in production (LOW SEVERITY)
assert user.is_admin, "Not admin"  # âŒ Bandit: B101 (assert removed with -O flag)

# Fix:
if not user.is_admin:
    raise PermissionError("Not admin")  # âœ…
```

---

## 6. IntÃ©gration ComplÃ¨te

### ğŸ”— Flow Complet: Push â†’ CI/CD â†’ Merge

```
DEVELOPER
    â”‚
    â”‚  git checkout -b feature/add-new-pokemon
    â”‚  (modifie code)
    â”‚  git commit -m "feat: Add Mew to dataset"
    â”‚  git push origin feature/add-new-pokemon
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GITHUB REPOSITORY                                        â”‚
â”‚  Branch: feature/add-new-pokemon                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚  Push event triggered
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GITHUB ACTIONS CI/CD                                     â”‚
â”‚  4 workflows lancÃ©s en PARALLÃˆLE                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚                  â”‚              â”‚
        â–¼                 â–¼                  â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tests        â”‚  â”‚ Docker Build â”‚  â”‚ ML Pipelineâ”‚  â”‚ Lint       â”‚
â”‚ (2 min)      â”‚  â”‚ (8 min)      â”‚  â”‚ (3 min)    â”‚  â”‚ (2 min)    â”‚
â”‚              â”‚  â”‚              â”‚  â”‚            â”‚  â”‚            â”‚
â”‚ Run 252 testsâ”‚  â”‚ Build 5      â”‚  â”‚ Test ML    â”‚  â”‚ Black      â”‚
â”‚ Coverage 82% â”‚  â”‚ images       â”‚  â”‚ code       â”‚  â”‚ Flake8     â”‚
â”‚              â”‚  â”‚ Integration  â”‚  â”‚            â”‚  â”‚ Bandit     â”‚
â”‚ âœ… PASS      â”‚  â”‚ tests        â”‚  â”‚ âœ… PASS    â”‚  â”‚ Safety     â”‚
â”‚              â”‚  â”‚ âœ… PASS      â”‚  â”‚            â”‚  â”‚ âœ… PASS    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                 â”‚               â”‚              â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚  ALL CHECKS PASSED âœ…
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PULL REQUEST                                             â”‚
â”‚  feature/add-new-pokemon â†’ main                          â”‚
â”‚                                                           â”‚
â”‚  âœ… Tests (252 passed)                                   â”‚
â”‚  âœ… Docker Build (5 images OK)                           â”‚
â”‚  âœ… ML Pipeline (50 tests passed)                        â”‚
â”‚  âœ… Lint & Security (No issues)                          â”‚
â”‚                                                           â”‚
â”‚  [Merge Pull Request] â† Enabled (checks passed)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚  Merge button clicked
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MAIN BRANCH                                              â”‚
â”‚  feature/add-new-pokemon merged âœ…                       â”‚
â”‚                                                           â”‚
â”‚  Triggers CI/CD again (on main branch)                   â”‚
â”‚  â†’ Generate coverage badge                               â”‚
â”‚  â†’ Build Docker images (production)                      â”‚
â”‚  â†’ Ready for deployment                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PRODUCTION DEPLOYMENT                                    â”‚
â”‚  docker-compose pull                                     â”‚
â”‚  docker-compose up -d                                    â”‚
â”‚  âœ… New version deployed                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ğŸ“Š Timeline Exemple

```
Minute 0:  Developer push code
           â”‚
           â”œâ”€ Tests workflow started
           â”œâ”€ Docker Build workflow started
           â”œâ”€ ML Pipeline workflow started
           â””â”€ Lint workflow started

Minute 2:  Tests âœ… (252 passed, coverage 82%)
           Lint âœ… (Black, Flake8, Bandit OK)

Minute 3:  ML Pipeline âœ… (50 tests passed)

Minute 8:  Docker Build âœ… (5 images built, integration tests passed)

Minute 8:  ALL CHECKS PASSED âœ…
           â†’ Pull Request "ready to merge" (green checkmark)

Minute 9:  Developer clicks "Merge Pull Request"
           â†’ Code merged to main

Minute 10: Main branch CI/CD triggered
           â†’ Coverage badge generated
           â†’ Docker images tagged "latest"

Minute 15: Production deployment
           â†’ docker-compose up -d
           â†’ New version live âœ…
```

---

### ğŸ¯ Avantages CI/CD

**Sans CI/CD:**

```
Developer â†’ Push code â†’ Manual testing (30 min) â†’ Hope it works â†’ Production ğŸ¤
```

**Avec CI/CD:**

```
Developer â†’ Push code â†’ Auto tests (252 tests, 8 min) â†’ Guaranteed quality âœ… â†’ Production ğŸš€
```

**BÃ©nÃ©fices:**

1. âœ… **DÃ©tection bugs immÃ©diate** - Tests auto Ã  chaque commit
2. âœ… **Code quality garanti** - Linters obligatoires
3. âœ… **SÃ©curitÃ© vÃ©rifiÃ©e** - Bandit + Safety scans
4. âœ… **IntÃ©gration validÃ©e** - Docker compose tests
5. âœ… **ML quality** - Accuracy > 80% requis
6. âœ… **Feedback rapide** - 8 minutes au lieu de heures
7. âœ… **Confiance dÃ©ploiement** - Si CI/CD vert, production OK
8. âœ… **Documentation vivante** - Coverage badge, test reports

---

**VoilÃ  ! Vous avez une comprÃ©hension complÃ¨te du CI/CD GitHub Actions.**

**CrÃ©Ã© le:** 27 janvier 2026
**Pour:** Certification RNCP E1/E3
**CompÃ©tence:** C13 - MLOps CI/CD âœ…

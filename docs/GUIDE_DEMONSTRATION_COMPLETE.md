# üéì Guide de D√©monstration Compl√®te - PredictionDex

**Date:** 27 janvier 2026
**Pour:** Soutenance Certification RNCP E1/E3
**Dur√©e totale:** 25-30 minutes

---

## üìã Table des Mati√®res

1. [Pr√©paration Avant D√©monstration](#1-pr√©paration-avant-d√©monstration-5-min)
2. [D√©monstration Compl√®te (20 min)](#2-d√©monstration-compl√®te-20-min)
3. [Questions/R√©ponses Anticip√©es](#3-questionsr√©ponses-anticip√©es)

---

## 1. Pr√©paration Avant D√©monstration (5 min)

### ‚úÖ Checklist Pr√©-D√©mo

```bash
# 1. V√©rifier que tout est arr√™t√©
cd /path/to/lets-go-predictiondex
docker-compose down -v

# 2. Nettoyer containers/volumes
docker system prune -f
docker volume prune -f

# 3. V√©rifier les fichiers requis
ls -lh models/battle_winner_model_v2.pkl         # Mod√®le ML
ls -lh data/datasets/X_train.parquet             # Reference data Evidently
ls -lh data/datasets/battles_dataset_v2.parquet  # Dataset ML

# 4. V√©rifier .env
cat .env
# Doit contenir:
# - POSTGRES_* (credentials DB)
# - API_KEYS (cl√© API)
# - MLFLOW_TRACKING_URI=http://mlflow:5000

# 5. D√©marrer l'infrastructure compl√®te
docker-compose up -d

# 6. Attendre que tout soit pr√™t (2 minutes)
echo "‚è≥ Attente services (2 min)..."
sleep 120

# 7. V√©rifier les services
docker-compose ps
# Tous doivent √™tre "Up" (healthy)
```

### üîç V√©rification Rapide Endpoints

```bash
# API
curl http://localhost:8080/health
# R√©ponse: {"status":"healthy"}

# MLflow
curl http://localhost:5001/health
# R√©ponse: {"status":"ok"}

# Prometheus
curl http://localhost:9091/-/healthy
# R√©ponse: "Prometheus is Healthy."

# Streamlit
curl http://localhost:8502
# R√©ponse: HTML (page charg√©e)

# Grafana
curl http://localhost:3001/api/health
# R√©ponse: {"commit":"...","database":"ok"}
```

**Si un service est down:**
```bash
# V√©rifier les logs
docker-compose logs api
docker-compose logs mlflow
docker-compose logs streamlit

# Red√©marrer le service probl√©matique
docker-compose restart api
```

---

## 2. D√©monstration Compl√®te (20 min)

### üéØ Plan de D√©monstration

| √âtape | Dur√©e | Objectif | Comp√©tences |
|-------|-------|----------|-------------|
| 1. Architecture Projet | 2 min | Vue d'ensemble | E1, E3 |
| 2. Pipeline ETL | 3 min | Collecte donn√©es | C1, C2, C3 |
| 3. Base de Donn√©es | 2 min | Stockage structur√© | C4 |
| 4. Machine Learning | 4 min | Mod√®le IA | C12 |
| 5. API REST | 3 min | Exposition mod√®le | C9, C10 |
| 6. Interface Streamlit | 2 min | Application finale | C10 |
| 7. Monitoring | 3 min | M√©triques + Drift | C11 |
| 8. CI/CD MLOps | 3 min | Livraison continue | C13 |

---

### √âtape 1: Architecture Projet (2 min)

**Objectif:** Montrer la vue d'ensemble du projet

**Script:**

> "PredictionDex est une application MLOps compl√®te pour pr√©dire le meilleur coup Pok√©mon.
> L'architecture suit une approche microservices avec 9 conteneurs Docker."

**Montrer:**

```bash
# 1. Afficher l'arborescence
tree -L 2 -I '.venv|__pycache__|node_modules'

# 2. Montrer docker-compose.yml
cat docker-compose.yml | head -40
```

**Expliquer les 9 services:**

```yaml
services:
  db:           # PostgreSQL - Base de donn√©es
  etl:          # Pipeline ETL - Collecte donn√©es
  ml_builder:   # Training ML - Entra√Ænement mod√®le
  api:          # FastAPI - Exposition mod√®le IA
  streamlit:    # Interface utilisateur
  mlflow:       # Model Registry - Versioning mod√®le
  prometheus:   # M√©triques temps r√©el
  grafana:      # Dashboards visualisation
  node-exporter:# M√©triques syst√®me
```

**Diagramme √† montrer:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Pok√©API     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ ETL Pipeline ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ PostgreSQL  ‚îÇ
‚îÇ Pokepedia   ‚îÇ     ‚îÇ (Scrapy)     ‚îÇ     ‚îÇ (11 tables) ‚îÇ
‚îÇ CSV Files   ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                ‚îÇ
                                               ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ  ML Builder  ‚îÇ
            ‚îÇ  (XGBoost)   ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ MLflow       ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ FastAPI     ‚îÇ
            ‚îÇ (Registry)   ‚îÇ     ‚îÇ (Predict)   ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                       ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚ñº                  ‚ñº                  ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ Streamlit    ‚îÇ   ‚îÇ Prometheus  ‚îÇ  ‚îÇ Evidently    ‚îÇ
            ‚îÇ (Interface)  ‚îÇ   ‚îÇ (Metrics)   ‚îÇ  ‚îÇ (Drift)      ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                     ‚ñº
                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                              ‚îÇ Grafana     ‚îÇ
                              ‚îÇ (Dashboards)‚îÇ
                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### √âtape 2: Pipeline ETL (3 min) - **C1, C2, C3**

**Objectif:** D√©montrer la collecte et nettoyage des donn√©es

#### C1: Extraction Donn√©es Automatis√©e

**Script:**

> "Le pipeline ETL collecte automatiquement les donn√©es depuis 3 sources h√©t√©rog√®nes."

**Montrer le code ETL:**

```bash
# 1. Scraper Pokepedia (scraping web)
cat etl_pokemon/pokepedia_scraper/pokepedia_scraper/spiders/lgpe_moves_sql_spider.py | head -50
```

**Expliquer:**

```python
class LgpeMovesSpider(scrapy.Spider):
    """Spider Scrapy pour scraper les capacit√©s Pokepedia"""

    name = 'lgpe_moves_sql'
    start_urls = ['https://www.pokepedia.fr/Liste_des_capacit√©s']

    def parse(self, response):
        # Extraction HTML table ‚Üí SQL inserts
        for row in response.css('table.sortable tr'):
            move_name = row.css('td:nth-child(2) a::text').get()
            move_type = row.css('td:nth-child(3)::text').get()
            # ... extraction 10+ champs
            yield {
                'name': move_name,
                'type': move_type,
                'power': power,
                'accuracy': accuracy,
                # ...
            }
```

**Montrer l'appel Pok√©API:**

```bash
cat etl_pokemon/scripts/etl_enrich_pokeapi.py | grep -A 10 "def fetch_from_pokeapi"
```

```python
def fetch_from_pokeapi(pokemon_id: int):
    """Appel API REST Pok√©API pour enrichir donn√©es"""
    url = f"https://pokeapi.co/api/v2/pokemon/{pokemon_id}"
    response = requests.get(url, timeout=10)
    data = response.json()

    # Extraction stats, types, sprites
    return {
        'hp': data['stats'][0]['base_stat'],
        'attack': data['stats'][1]['base_stat'],
        # ... 6 stats
        'sprite_url': data['sprites']['front_default']
    }
```

**Montrer le CSV:**

```bash
head -5 data/csv/pokemon_species.csv
```

**R√©sultat:** 3 sources automatis√©es ‚úÖ

---

#### C2: Requ√™tes SQL Extraction

**Montrer les requ√™tes SQL:**

```bash
# 1. Exemple requ√™te complexe extraction
cat core/db/guards/pokemon.py | grep -A 20 "def get_pokemon_with_moves"
```

```python
def get_pokemon_with_moves(db: Session, pokemon_id: int):
    """Requ√™te SQL jointure multiple pour extraction donn√©es"""
    return (
        db.query(Pokemon)
        .options(
            joinedload(Pokemon.species),
            joinedload(Pokemon.stats),
            joinedload(Pokemon.types).joinedload(PokemonType.type),
            joinedload(Pokemon.moves).joinedload(PokemonMove.move)
        )
        .filter(Pokemon.id == pokemon_id)
        .first()
    )
```

**Expliquer:**
- ‚úÖ Requ√™te SQL avec jointures (4 tables)
- ‚úÖ Optimisation eager loading (N+1 √©vit√©)
- ‚úÖ Extraction compl√®te donn√©es Pok√©mon

---

#### C3: Agr√©gation et Nettoyage

**Montrer le script d'agr√©gation:**

```bash
cat etl_pokemon/scripts/etl_post_process.py | head -80
```

**Expliquer les √©tapes:**

```python
def aggregate_and_clean_data(db: Session):
    """Agr√©gation multi-sources + nettoyage"""

    # 1. Suppression entr√©es corrompues
    corrupted = db.query(Pokemon).filter(Pokemon.stats == None).all()
    for p in corrupted:
        db.delete(p)

    # 2. Homog√©n√©isation formats
    for move in db.query(Move).all():
        # Normaliser noms (accents, casse)
        move.name = normalize_move_name(move.name)

        # Convertir types string ‚Üí ID
        move.type_id = get_or_create_type(db, move.type_name)

    # 3. Agr√©gation affinit√©s types
    for type_a in all_types:
        for type_b in all_types:
            multiplier = calculate_effectiveness(type_a, type_b)
            # Ins√©rer dans table type_effectiveness
            db.add(TypeEffectiveness(
                attacking_type_id=type_a.id,
                defending_type_id=type_b.id,
                multiplier=multiplier
            ))

    db.commit()
```

**R√©sultat:** Donn√©es nettoy√©es et agr√©g√©es ‚úÖ

---

### √âtape 3: Base de Donn√©es (2 min) - **C4, C5**

#### C4: Base de Donn√©es PostgreSQL

**Script:**

> "La base PostgreSQL est normalis√©e 3NF avec 11 tables et contraintes d'int√©grit√©."

**Montrer le sch√©ma:**

```bash
# Connexion PostgreSQL
docker-compose exec db psql -U letsgo_user -d letsgo_db

# Liste tables
\dt

# Sch√©ma table pokemon
\d pokemon

# Sch√©ma table type_effectiveness
\d type_effectiveness

# Quitter
\q
```

**Expliquer la normalisation:**

```sql
-- Table pokemon (entit√© principale)
CREATE TABLE pokemon (
    id SERIAL PRIMARY KEY,
    species_id INT REFERENCES pokemon_species(id),
    form_id INT REFERENCES forms(id),
    sprite_url VARCHAR(255),
    UNIQUE(species_id, form_id)  -- Contrainte unicit√©
);

-- Table pokemon_type (relation N-M)
CREATE TABLE pokemon_type (
    pokemon_id INT REFERENCES pokemon(id) ON DELETE CASCADE,
    type_id INT REFERENCES types(id),
    slot INT CHECK (slot IN (1, 2)),
    PRIMARY KEY (pokemon_id, slot)
);

-- Table type_effectiveness (matrice affinit√©s)
CREATE TABLE type_effectiveness (
    attacking_type_id INT REFERENCES types(id),
    defending_type_id INT REFERENCES types(id),
    multiplier FLOAT CHECK (multiplier IN (0, 0.25, 0.5, 1, 2, 4)),
    PRIMARY KEY (attacking_type_id, defending_type_id)
);
```

**Montrer les donn√©es:**

```sql
-- Compter Pok√©mon
SELECT COUNT(*) FROM pokemon;
-- R√©sultat: 188

-- Compter capacit√©s
SELECT COUNT(*) FROM moves;
-- R√©sultat: 226

-- Afficher affinit√©s type Feu
SELECT
    t1.name AS attacking_type,
    t2.name AS defending_type,
    te.multiplier
FROM type_effectiveness te
JOIN types t1 ON te.attacking_type_id = t1.id
JOIN types t2 ON te.defending_type_id = t2.id
WHERE t1.name = 'feu'
ORDER BY te.multiplier DESC;
```

**R√©sultat:** Base normalis√©e 3NF ‚úÖ (C4)

---

#### C5: Partage Donn√©es (API)

**Script:**

> "L'API FastAPI expose les donn√©es via interface REST."

**Montrer Swagger:**

```bash
# Ouvrir navigateur
firefox http://localhost:8080/docs
```

**D√©montrer les endpoints:**

1. **GET /pokemon/** - Liste tous les Pok√©mon
   ```bash
   curl http://localhost:8080/pokemon/ | jq '.[0:2]'
   ```

2. **GET /pokemon/25** - D√©tails Pikachu
   ```bash
   curl http://localhost:8080/pokemon/25 | jq
   ```

3. **GET /types/affinities** - Matrice affinit√©s
   ```bash
   curl http://localhost:8080/types/affinities | jq '.[0:5]'
   ```

**R√©sultat:** Donn√©es partag√©es via API REST ‚úÖ (C5)

---

### √âtape 4: Machine Learning (4 min) - **C12**

**Objectif:** D√©montrer le pipeline ML complet

#### Pipeline ML Complet

**Script:**

> "Le mod√®le XGBoost est entra√Æn√© sur 898,472 combats simul√©s avec 88.23% d'accuracy."

**Montrer le pipeline ML:**

```bash
# 1. Structure fichiers ML
ls -lh machine_learning/
```

```
run_machine_learning.py     # Pipeline ML principal (1239 lignes)
train_model.py              # Wrapper entra√Ænement
build_battle_winner_dataset_v2.py  # G√©n√©ration dataset
mlflow_integration.py       # MLflow tracking
```

**Montrer le code principal:**

```bash
cat machine_learning/run_machine_learning.py | grep -A 30 "def run_dataset_preparation"
```

**Expliquer les √©tapes:**

```python
def run_machine_learning_pipeline():
    """Pipeline ML complet"""

    # 1. Pr√©paration dataset (898,472 combats)
    X_train, X_test, y_train, y_test = run_dataset_preparation()

    # 2. Feature engineering (133 features)
    X_train_eng = engineer_features(X_train)
    X_test_eng = engineer_features(X_test)

    # 3. Training XGBoost
    model = train_model(X_train_eng, y_train, hyperparams={
        'n_estimators': 100,
        'max_depth': 6,
        'learning_rate': 0.1,
        'tree_method': 'hist',  # CPU optimis√©
        'n_jobs': -1            # Tous les cores
    })

    # 4. √âvaluation
    metrics = evaluate_model(model, X_test_eng, y_test)
    # R√©sultat: 88.23% accuracy

    # 5. Export mod√®le (compression Joblib)
    export_model(model, scalers, metadata, version='v2')

    # 6. Enregistrement MLflow
    tracker.log_model(model, artifact_path='model')
    tracker.register_model(model_name='battle_winner_predictor')
    tracker.promote_best_model(metric='test_accuracy', minimum=0.80)
```

**Montrer le dataset:**

```bash
# Taille dataset
ls -lh data/datasets/battles_dataset_v2.parquet
# ~220 MB (898,472 combats)

# Explorer dataset
python3 << EOF
import pandas as pd
df = pd.read_parquet('data/datasets/battles_dataset_v2.parquet')
print(f"Combats: {len(df):,}")
print(f"Features: {df.shape[1]}")
print(f"\nColonnes:\n{df.columns.tolist()[:20]}")
print(f"\nDistribution gagnants:\n{df['winner'].value_counts()}")
EOF
```

**R√©sultat attendu:**

```
Combats: 898,472
Features: 39

Colonnes:
['a_hp', 'a_attack', 'a_defense', 'a_sp_attack', 'a_sp_defense',
 'a_speed', 'a_type_1', 'a_type_2', 'b_hp', 'b_attack', ...]

Distribution gagnants:
1    456,234  (Pokemon A gagne)
0    442,238  (Pokemon B gagne)
```

---

#### C12: Tests Automatis√©s ML

**Montrer les tests ML:**

```bash
# 1. Structure tests ML
ls -lh tests/ml/
```

```
test_dataset_preparation.py      # Tests dataset (25 tests)
test_feature_engineering.py      # Tests features (15 tests)
test_model_training.py           # Tests entra√Ænement (10 tests)
```

**Ex√©cuter les tests:**

```bash
# Tests ML
pytest tests/ml/ -v

# R√©sultat:
# ========================= 50 passed in 12.34s =========================
```

**Montrer un test exemple:**

```bash
cat tests/ml/test_feature_engineering.py | head -40
```

```python
def test_engineer_features_output_shape():
    """Test: feature engineering produit 133 features"""
    df_raw = create_mock_battle_df()  # 38 features brutes

    df_engineered = engineer_features(df_raw)

    assert df_engineered.shape[1] == 133  # 133 features engineered
    assert 'effective_power_a' in df_engineered.columns
    assert 'stat_ratio' in df_engineered.columns

def test_model_accuracy_threshold():
    """Test: accuracy > 80% requis"""
    model, metrics = train_and_evaluate_model()

    assert metrics['test_accuracy'] > 0.80
    assert metrics['test_roc_auc'] > 0.85
```

**R√©sultat:** Tests automatis√©s ML ‚úÖ (C12)

---

### √âtape 5: API REST (3 min) - **C9, C10**

#### C9: API REST Exposant Mod√®le IA

**Script:**

> "L'API FastAPI expose le mod√®le XGBoost via endpoint /predict/best-move."

**Ouvrir Swagger:**

```bash
firefox http://localhost:8080/docs
```

**Montrer l'endpoint de pr√©diction:**

1. **Cliquer sur POST /predict/best-move**
2. **Cliquer "Try it out"**
3. **Remplir le JSON:**

```json
{
  "pokemon_a_id": 25,
  "pokemon_b_id": 1,
  "available_moves": ["Fatal-Foudre", "Vive-Attaque", "Queue de Fer", "Tonnerre"]
}
```

4. **Cliquer "Execute"**

**R√©sultat attendu:**

```json
{
  "pokemon_a_id": 25,
  "pokemon_a_name": "Pikachu",
  "pokemon_b_id": 1,
  "pokemon_b_name": "Bulbizarre",
  "recommended_move": "Fatal-Foudre",
  "win_probability": 0.8734,
  "all_moves": [
    {
      "move_name": "Fatal-Foudre",
      "move_type": "√©lectrik",
      "move_power": 150,
      "type_multiplier": 1.0,
      "stab": 1.5,
      "win_probability": 0.8734,
      "predicted_winner": "A"
    },
    {
      "move_name": "Tonnerre",
      "move_type": "√©lectrik",
      "move_power": 110,
      "type_multiplier": 1.0,
      "stab": 1.5,
      "win_probability": 0.8456,
      "predicted_winner": "A"
    }
  ]
}
```

**Montrer le code API:**

```bash
cat api_pokemon/routes/prediction_route.py | head -110
```

**Expliquer:**

```python
@router.post("/best-move", response_model=PredictBestMoveResponse)
def predict_best_move(request: PredictBestMoveRequest, db: Session):
    """
    Pr√©diction ML via API REST.

    S√©curit√©:
    - API Key requise (SHA-256)
    - Validation Pydantic schema
    - Rate limiting (30 req/min)

    Monitoring:
    - M√©triques Prometheus
    - Drift detection Evidently
    - Logs structur√©s
    """
    # 1. Validation input (Pydantic)
    # 2. Load mod√®le depuis MLflow Registry
    # 3. Pr√©diction XGBoost
    # 4. Track m√©triques Prometheus
    # 5. Log drift Evidently
    # 6. Retour JSON
```

**Tester avec curl:**

```bash
curl -X POST http://localhost:8080/predict/best-move \
  -H "Content-Type: application/json" \
  -H "X-API-Key: YOUR_API_KEY" \
  -d '{
    "pokemon_a_id": 25,
    "pokemon_b_id": 6,
    "available_moves": ["Fatal-Foudre", "Tonnerre"]
  }' | jq
```

**R√©sultat:** API REST exposant IA ‚úÖ (C9)

---

#### C10: Int√©gration API dans Application

**Script:**

> "L'interface Streamlit int√®gre l'API pour fournir une exp√©rience utilisateur."

**Ouvrir Streamlit:**

```bash
firefox http://localhost:8502
```

**D√©monstration interactive:**

1. **Page d'accueil** - Pr√©sentation projet
2. **Aller dans "Combat et Pr√©diction"** (menu gauche)
3. **S√©lectionner:**
   - Ton Pok√©mon: Pikachu (#25)
   - Adversaire: Dracaufeu (#6)
   - Capacit√©s: Fatal-Foudre, Tonnerre, Vive-Attaque, Queue de Fer
4. **Cliquer "Pr√©dire"**

**R√©sultat affich√©:**

```
‚úÖ Capacit√© recommand√©e: Fatal-Foudre

Probabilit√© de victoire: 87.3%

Toutes les capacit√©s test√©es:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Capacit√©        ‚îÇ Probabilit√© ‚îÇ Type               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Fatal-Foudre    ‚îÇ 87.3%       ‚îÇ √©lectrik (√ó1.0)    ‚îÇ
‚îÇ Tonnerre        ‚îÇ 84.6%       ‚îÇ √©lectrik (√ó1.0)    ‚îÇ
‚îÇ Vive-Attaque    ‚îÇ 45.2%       ‚îÇ normal (√ó1.0)      ‚îÇ
‚îÇ Queue de Fer    ‚îÇ 38.7%       ‚îÇ acier (√ó0.5)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Montrer le code Streamlit:**

```bash
cat interface/services/api_client.py | grep -A 20 "def predict_best_move"
```

```python
def predict_best_move(
    pokemon_a_id: int,
    pokemon_b_id: int,
    available_moves: List[str]
) -> Dict:
    """Appel API REST depuis Streamlit."""
    payload = {
        "pokemon_a_id": pokemon_a_id,
        "pokemon_b_id": pokemon_b_id,
        "available_moves": available_moves
    }

    response = requests.post(
        f"{API_BASE_URL}/predict/best-move",
        json=payload,
        headers={"X-API-Key": API_KEY},
        timeout=60
    )

    return response.json()
```

**R√©sultat:** API int√©gr√©e dans app ‚úÖ (C10)

---

### √âtape 6: Interface Streamlit (2 min)

**Script:**

> "L'interface Streamlit propose 8 pages interactives pour l'utilisateur final."

**Tour rapide des pages:**

1. **üè† Accueil** - Pr√©sentation projet, features, guide
2. **1. Capacit√©s** - Catalogue 226 capacit√©s avec filtres
3. **2. Combat et Pr√©diction** - ‚≠ê Pr√©diction ML principale
4. **3. D√©tails Pok√©mon** - Fiches 188 Pok√©mon
5. **4. Types et Affinit√©s** - Matrice 18√ó18 types
6. **5. Quiz Types** - Quiz interactif affinit√©s
7. **6. Cr√©dits** - Sources donn√©es, remerciements
8. **10. API Documentation** - Guide utilisation API

**Montrer quelques fonctionnalit√©s:**

- **Filtres dynamiques** (types, puissance, cat√©gorie)
- **Visualisations** (stats radar, heatmap affinit√©s)
- **Th√®me Pok√©mon** (couleurs types, sprites anim√©s)

---

### √âtape 7: Monitoring (3 min) - **C11**

**Objectif:** D√©montrer le monitoring complet du mod√®le IA

#### 7.1 Prometheus (M√©triques Temps R√©el)

**Ouvrir Prometheus:**

```bash
firefox http://localhost:9091
```

**Montrer les m√©triques cl√©s:**

1. **Dans la barre de recherche, taper:**

   ```promql
   # Nombre total de pr√©dictions
   model_predictions_total

   # Latence P95 des pr√©dictions
   histogram_quantile(0.95, rate(model_prediction_duration_seconds_bucket[5m]))

   # Distribution probabilit√©s victoire
   model_win_probability

   # Confiance mod√®le
   model_confidence_score
   ```

2. **Cliquer "Execute" puis "Graph"**

**Expliquer les m√©triques:**

```python
# api_pokemon/monitoring/metrics.py

# M√©triques API
api_requests_total          # Counter - Nombre requ√™tes
api_request_duration_seconds # Histogram - Latence requ√™tes
api_errors_total            # Counter - Erreurs

# M√©triques ML
model_predictions_total     # Counter - Pr√©dictions totales
model_prediction_duration_seconds  # Histogram - Latence mod√®le
model_confidence_score      # Gauge - Confiance (0-1)
model_win_probability       # Histogram - Distribution probas

# M√©triques syst√®me
system_cpu_usage_percent    # Gauge - CPU %
system_memory_usage_bytes   # Gauge - RAM utilis√©e
```

---

#### 7.2 Grafana (Dashboards)

**Ouvrir Grafana:**

```bash
firefox http://localhost:3001
# Login: admin / admin
```

**Montrer les dashboards:**

1. **Dashboard "API Performance"**
   - Request Rate (requ√™tes/sec)
   - Latency P50, P95, P99
   - Error Rate
   - Status codes distribution

2. **Dashboard "Model Performance"**
   - Predictions per second
   - Model latency histogram
   - Confidence score over time
   - Win probability distribution

**Cr√©er un panel en direct (optionnel):**

```promql
# Panel: Latence P95 API
histogram_quantile(0.95,
  rate(api_request_duration_seconds_bucket{endpoint="/predict/best-move"}[5m])
)
```

---

#### 7.3 Evidently AI (Drift Detection)

**Script:**

> "Evidently d√©tecte automatiquement le drift des features du mod√®le."

**Montrer le code drift:**

```bash
cat api_pokemon/monitoring/drift_detection.py | head -100
```

**Expliquer le fonctionnement:**

```python
class DriftDetector:
    """Singleton drift detection avec Evidently AI 0.7"""

    def __init__(self):
        # 1. Load reference data (training set)
        self.reference_data = Dataset.from_pandas(
            pd.read_parquet('data/datasets/X_train.parquet').sample(10000)
        )

        # 2. Buffer production predictions
        self.production_buffer = []  # Max 1000 predictions

        # 3. Auto-report every hour
        self.report_frequency = timedelta(hours=1)

    def add_prediction(self, features, prediction, probability):
        """
        Ajout pr√©diction au buffer.

        D√©clenche automatiquement:
        - Report drift si buffer plein (1000 predictions)
        - Report drift si 1h √©coul√©e
        """
        self.production_buffer.append({
            **features,
            'prediction': prediction,
            'probability': probability,
            'timestamp': datetime.now()
        })

        # Auto-generate report si conditions remplies
        if len(self.production_buffer) >= 1000:
            self.generate_drift_report()

    def generate_drift_report(self):
        """
        G√©n√®re rapport drift avec Evidently.

        Outputs:
        - HTML dashboard interactif
        - JSON metrics
        - Production data sauvegard√©e (parquet)
        """
        production_df = pd.DataFrame(self.production_buffer)
        production_dataset = Dataset.from_pandas(production_df)

        # Evidently Report
        report = Report([DataDriftPreset()])
        report.run(production_dataset, self.reference_data)

        # Save HTML
        report.save_html('drift_dashboard_{timestamp}.html')

        # Extract metrics
        drift_summary = {
            'n_features': ...,
            'n_drifted_features': ...,
            'share_drifted_features': ...,
            'dataset_drift': True/False
        }

        return drift_summary
```

**Montrer un rapport drift:**

```bash
# Lister les rapports g√©n√©r√©s
ls -lh api_pokemon/monitoring/drift_reports/

# Ouvrir le dernier rapport HTML
firefox api_pokemon/monitoring/drift_reports/drift_dashboard_$(ls -t api_pokemon/monitoring/drift_reports/ | head -1)
```

**Expliquer le rapport Evidently:**

- üìä **Dataset Summary** - Nombre features, samples
- üîç **Dataset Drift** - Drift d√©tect√© ou non (True/False)
- üìà **Feature Drift** - Liste features drift√©es (ex: 5/133 features)
- üìâ **Drift Score** - Score 0-1 pour chaque feature
- üìä **Distribution Plots** - Histogrammes reference vs production

**R√©sultat:** Monitoring complet mod√®le IA ‚úÖ (C11)

---

### √âtape 8: CI/CD MLOps (3 min) - **C13**

**Objectif:** D√©montrer la cha√Æne de livraison continue

#### GitHub Actions Workflows

**Ouvrir GitHub:**

```bash
firefox https://github.com/votre-username/lets-go-predictiondex/actions
```

**Montrer les 4 workflows:**

1. **Tests** (.github/workflows/tests.yml)
2. **Docker Build** (.github/workflows/docker-build.yml)
3. **ML Pipeline** (.github/workflows/ml-pipeline.yml)
4. **Lint & Security** (.github/workflows/lint.yml)

**Expliquer chaque workflow:**

---

#### Workflow 1: Tests

**Montrer le fichier:**

```bash
cat .github/workflows/tests.yml
```

**Expliquer:**

```yaml
name: Tests

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:15
        # Health checks automatiques

    steps:
      - Checkout code
      - Setup Python 3.11
      - Cache pip dependencies
      - Install requirements
      - Run pytest (252 tests)
      - Generate coverage (82%)
      - Upload to Codecov
      - Generate coverage badge
```

**R√©sultat:** Tests automatiques sur chaque commit ‚úÖ

---

#### Workflow 2: Docker Build

**Expliquer:**

```yaml
name: Docker Build

jobs:
  build-and-test:
    strategy:
      matrix:
        service: [api, etl, ml, streamlit, mlflow]  # Build parall√®le

    steps:
      - Docker Buildx setup
      - Cache Docker layers (performance)
      - Build image
      - Save + upload artifact

  integration-test:
    needs: build-and-test

    steps:
      - Download all artifacts
      - Load Docker images
      - docker-compose up -d
      - Health checks (API, MLflow, Prometheus)
      - Run integration tests
      - Logs si √©chec
```

**R√©sultat:** Build + tests int√©gration automatiques ‚úÖ

---

#### Workflow 3: ML Pipeline

**Expliquer:**

```yaml
name: ML Pipeline

on:
  push:
    paths:
      - 'machine_learning/**'
      - 'data/ml/**'
  workflow_dispatch:  # Trigger manuel

jobs:
  test-ml:
    services:
      postgres: ...
      mlflow: ...  # MLflow server int√©gr√©

    steps:
      - Run ML tests (50 tests)

      - Train model (si manuel)
        python run_machine_learning.py --mode=train

      - Validate metrics (accuracy > 80%)
        assert metrics['test_accuracy'] > 0.80

      - Upload model artifacts (90 jours)

      - Comment PR avec m√©triques
```

**D√©clencher un training manuel:**

1. **Actions ‚Üí ML Pipeline**
2. **Run workflow**
3. **Param√®tres:**
   - dataset_version: v2
   - model_version: ci_test
4. **Run workflow**

**R√©sultat:** Pipeline ML automatis√© ‚úÖ

---

#### Workflow 4: Lint & Security

**Expliquer:**

```yaml
name: Lint and Format

jobs:
  lint:
    - Black (code formatting)
    - isort (imports sorting)
    - Flake8 (style guide PEP8)
    - Pylint (code quality)
    - Mypy (type checking)

  security:
    - Bandit (security linter)
    - Safety (dependency vulnerabilities)
    - Upload security reports
```

**R√©sultat:** Qualit√© code + s√©curit√© automatiques ‚úÖ

---

### R√©sum√© D√©monstration (1 min)

**Script de conclusion:**

> "En r√©sum√©, PredictionDex est un projet MLOps complet qui d√©montre:
>
> **E1 - Collecte et Traitement Donn√©es:**
> - ‚úÖ ETL automatis√© (3 sources: Pok√©API, Pokepedia, CSV)
> - ‚úÖ Base PostgreSQL normalis√©e 3NF (11 tables)
> - ‚úÖ Agr√©gation et nettoyage donn√©es (898,472 combats)
> - ‚úÖ API REST pour partage donn√©es
>
> **E3 - Int√©gration IA Production:**
> - ‚úÖ Mod√®le XGBoost 88.23% accuracy (133 features)
> - ‚úÖ API REST exposant mod√®le IA (FastAPI + Swagger)
> - ‚úÖ Interface Streamlit int√©gr√©e (8 pages)
> - ‚úÖ Monitoring complet (Prometheus + Grafana + Evidently)
> - ‚úÖ Tests automatis√©s ML (252 tests, coverage 82%)
> - ‚úÖ CI/CD MLOps (4 workflows GitHub Actions)
> - ‚úÖ MLflow Model Registry (auto-promotion)
>
> Le projet est production-ready avec 9 services Docker orchestr√©s."

---

## 3. Questions/R√©ponses Anticip√©es

### Q1: "Comment garantissez-vous la qualit√© des donn√©es collect√©es ?"

**R√©ponse:**

> "Nous avons 3 m√©canismes de validation:
>
> 1. **Guards Pydantic** - Validation sch√©ma lors de l'insertion
>    ```python
>    class PokemonGuard(BaseModel):
>        species_id: int
>        hp: int = Field(ge=1, le=255)
>        attack: int = Field(ge=1, le=255)
>        # ...
>    ```
>
> 2. **Contraintes SQL** - Int√©grit√© base de donn√©es
>    ```sql
>    CHECK (hp BETWEEN 1 AND 255)
>    FOREIGN KEY (species_id) REFERENCES pokemon_species(id)
>    UNIQUE (species_id, form_id)
>    ```
>
> 3. **Post-processing** - Nettoyage apr√®s collecte
>    - Suppression entr√©es NULL/corrompues
>    - D√©doublonnage
>    - Normalisation formats (accents, casse)"

---

### Q2: "Comment g√©rez-vous le versioning des mod√®les ML ?"

**R√©ponse:**

> "Nous utilisons MLflow Model Registry avec 3 stages:
>
> 1. **None** - Mod√®le entra√Æn√©, pas encore enregistr√©
> 2. **Staging** - Mod√®le en test
> 3. **Production** - Mod√®le servi par l'API
> 4. **Archived** - Anciennes versions
>
> **Auto-promotion intelligente:**
> ```python
> if test_accuracy >= 0.85:
>     promote_to_production(model_version)
>     archive_old_production_models()
> ```
>
> **Fallback automatique:**
> Si MLflow indisponible, l'API charge le mod√®le depuis fichiers locaux."

---

### Q3: "Comment d√©tectez-vous la d√©gradation du mod√®le ?"

**R√©ponse:**

> "Nous utilisons Evidently AI pour d√©tecter le drift:
>
> 1. **Reference data** - 10,000 √©chantillons training set
> 2. **Production buffer** - 1,000 derni√®res pr√©dictions
> 3. **Auto-reports** - G√©n√©ration automatique chaque heure
>
> **M√©triques drift:**
> - Dataset drift: True/False
> - Features drift√©es: 5/133 (3.7%)
> - Drift score par feature (0-1)
>
> **Actions si drift d√©tect√©:**
> - Alert √©quipe ML
> - Retraining mod√®le avec nouvelles donn√©es
> - A/B test nouveau mod√®le vs ancien"

---

### Q4: "Quelle est votre strat√©gie de tests ?"

**R√©ponse:**

> "Pyramide de tests √† 3 niveaux:
>
> **1. Tests Unitaires (200 tests)**
> - Services API (64 tests)
> - Core models (15 tests)
> - ML pipeline (50 tests)
> - MLflow (17 tests)
>
> **2. Tests Int√©gration (9 tests)**
> - MLflow ‚Üí API
> - API ‚Üí PostgreSQL
> - End-to-end predictions
>
> **3. Tests Syst√®me (CI/CD)**
> - Health checks services
> - Docker compose up
> - Smoke tests
>
> **Coverage: 82%** (cible: 80%+)"

---

### Q5: "Comment s√©curisez-vous l'API ?"

**R√©ponse:**

> "3 couches de s√©curit√©:
>
> **1. Authentification API Key**
> ```python
> # SHA-256 hashing (jamais plaintext)
> valid_keys = {hashlib.sha256(key.encode()).hexdigest()
>               for key in os.getenv('API_KEYS').split(',')}
> ```
>
> **2. Validation Input**
> ```python
> # Pydantic schemas
> class PredictRequest(BaseModel):
>     pokemon_a_id: int = Field(ge=1, le=188)
>     pokemon_b_id: int = Field(ge=1, le=188)
>     available_moves: List[str] = Field(min_items=1, max_items=4)
> ```
>
> **3. Security Scanning**
> - Bandit (code vulnerabilities)
> - Safety (dependencies CVEs)
> - GitHub Actions automatique"

---

### Q6: "Combien de temps pour d√©ployer en production ?"

**R√©ponse:**

> "**1 commande, 2 minutes:**
>
> ```bash
> # Clone repo
> git clone https://github.com/you/lets-go-predictiondex
> cd lets-go-predictiondex
>
> # Configure .env
> cp .env.example .env
> nano .env  # Ajouter API_KEYS, credentials
>
> # Deploy
> docker-compose up -d
>
> # Attendre 2 minutes (services ready)
> # ‚úÖ API: http://localhost:8080
> # ‚úÖ Streamlit: http://localhost:8502
> # ‚úÖ MLflow: http://localhost:5001
> # ‚úÖ Grafana: http://localhost:3001
> ```
>
> **Rollback instantan√©:**
> ```bash
> docker-compose down
> git checkout v1.9.0
> docker-compose up -d
> ```"

---

## üéØ Checklist Finale D√©monstration

### Avant la Soutenance

- [ ] Tester docker-compose up -d (d√©marrage clean)
- [ ] V√©rifier tous les endpoints (health checks)
- [ ] Pr√©parer 2-3 exemples pr√©dictions
- [ ] G√©n√©rer un rapport drift Evidently frais
- [ ] V√©rifier que les dashboards Grafana s'affichent
- [ ] Tester l'interface Streamlit (toutes les pages)
- [ ] V√©rifier les logs (pas d'erreurs critiques)

### Pendant la Soutenance

- [ ] Parler clairement et lentement
- [ ] Montrer le code source (pas juste les r√©sultats)
- [ ] Expliquer les choix techniques (pourquoi XGBoost, PostgreSQL, etc.)
- [ ] Anticiper les questions (voir section Q&A)
- [ ] Garder un navigateur avec onglets pr√©-ouverts:
  - Swagger API (localhost:8080/docs)
  - Streamlit (localhost:8502)
  - Grafana (localhost:3001)
  - Prometheus (localhost:9091)
  - MLflow (localhost:5001)
  - GitHub Actions

### Apr√®s la D√©monstration

- [ ] Noter les questions pos√©es (pour am√©lioration)
- [ ] Demander feedback jury
- [ ] Proposer d√©mo live suppl√©mentaire si besoin

---

**Dur√©e totale:** 25-30 minutes
**Niveau:** Production-Ready
**Score attendu:** 9-10/10 pour E1/E3

---

**Cr√©√© le:** 27 janvier 2026
**Pour:** Certification RNCP Niveau 6 - Concepteur D√©veloppeur d'Applications
**Blocs:** E1 (Donn√©es) + E3 (IA Production)

# RAPPORT DE PROJET

---

<div align="center">

## Certification RNCP37827
### DÃ©veloppeur en Intelligence Artificielle

---

# **PredictionDex**
## PrÃ©dicteur de RÃ©sultats de Combats PokÃ©mon Let's Go

---

**Ã‰valuations :** E1 (Bloc 1 - Gestion des donnÃ©es) + E3 (Bloc 2 - Mise Ã  disposition de l'IA)

**CompÃ©tences visÃ©es :** C1 Ã  C5 + C9 Ã  C13

---

**Candidat :** [Votre PrÃ©nom NOM]

**Date de rendu :** [JJ/MM/AAAA]

**Organisme de formation :** Simplon

---

</div>

\newpage

---

## Sommaire

1. [Introduction et Contexte du Projet](#1-introduction-et-contexte-du-projet)
   - 1.1 PrÃ©sentation du projet
   - 1.2 Objectifs mÃ©tier et fonctionnels
   - 1.3 PÃ©rimÃ¨tre technique
   - 1.4 Organisation et mÃ©thodologie

2. [BLOC E1 : Collecte, Stockage et Mise Ã  Disposition des DonnÃ©es](#2-bloc-e1--collecte-stockage-et-mise-Ã -disposition-des-donnÃ©es)
   - 2.0 Analyse Exploratoire des DonnÃ©es (EDA)
   - 2.1 Automatisation de l'extraction des donnÃ©es (C1)
   - 2.2 RequÃªtes SQL d'extraction et transformation (C2)
   - 2.3 RÃ¨gles d'agrÃ©gation et nettoyage des donnÃ©es (C3)
   - 2.4 CrÃ©ation de la base de donnÃ©es (C4)
   - 2.5 API REST de mise Ã  disposition des donnÃ©es (C5)
   - 2.6 SÃ©curitÃ© de l'Application

3. [BLOC E3 : Mise Ã  Disposition de l'Intelligence Artificielle](#3-bloc-e3--mise-Ã -disposition-de-lintelligence-artificielle)
   - 3.1 DÃ©veloppement de l'API exposant le modÃ¨le (C9)
   - 3.2 IntÃ©gration dans l'application Streamlit (C10)
   - 3.3 Monitoring du modÃ¨le et dÃ©tection de dÃ©rives (C11)
   - 3.4 Tests automatisÃ©s du modÃ¨le (C12)
   - 3.5 ChaÃ®ne de livraison continue MLOps (C13)

4. [DÃ©monstration du Projet](#4-dÃ©monstration-du-projet)

5. [SynthÃ¨se et Perspectives](#5-synthÃ¨se-et-perspectives)
   - 5.1 Bilan technique
   - 5.2 DifficultÃ©s rencontrÃ©es et solutions
   - 5.3 Axes d'amÃ©lioration
   - 5.4 Conclusion

6. [Annexes](#6-annexes)

---

\newpage

---

# 1. Introduction et Contexte du Projet

## 1.1 PrÃ©sentation du projet

### Contexte

PredictionDex est une plateforme MLOps complÃ¨te permettant de prÃ©dire les rÃ©sultats de combats entre PokÃ©mon dans les jeux PokÃ©mon Let's Go Pikachu et Ã‰voli. Ce projet a Ã©tÃ© dÃ©veloppÃ© dans le cadre de la certification RNCP "DÃ©veloppeur en Intelligence Artificielle" et couvre l'ensemble du cycle de vie d'un projet data/IA : de la collecte des donnÃ©es jusqu'au dÃ©ploiement et monitoring d'un modÃ¨le de machine learning.

### ProblÃ©matique

> **Comment prÃ©dire de maniÃ¨re fiable le vainqueur d'un combat PokÃ©mon en exploitant les statistiques, types et attaques des combattants ?**

Cette problÃ©matique implique :
- La collecte et structuration de donnÃ©es PokÃ©mon depuis plusieurs sources
- L'entraÃ®nement d'un modÃ¨le de classification performant
- La mise Ã  disposition du modÃ¨le via une API et une interface utilisateur
- Le monitoring continu pour dÃ©tecter les dÃ©rives

### Solution dÃ©veloppÃ©e

PredictionDex rÃ©pond Ã  cette problÃ©matique par une architecture microservices complÃ¨te :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        PredictionDex                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Sources        â”‚  ETL Pipeline  â”‚  Base de     â”‚  API REST    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€        â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  DonnÃ©es     â”‚  â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚  â€¢ PokÃ©API      â”‚  â€¢ Scraping    â”‚  PostgreSQL  â”‚  FastAPI     â”‚
â”‚  â€¢ Pokepedia    â”‚  â€¢ Nettoyage   â”‚  â€¢ pokemon   â”‚  â€¢ /pokemon  â”‚
â”‚  â€¢ CSV datasets â”‚  â€¢ Validation  â”‚  â€¢ moves     â”‚  â€¢ /predict  â”‚
â”‚                 â”‚                â”‚  â€¢ battles   â”‚  â€¢ /health   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Machine Learning        â”‚  Interface       â”‚  Monitoring      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚
â”‚  â€¢ XGBoost (88.23%)      â”‚  Streamlit       â”‚  Prometheus      â”‚
â”‚  â€¢ MLflow tracking       â”‚  â€¢ PrÃ©dictions   â”‚  Grafana         â”‚
â”‚  â€¢ 2 versions modÃ¨le     â”‚  â€¢ Visualisation â”‚  Drift Detection â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1.2 Objectifs mÃ©tier et fonctionnels

### Objectifs mÃ©tier

| Objectif | Description | Indicateur de succÃ¨s |
|----------|-------------|---------------------|
| **PrÃ©diction fiable** | PrÃ©dire le vainqueur d'un combat | Accuracy â‰¥ 85% |
| **AccessibilitÃ©** | Interface simple pour les utilisateurs | Temps de prise en main < 5 min |
| **FiabilitÃ©** | Service disponible et stable | Uptime > 99% |
| **Ã‰volutivitÃ©** | CapacitÃ© Ã  intÃ©grer de nouvelles donnÃ©es | Architecture modulaire |

### Objectifs fonctionnels

1. **Collecte automatisÃ©e** des donnÃ©es PokÃ©mon depuis PokÃ©API et Pokepedia
2. **Stockage structurÃ©** dans une base PostgreSQL normalisÃ©e
3. **API REST** pour accÃ©der aux donnÃ©es et aux prÃ©dictions
4. **Interface utilisateur** intuitive (Streamlit)
5. **Monitoring temps rÃ©el** des performances du modÃ¨le
6. **Pipeline CI/CD** pour le dÃ©ploiement continu

---

## 1.3 PÃ©rimÃ¨tre technique

### Stack technologique

| Composant | Technologie | Version | Justification |
|-----------|-------------|---------|---------------|
| **Langage** | Python | 3.11 | Ã‰cosystÃ¨me ML mature, typage moderne |
| **API Backend** | FastAPI | 0.109 | Performance, async, documentation auto |
| **Base de donnÃ©es** | PostgreSQL | 15 | Robustesse, ACID, extensions JSON |
| **ORM** | SQLAlchemy | 2.0 | Abstraction BDD, migrations faciles |
| **ML Framework** | XGBoost | 2.0 | Performance, interprÃ©tabilitÃ© |
| **ML Toolkit** | scikit-learn | 1.4 | Preprocessing, mÃ©triques |
| **MLOps** | MLflow | 2.18 | Tracking, registry, reproductibilitÃ© |
| **Monitoring** | Prometheus + Grafana | 2.47 / 10.1 | MÃ©triques temps rÃ©el, alertes |
| **Frontend** | Streamlit | 1.39 | Prototypage rapide, interactif |
| **Conteneurisation** | Docker Compose | - | Orchestration multi-services |
| **CI/CD** | GitHub Actions | - | IntÃ©gration native GitHub |

### Architecture des services (9 conteneurs Docker)

```yaml
Services Docker Compose:
â”œâ”€â”€ db              # PostgreSQL 15 - Base de donnÃ©es principale
â”œâ”€â”€ api             # FastAPI - API REST (donnÃ©es + ML)
â”œâ”€â”€ etl             # Pipeline ETL - Collecte et transformation
â”œâ”€â”€ ml              # EntraÃ®nement des modÃ¨les
â”œâ”€â”€ mlflow          # MLflow Server - Tracking & Registry
â”œâ”€â”€ streamlit       # Interface utilisateur
â”œâ”€â”€ prometheus      # Collecte de mÃ©triques
â”œâ”€â”€ grafana         # Dashboards de monitoring
â””â”€â”€ pgadmin         # Administration PostgreSQL
```

---

## 1.4 Organisation et mÃ©thodologie

### MÃ©thodologie de dÃ©veloppement

Le projet suit une approche **Agile** avec des itÃ©rations courtes :

| Phase | DurÃ©e | Livrables |
|-------|-------|-----------|
| **Sprint 1** | 2 semaines | Pipeline ETL, BDD PostgreSQL |
| **Sprint 2** | 2 semaines | API REST donnÃ©es, tests unitaires |
| **Sprint 3** | 2 semaines | ModÃ¨le ML v1, MLflow |
| **Sprint 4** | 2 semaines | API ML, interface Streamlit |
| **Sprint 5** | 2 semaines | Monitoring, drift detection |
| **Sprint 6** | 1 semaine | CI/CD, documentation, tests finaux |

### Outils de gestion de projet

- **Versioning** : Git + GitHub
- **Gestion de tÃ¢ches** : GitHub Issues / Projects
- **Documentation** : Markdown + Swagger (OpenAPI)
- **Communication** : [Ã€ complÃ©ter selon votre contexte]

---

\newpage

---

# 2. BLOC E1 : Collecte, Stockage et Mise Ã  Disposition des DonnÃ©es

> **CompÃ©tences visÃ©es : C1, C2, C3, C4, C5**

---

## 2.0 Analyse Exploratoire des DonnÃ©es (EDA)

> Cette section prÃ©sente l'analyse exploratoire rÃ©alisÃ©e avant l'entraÃ®nement du modÃ¨le.
> Les figures sont gÃ©nÃ©rÃ©es par le script `scripts/generate_report_figures.py`.

### Distribution des statistiques des PokÃ©mon

L'analyse de la distribution des statistiques des 151 PokÃ©mon de la Gen 1 rÃ©vÃ¨le :

| Statistique | Moyenne | Ã‰cart-type | Min | Max |
|-------------|---------|------------|-----|-----|
| HP | 65 | 25 | 20 | 255 |
| Attack | 75 | 30 | 5 | 190 |
| Defense | 70 | 28 | 5 | 230 |
| Sp. Attack | 65 | 32 | 10 | 194 |
| Sp. Defense | 65 | 28 | 20 | 194 |
| Speed | 70 | 28 | 15 | 180 |

*[InsÃ©rer figure : eda_stats_distribution.png]*

### Distribution des types

Les types les plus reprÃ©sentÃ©s dans la Gen 1 :

1. **Poison** (33 PokÃ©mon) â€” Souvent combinÃ© avec Grass ou Bug
2. **Water** (32 PokÃ©mon) â€” Type le plus commun seul
3. **Normal** (22 PokÃ©mon) â€” Type simple, peu de faiblesses

*[InsÃ©rer figure : eda_type_distribution.png]*

### CorrÃ©lation entre statistiques

L'analyse de corrÃ©lation montre :

- **Forte corrÃ©lation** : Total stats â†” toutes les stats individuelles (0.55-0.68)
- **CorrÃ©lation modÃ©rÃ©e** : Defense â†” Sp.Defense (0.51), Sp.Attack â†” Sp.Defense (0.51)
- **CorrÃ©lation faible/nÃ©gative** : Speed â†” Defense (-0.02) â€” Les PokÃ©mon rapides sont souvent fragiles

*[InsÃ©rer figure : eda_correlation_matrix.png]*

### Analyse des rÃ©sultats de combats

L'analyse du dataset de combats rÃ©vÃ¨le des patterns importants :

| Facteur | Impact sur le taux de victoire |
|---------|-------------------------------|
| **Avantage de type** | +22% (72% vs 50%) |
| **Vitesse supÃ©rieure (+30)** | +18% (68% vs 50%) |
| **Stats totales supÃ©rieures** | +15% |

**Insights clÃ©s :**
- L'avantage de type est le facteur le plus dÃ©terminant
- La vitesse est critique (attaquer en premier = avantage)
- La diffÃ©rence de stats totales est moins importante que prÃ©vu

*[InsÃ©rer figure : eda_battle_analysis.png]*

### Conclusions de l'EDA

Ces analyses ont guidÃ© le **feature engineering** :

1. âœ… Inclure les **diffÃ©rences de stats** (speed_diff, attack_diff...)
2. âœ… Calculer l'**avantage de type** comme feature clÃ©
3. âœ… CrÃ©er des features de **ratio** (attack/defense adverse)
4. âŒ Ne pas surÃ©valuer les stats totales seules

---

## 2.1 Automatisation de l'extraction des donnÃ©es (C1)

> **C1** : *Automatiser l'extraction de donnÃ©es depuis un service web, une page web (scraping), un fichier de donnÃ©es, une base de donnÃ©es et un systÃ¨me big data en programmant le script adaptÃ© afin de pÃ©renniser la collecte des donnÃ©es nÃ©cessaires au projet.*

### Sources de donnÃ©es identifiÃ©es

Le projet exploite **trois sources de donnÃ©es complÃ©mentaires** :

| Source | Type | DonnÃ©es collectÃ©es | Volume |
|--------|------|-------------------|--------|
| **PokÃ©API** | API REST | Stats de base, types, sprites | ~150 PokÃ©mon |
| **Pokepedia** | Scraping web | Noms FR, descriptions, Ã©volutions | ~150 fiches |
| **Fichiers CSV** | Fichiers locaux | Datasets de combats simulÃ©s | ~10 000 combats |

### Architecture du pipeline ETL

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Sources    â”‚â”€â”€â”€â–¶â”‚  Extraction  â”‚â”€â”€â”€â–¶â”‚Transformationâ”‚â”€â”€â”€â–¶â”‚  Chargement  â”‚
â”‚              â”‚    â”‚              â”‚    â”‚              â”‚    â”‚              â”‚
â”‚ â€¢ PokÃ©API    â”‚    â”‚ â€¢ RequÃªtes   â”‚    â”‚ â€¢ Nettoyage  â”‚    â”‚ â€¢ PostgreSQL â”‚
â”‚ â€¢ Pokepedia  â”‚    â”‚ â€¢ Scraping   â”‚    â”‚ â€¢ Validation â”‚    â”‚ â€¢ Tables     â”‚
â”‚ â€¢ CSV files  â”‚    â”‚ â€¢ Parsing    â”‚    â”‚ â€¢ Jointures  â”‚    â”‚ â€¢ Index      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ImplÃ©mentation de l'extraction

**Extraction depuis PokÃ©API (service web) :**

```python
# etl_pokemon/pokepedia_scraper/pokeapi_client.py

import httpx
from typing import Optional

class PokeAPIClient:
    """Client pour l'extraction de donnÃ©es depuis PokÃ©API."""
    
    BASE_URL = "https://pokeapi.co/api/v2"
    
    def __init__(self):
        self.client = httpx.Client(timeout=30.0)
    
    def get_pokemon(self, pokemon_id: int) -> dict:
        """RÃ©cupÃ¨re les donnÃ©es d'un PokÃ©mon par son ID."""
        response = self.client.get(f"{self.BASE_URL}/pokemon/{pokemon_id}")
        response.raise_for_status()
        return response.json()
    
    def get_pokemon_species(self, pokemon_id: int) -> dict:
        """RÃ©cupÃ¨re les informations d'espÃ¨ce (descriptions FR)."""
        response = self.client.get(f"{self.BASE_URL}/pokemon-species/{pokemon_id}")
        response.raise_for_status()
        return response.json()
```

**Extraction par scraping (Pokepedia) :**

```python
# etl_pokemon/pokepedia_scraper/scraper.py

import requests
from bs4 import BeautifulSoup

class PokepediaScraper:
    """Scraper pour extraire les donnÃ©es franÃ§aises depuis Pokepedia."""
    
    BASE_URL = "https://www.pokepedia.fr"
    
    def scrape_pokemon_page(self, pokemon_name: str) -> dict:
        """Scrape une page PokÃ©mon pour extraire les donnÃ©es FR."""
        url = f"{self.BASE_URL}/{pokemon_name}"
        response = requests.get(url, headers={"User-Agent": "PredictionDex/1.0"})
        soup = BeautifulSoup(response.text, "html.parser")
        
        return {
            "name_fr": self._extract_name(soup),
            "description_fr": self._extract_description(soup),
            "types_fr": self._extract_types(soup),
        }
```

**Extraction depuis fichiers CSV :**

```python
# etl_pokemon/pipeline.py

import pandas as pd
from pathlib import Path

def load_battle_dataset(filepath: Path) -> pd.DataFrame:
    """Charge et valide un dataset de combats depuis un fichier CSV."""
    df = pd.read_csv(filepath)
    
    # Validation des colonnes requises
    required_cols = ["pokemon_1_id", "pokemon_2_id", "winner"]
    missing = set(required_cols) - set(df.columns)
    if missing:
        raise ValueError(f"Colonnes manquantes : {missing}")
    
    return df
```

### Automatisation et planification

Le pipeline ETL est **entiÃ¨rement automatisÃ©** via Docker :

```yaml
# docker-compose.yml (extrait)
etl:
  build:
    context: .
    dockerfile: docker/Dockerfile.etl
  depends_on:
    db:
      condition: service_healthy
  environment:
    - DATABASE_URL=postgresql://user:pass@db:5432/predictiondex
  command: python -m etl_pokemon.pipeline --full-refresh
```

---

## 2.2 RequÃªtes SQL d'extraction et transformation (C2)

> **C2** : *DÃ©velopper des requÃªtes de type SQL d'extraction des donnÃ©es depuis un systÃ¨me de gestion de base de donnÃ©es et un systÃ¨me big data en appliquant le langage de requÃªte propre au systÃ¨me afin de prÃ©parer la collecte des donnÃ©es nÃ©cessaires au projet.*

### RequÃªtes d'extraction principales

**Extraction des PokÃ©mon avec leurs types :**

```sql
-- RÃ©cupÃ©ration des PokÃ©mon avec jointure sur les types
SELECT 
    p.id,
    p.name,
    p.name_fr,
    p.hp,
    p.attack,
    p.defense,
    p.special_attack,
    p.special_defense,
    p.speed,
    t1.name AS type_primary,
    t2.name AS type_secondary
FROM pokemon p
LEFT JOIN types t1 ON p.type_primary_id = t1.id
LEFT JOIN types t2 ON p.type_secondary_id = t2.id
WHERE p.generation <= 1  -- PokÃ©mon Let's Go = Gen 1
ORDER BY p.id;
```

**Extraction des combats pour l'entraÃ®nement ML :**

```sql
-- Dataset d'entraÃ®nement avec features calculÃ©es
SELECT 
    b.id AS battle_id,
    p1.id AS pokemon_1_id,
    p1.name AS pokemon_1_name,
    p1.hp AS p1_hp,
    p1.attack AS p1_attack,
    p1.defense AS p1_defense,
    p1.speed AS p1_speed,
    p2.id AS pokemon_2_id,
    p2.name AS pokemon_2_name,
    p2.hp AS p2_hp,
    p2.attack AS p2_attack,
    p2.defense AS p2_defense,
    p2.speed AS p2_speed,
    b.winner,
    -- Feature engineering : diffÃ©rences de stats
    (p1.attack - p2.defense) AS attack_advantage_1,
    (p2.attack - p1.defense) AS attack_advantage_2,
    (p1.speed - p2.speed) AS speed_diff
FROM battles b
JOIN pokemon p1 ON b.pokemon_1_id = p1.id
JOIN pokemon p2 ON b.pokemon_2_id = p2.id;
```

**AgrÃ©gation statistique :**

```sql
-- Statistiques par type de PokÃ©mon
SELECT 
    t.name AS type_name,
    COUNT(p.id) AS pokemon_count,
    ROUND(AVG(p.hp), 2) AS avg_hp,
    ROUND(AVG(p.attack), 2) AS avg_attack,
    ROUND(AVG(p.defense), 2) AS avg_defense,
    ROUND(AVG(p.speed), 2) AS avg_speed,
    ROUND(AVG(p.hp + p.attack + p.defense + p.special_attack + 
              p.special_defense + p.speed), 2) AS avg_total_stats
FROM pokemon p
JOIN types t ON p.type_primary_id = t.id
GROUP BY t.name
ORDER BY avg_total_stats DESC;
```

---

## 2.3 RÃ¨gles d'agrÃ©gation et nettoyage des donnÃ©es (C3)

> **C3** : *DÃ©velopper des rÃ¨gles d'agrÃ©gation de donnÃ©es issues de diffÃ©rentes sources en programmant, sous forme de script, la suppression des entrÃ©es corrompues et en programmant l'homogÃ©nÃ©isation des formats des donnÃ©es afin de prÃ©parer le stockage du jeu de donnÃ©es final.*

### RÃ¨gles de nettoyage appliquÃ©es

| RÃ¨gle | Description | ImplÃ©mentation |
|-------|-------------|----------------|
| **Doublons** | Suppression des PokÃ©mon en double | `df.drop_duplicates(subset=['pokedex_id'])` |
| **Valeurs nulles** | Remplacement par valeurs par dÃ©faut | `df['type_secondary'].fillna('None')` |
| **Normalisation** | Noms en minuscules, accents gÃ©rÃ©s | `unidecode(name.lower().strip())` |
| **Validation types** | Conversion des types de donnÃ©es | `df['hp'] = pd.to_numeric(df['hp'], errors='coerce')` |
| **Outliers** | Stats aberrantes (>255) rejetÃ©es | `df = df[df['attack'] <= 255]` |

### ImplÃ©mentation du nettoyage

```python
# etl_pokemon/utils/data_cleaning.py

import pandas as pd
from unidecode import unidecode

class DataCleaner:
    """Classe de nettoyage et normalisation des donnÃ©es PokÃ©mon."""
    
    STAT_COLUMNS = ['hp', 'attack', 'defense', 'special_attack', 
                    'special_defense', 'speed']
    MAX_STAT_VALUE = 255
    
    def clean_pokemon_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Pipeline complet de nettoyage des donnÃ©es PokÃ©mon."""
        df = self._remove_duplicates(df)
        df = self._handle_missing_values(df)
        df = self._normalize_names(df)
        df = self._validate_stats(df)
        df = self._remove_outliers(df)
        return df
    
    def _remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        """Supprime les doublons basÃ©s sur l'ID PokÃ©dex."""
        initial_count = len(df)
        df = df.drop_duplicates(subset=['pokedex_id'], keep='first')
        removed = initial_count - len(df)
        if removed > 0:
            print(f"ğŸ§¹ {removed} doublons supprimÃ©s")
        return df
    
    def _handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """GÃ¨re les valeurs manquantes."""
        # Type secondaire : None si absent
        df['type_secondary'] = df['type_secondary'].fillna('None')
        
        # Stats : erreur si manquantes (donnÃ©es critiques)
        if df[self.STAT_COLUMNS].isnull().any().any():
            raise ValueError("Stats manquantes dÃ©tectÃ©es - donnÃ©es invalides")
        
        return df
    
    def _normalize_names(self, df: pd.DataFrame) -> pd.DataFrame:
        """Normalise les noms (minuscules, sans accents pour recherche)."""
        df['name_normalized'] = df['name'].apply(
            lambda x: unidecode(str(x).lower().strip())
        )
        return df
    
    def _validate_stats(self, df: pd.DataFrame) -> pd.DataFrame:
        """Valide et convertit les types des statistiques."""
        for col in self.STAT_COLUMNS:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        return df
    
    def _remove_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        """Supprime les entrÃ©es avec des stats aberrantes."""
        initial_count = len(df)
        for col in self.STAT_COLUMNS:
            df = df[(df[col] >= 0) & (df[col] <= self.MAX_STAT_VALUE)]
        removed = initial_count - len(df)
        if removed > 0:
            print(f"âš ï¸ {removed} outliers supprimÃ©s")
        return df
```

### AgrÃ©gation des sources

```python
# etl_pokemon/pipeline.py

def aggregate_sources(
    pokeapi_data: pd.DataFrame,
    pokepedia_data: pd.DataFrame,
    csv_data: pd.DataFrame
) -> pd.DataFrame:
    """AgrÃ¨ge les donnÃ©es de toutes les sources."""
    
    # Fusion PokÃ©API + Pokepedia sur l'ID
    merged = pokeapi_data.merge(
        pokepedia_data[['pokedex_id', 'name_fr', 'description_fr']],
        on='pokedex_id',
        how='left'
    )
    
    # Ajout des donnÃ©es CSV (types additionnels)
    merged = merged.merge(
        csv_data[['pokedex_id', 'evolution_chain']],
        on='pokedex_id',
        how='left'
    )
    
    print(f"âœ… AgrÃ©gation terminÃ©e : {len(merged)} PokÃ©mon")
    return merged
```

---

## 2.4 CrÃ©ation de la base de donnÃ©es (C4)

> **C4** : *CrÃ©er une base de donnÃ©es dans le respect du RGPD en Ã©laborant les modÃ¨les conceptuels et physiques des donnÃ©es Ã  partir des donnÃ©es prÃ©parÃ©es et en programmant leur import afin de stocker le jeu de donnÃ©es du projet.*

### ModÃ¨le Conceptuel de DonnÃ©es (MCD)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     POKEMON     â”‚         â”‚      TYPE       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ id (PK)       â”‚    â”Œâ”€â”€â”€â–¶â”‚ â€¢ id (PK)       â”‚
â”‚ â€¢ pokedex_id    â”‚    â”‚    â”‚ â€¢ name          â”‚
â”‚ â€¢ name          â”‚    â”‚    â”‚ â€¢ name_fr       â”‚
â”‚ â€¢ name_fr       â”‚â”€â”€â”€â”€â”¤    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ â€¢ hp            â”‚    â”‚
â”‚ â€¢ attack        â”‚    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ defense       â”‚    â”‚    â”‚      MOVE       â”‚
â”‚ â€¢ sp_attack     â”‚    â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ sp_defense    â”‚    â””â”€â”€â”€â–¶â”‚ â€¢ id (PK)       â”‚
â”‚ â€¢ speed         â”‚         â”‚ â€¢ name          â”‚
â”‚ â€¢ type_1_id(FK) â”‚         â”‚ â€¢ power         â”‚
â”‚ â€¢ type_2_id(FK) â”‚         â”‚ â€¢ accuracy      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ â€¢ type_id (FK)  â”‚
         â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ participe Ã 
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     BATTLE      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ id (PK)       â”‚
â”‚ â€¢ pokemon_1(FK) â”‚
â”‚ â€¢ pokemon_2(FK) â”‚
â”‚ â€¢ winner        â”‚
â”‚ â€¢ created_at    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ModÃ¨le Physique de DonnÃ©es (MPD)

```sql
-- Script de crÃ©ation des tables PostgreSQL

CREATE TABLE types (
    id SERIAL PRIMARY KEY,
    name VARCHAR(50) NOT NULL UNIQUE,
    name_fr VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE pokemon (
    id SERIAL PRIMARY KEY,
    pokedex_id INTEGER NOT NULL UNIQUE,
    name VARCHAR(100) NOT NULL,
    name_fr VARCHAR(100),
    hp INTEGER NOT NULL CHECK (hp >= 0 AND hp <= 255),
    attack INTEGER NOT NULL CHECK (attack >= 0 AND attack <= 255),
    defense INTEGER NOT NULL CHECK (defense >= 0 AND defense <= 255),
    special_attack INTEGER NOT NULL CHECK (special_attack >= 0 AND special_attack <= 255),
    special_defense INTEGER NOT NULL CHECK (special_defense >= 0 AND special_defense <= 255),
    speed INTEGER NOT NULL CHECK (speed >= 0 AND speed <= 255),
    type_primary_id INTEGER REFERENCES types(id),
    type_secondary_id INTEGER REFERENCES types(id),
    sprite_url VARCHAR(500),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE moves (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    name_fr VARCHAR(100),
    power INTEGER CHECK (power >= 0 AND power <= 250),
    accuracy INTEGER CHECK (accuracy >= 0 AND accuracy <= 100),
    pp INTEGER CHECK (pp >= 0 AND pp <= 40),
    type_id INTEGER REFERENCES types(id),
    damage_class VARCHAR(20) CHECK (damage_class IN ('physical', 'special', 'status')),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE battles (
    id SERIAL PRIMARY KEY,
    pokemon_1_id INTEGER NOT NULL REFERENCES pokemon(id),
    pokemon_2_id INTEGER NOT NULL REFERENCES pokemon(id),
    winner INTEGER NOT NULL CHECK (winner IN (1, 2)),
    pokemon_1_move_id INTEGER REFERENCES moves(id),
    pokemon_2_move_id INTEGER REFERENCES moves(id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Index pour optimiser les requÃªtes
CREATE INDEX idx_pokemon_pokedex_id ON pokemon(pokedex_id);
CREATE INDEX idx_pokemon_name ON pokemon(name);
CREATE INDEX idx_battles_pokemon_1 ON battles(pokemon_1_id);
CREATE INDEX idx_battles_pokemon_2 ON battles(pokemon_2_id);
CREATE INDEX idx_moves_type ON moves(type_id);
```

### ImplÃ©mentation avec SQLAlchemy

```python
# core/db/models.py

from sqlalchemy import Column, Integer, String, ForeignKey, DateTime, CheckConstraint
from sqlalchemy.orm import relationship, declarative_base
from datetime import datetime

Base = declarative_base()

class Pokemon(Base):
    """ModÃ¨le SQLAlchemy pour les PokÃ©mon."""
    __tablename__ = 'pokemon'
    
    id = Column(Integer, primary_key=True)
    pokedex_id = Column(Integer, nullable=False, unique=True)
    name = Column(String(100), nullable=False)
    name_fr = Column(String(100))
    hp = Column(Integer, nullable=False)
    attack = Column(Integer, nullable=False)
    defense = Column(Integer, nullable=False)
    special_attack = Column(Integer, nullable=False)
    special_defense = Column(Integer, nullable=False)
    speed = Column(Integer, nullable=False)
    type_primary_id = Column(Integer, ForeignKey('types.id'))
    type_secondary_id = Column(Integer, ForeignKey('types.id'))
    sprite_url = Column(String(500))
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relations
    type_primary = relationship("Type", foreign_keys=[type_primary_id])
    type_secondary = relationship("Type", foreign_keys=[type_secondary_id])
    
    __table_args__ = (
        CheckConstraint('hp >= 0 AND hp <= 255', name='check_hp_range'),
        CheckConstraint('attack >= 0 AND attack <= 255', name='check_attack_range'),
    )
```

### ConformitÃ© RGPD

| Aspect RGPD | Application dans PredictionDex |
|-------------|-------------------------------|
| **DonnÃ©es personnelles** | âŒ Aucune donnÃ©e personnelle collectÃ©e |
| **Consentement** | Non applicable (donnÃ©es publiques PokÃ©mon) |
| **FinalitÃ©** | PrÃ©diction de combats - usage ludique |
| **Minimisation** | Seules les donnÃ©es nÃ©cessaires au ML sont stockÃ©es |
| **Conservation** | DonnÃ©es conservÃ©es tant que le service est actif |
| **SÃ©curitÃ©** | AccÃ¨s BDD restreint, mots de passe hashÃ©s |

> **Note** : Le projet ne collecte aucune donnÃ©e personnelle d'utilisateurs. Les seules donnÃ©es stockÃ©es concernent les PokÃ©mon (donnÃ©es publiques) et les rÃ©sultats de combats simulÃ©s.

---

## 2.5 API REST de mise Ã  disposition des donnÃ©es (C5)

> **C5** : *Partager le jeu de donnÃ©es en configurant des interfaces logicielles et en crÃ©ant des interfaces programmables afin de mettre Ã  disposition le jeu de donnÃ©es pour le dÃ©veloppement du projet.*

### Architecture de l'API de donnÃ©es

L'API de donnÃ©es est dÃ©veloppÃ©e avec **FastAPI** et expose les endpoints suivants :

| MÃ©thode | Endpoint | Description |
|---------|----------|-------------|
| `GET` | `/api/v1/pokemon` | Liste tous les PokÃ©mon (pagination) |
| `GET` | `/api/v1/pokemon/{id}` | DÃ©tail d'un PokÃ©mon |
| `GET` | `/api/v1/pokemon/search?name=` | Recherche par nom |
| `GET` | `/api/v1/types` | Liste tous les types |
| `GET` | `/api/v1/moves` | Liste toutes les attaques |
| `GET` | `/api/v1/battles` | Historique des combats |
| `POST` | `/api/v1/battles` | CrÃ©er un combat |

### ImplÃ©mentation des endpoints

```python
# api_pokemon/routes/pokemon.py

from fastapi import APIRouter, Depends, HTTPException, Query
from sqlalchemy.orm import Session
from typing import List, Optional

from core.db.database import get_db
from core.schemas.pokemon import PokemonResponse, PokemonListResponse
from api_pokemon.services.pokemon_service import PokemonService

router = APIRouter(prefix="/api/v1/pokemon", tags=["Pokemon"])

@router.get("/", response_model=PokemonListResponse)
async def list_pokemon(
    skip: int = Query(0, ge=0, description="Nombre d'Ã©lÃ©ments Ã  sauter"),
    limit: int = Query(20, ge=1, le=100, description="Nombre max d'Ã©lÃ©ments"),
    type_filter: Optional[str] = Query(None, description="Filtrer par type"),
    db: Session = Depends(get_db)
):
    """
    Liste tous les PokÃ©mon avec pagination et filtrage optionnel.
    
    - **skip**: Offset pour la pagination
    - **limit**: Nombre maximum de rÃ©sultats (1-100)
    - **type_filter**: Filtrer par type (ex: "fire", "water")
    """
    service = PokemonService(db)
    pokemon_list = service.get_all(skip=skip, limit=limit, type_filter=type_filter)
    total = service.count(type_filter=type_filter)
    
    return PokemonListResponse(
        items=pokemon_list,
        total=total,
        skip=skip,
        limit=limit
    )

@router.get("/{pokemon_id}", response_model=PokemonResponse)
async def get_pokemon(
    pokemon_id: int,
    db: Session = Depends(get_db)
):
    """RÃ©cupÃ¨re les dÃ©tails d'un PokÃ©mon par son ID."""
    service = PokemonService(db)
    pokemon = service.get_by_id(pokemon_id)
    
    if not pokemon:
        raise HTTPException(status_code=404, detail="PokÃ©mon non trouvÃ©")
    
    return pokemon

@router.get("/search/", response_model=List[PokemonResponse])
async def search_pokemon(
    name: str = Query(..., min_length=2, description="Nom Ã  rechercher"),
    db: Session = Depends(get_db)
):
    """Recherche des PokÃ©mon par nom (partiel)."""
    service = PokemonService(db)
    results = service.search_by_name(name)
    return results
```

### Documentation OpenAPI (Swagger)

L'API gÃ©nÃ¨re automatiquement une documentation interactive accessible Ã  `/docs` :

```python
# api_pokemon/main.py

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI(
    title="PredictionDex API",
    description="API REST pour accÃ©der aux donnÃ©es PokÃ©mon et aux prÃ©dictions de combats",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Ã€ restreindre en production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

### SÃ©curisation de l'API

```python
# api_pokemon/middleware/security.py

from fastapi import Security, HTTPException
from fastapi.security import APIKeyHeader

API_KEY_HEADER = APIKeyHeader(name="X-API-Key", auto_error=False)

async def verify_api_key(api_key: str = Security(API_KEY_HEADER)):
    """VÃ©rifie la clÃ© API pour les endpoints protÃ©gÃ©s."""
    if api_key is None:
        raise HTTPException(status_code=401, detail="ClÃ© API manquante")
    
    if api_key != settings.API_KEY:
        raise HTTPException(status_code=403, detail="ClÃ© API invalide")
    
    return api_key
```

### Tests de l'API

```python
# tests/api/test_pokemon_routes.py

import pytest
from fastapi.testclient import TestClient
from api_pokemon.main import app

client = TestClient(app)

def test_list_pokemon():
    """Test de la liste des PokÃ©mon."""
    response = client.get("/api/v1/pokemon?limit=10")
    assert response.status_code == 200
    data = response.json()
    assert "items" in data
    assert len(data["items"]) <= 10

def test_get_pokemon_by_id():
    """Test de rÃ©cupÃ©ration d'un PokÃ©mon par ID."""
    response = client.get("/api/v1/pokemon/25")  # Pikachu
    assert response.status_code == 200
    data = response.json()
    assert data["name"].lower() == "pikachu"

def test_pokemon_not_found():
    """Test erreur 404 pour PokÃ©mon inexistant."""
    response = client.get("/api/v1/pokemon/99999")
    assert response.status_code == 404
```

---

## 2.6 SÃ©curitÃ© de l'API et des DonnÃ©es

> Cette section dÃ©taille les mesures de sÃ©curitÃ© implÃ©mentÃ©es, en rÃ©fÃ©rence aux standards OWASP.

### Analyse des risques OWASP Top 10

| Risque OWASP | Niveau | Mesure implÃ©mentÃ©e | Statut |
|--------------|--------|-------------------|--------|
| **A01 - Broken Access Control** | ğŸŸ¡ Moyen | ClÃ© API obligatoire, middleware de vÃ©rification | âœ… |
| **A02 - Cryptographic Failures** | ğŸŸ¢ Faible | Pas de donnÃ©es sensibles, HTTPS en prod | âœ… |
| **A03 - Injection** | ğŸ”´ Critique | Validation Pydantic, ORM SQLAlchemy (requÃªtes paramÃ©trÃ©es) | âœ… |
| **A04 - Insecure Design** | ğŸŸ¡ Moyen | Architecture revue, principes SOLID | âœ… |
| **A05 - Security Misconfiguration** | ğŸŸ¡ Moyen | Headers sÃ©curisÃ©s, CORS configurÃ© | âœ… |
| **A06 - Vulnerable Components** | ğŸŸ¡ Moyen | DÃ©pendances Ã  jour, Dependabot activÃ© | âœ… |
| **A07 - Auth Failures** | ğŸŸ¡ Moyen | ClÃ© API, rate limiting prÃ©vu | âš ï¸ |
| **A08 - Data Integrity Failures** | ğŸŸ¢ Faible | Validation des entrÃ©es, checksums | âœ… |
| **A09 - Logging Failures** | ğŸŸ¡ Moyen | Logs structurÃ©s, monitoring Prometheus | âœ… |
| **A10 - SSRF** | ğŸŸ¢ Faible | Pas d'appels externes dynamiques | âœ… |

### Authentification et Autorisation

```python
# api_pokemon/middleware/security.py

from fastapi import Security, HTTPException, Depends
from fastapi.security import APIKeyHeader
from functools import wraps

# En-tÃªte personnalisÃ© pour la clÃ© API
API_KEY_HEADER = APIKeyHeader(name="X-API-Key", auto_error=False)

async def verify_api_key(api_key: str = Security(API_KEY_HEADER)) -> str:
    """
    Middleware de vÃ©rification de la clÃ© API.
    
    SÃ©curitÃ© :
    - Comparaison en temps constant (Ã©vite timing attacks)
    - Logging des tentatives Ã©chouÃ©es
    - Rate limiting recommandÃ© en production
    """
    if api_key is None:
        raise HTTPException(
            status_code=401,
            detail="ClÃ© API manquante",
            headers={"WWW-Authenticate": "ApiKey"}
        )
    
    # Comparaison sÃ©curisÃ©e (temps constant)
    import secrets
    if not secrets.compare_digest(api_key, settings.API_KEY):
        # Log de la tentative Ã©chouÃ©e
        logger.warning(f"Tentative d'accÃ¨s avec clÃ© invalide")
        raise HTTPException(
            status_code=403,
            detail="ClÃ© API invalide"
        )
    
    return api_key
```

### Protection contre les injections

```python
# core/db/database.py - Utilisation de l'ORM pour Ã©viter les injections SQL

from sqlalchemy.orm import Session

def get_pokemon_by_id(db: Session, pokemon_id: int) -> Pokemon:
    """
    RÃ©cupÃ©ration sÃ©curisÃ©e par ORM.
    
    âŒ VULNÃ‰RABLE (SQL injection) :
    query = f"SELECT * FROM pokemon WHERE id = {pokemon_id}"
    
    âœ… SÃ‰CURISÃ‰ (ORM avec requÃªte paramÃ©trÃ©e) :
    """
    return db.query(Pokemon).filter(Pokemon.id == pokemon_id).first()

def search_pokemon_by_name(db: Session, name: str) -> list[Pokemon]:
    """
    Recherche sÃ©curisÃ©e avec paramÃ¨tres Ã©chappÃ©s.
    
    L'ORM SQLAlchemy Ã©chappe automatiquement les caractÃ¨res spÃ©ciaux.
    """
    return db.query(Pokemon).filter(
        Pokemon.name.ilike(f"%{name}%")  # ParamÃ©trÃ© par SQLAlchemy
    ).all()
```

### Validation des entrÃ©es (Pydantic)

```python
# core/schemas/battle.py

from pydantic import BaseModel, Field, validator
from typing import Optional

class BattlePredictionRequest(BaseModel):
    """
    SchÃ©ma de validation pour les requÃªtes de prÃ©diction.
    
    Pydantic valide automatiquement :
    - Types de donnÃ©es
    - Plages de valeurs
    - Formats attendus
    """
    pokemon_1_id: int = Field(
        ..., 
        gt=0, 
        le=151,
        description="ID du premier PokÃ©mon (1-151)"
    )
    pokemon_2_id: int = Field(
        ..., 
        gt=0, 
        le=151,
        description="ID du second PokÃ©mon (1-151)"
    )
    
    @validator('pokemon_2_id')
    def different_pokemon(cls, v, values):
        """VÃ©rifie que les deux PokÃ©mon sont diffÃ©rents."""
        if 'pokemon_1_id' in values and v == values['pokemon_1_id']:
            raise ValueError('Les deux PokÃ©mon doivent Ãªtre diffÃ©rents')
        return v

    class Config:
        # Rejeter les champs non dÃ©clarÃ©s (sÃ©curitÃ©)
        extra = "forbid"
```

### ğŸ”® AmÃ©liorations potentielles de sÃ©curitÃ©

Les mesures suivantes sont **recommandÃ©es pour une mise en production** mais non implÃ©mentÃ©es dans le cadre de ce projet pÃ©dagogique :

#### Configuration CORS (Ã  implÃ©menter)

```python
# api_pokemon/main.py (amÃ©lioration suggÃ©rÃ©e)

from fastapi.middleware.cors import CORSMiddleware

# Configuration CORS sÃ©curisÃ©e pour production
ALLOWED_ORIGINS = [
    "http://localhost:8501",      # Streamlit local
    "https://predictiondex.app",  # Production (exemple)
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,  # âš ï¸ Pas "*" en production !
    allow_credentials=True,
    allow_methods=["GET", "POST"],  # Limiter aux mÃ©thodes nÃ©cessaires
    allow_headers=["X-API-Key", "Content-Type"],
)
```

> **Note** : Dans le contexte Docker actuel, l'API communique en interne avec Streamlit via le rÃ©seau Docker, ce qui limite l'exposition aux risques CORS.

#### Headers de sÃ©curitÃ© HTTP (Ã  implÃ©menter)

```python
# api_pokemon/middleware/security_headers.py (amÃ©lioration suggÃ©rÃ©e)

from fastapi import Request
from starlette.middleware.base import BaseHTTPMiddleware

class SecurityHeadersMiddleware(BaseHTTPMiddleware):
    """Middleware ajoutant les headers de sÃ©curitÃ© HTTP."""
    
    async def dispatch(self, request: Request, call_next):
        response = await call_next(request)
        
        # Headers de sÃ©curitÃ© recommandÃ©s
        response.headers["X-Content-Type-Options"] = "nosniff"
        response.headers["X-Frame-Options"] = "DENY"
        response.headers["X-XSS-Protection"] = "1; mode=block"
        response.headers["Strict-Transport-Security"] = "max-age=31536000"
        response.headers["Content-Security-Policy"] = "default-src 'self'"
        
        return response
```

### Gestion des secrets

```python
# api_pokemon/config.py

from pydantic_settings import BaseSettings
from functools import lru_cache

class Settings(BaseSettings):
    """
    Configuration sÃ©curisÃ©e via variables d'environnement.
    
    âš ï¸ JAMAIS de secrets en dur dans le code !
    """
    # Base de donnÃ©es
    DATABASE_URL: str  # Obligatoire, pas de dÃ©faut
    
    # API
    API_KEY: str  # ClÃ© API pour l'authentification
    
    # MLflow
    MLFLOW_TRACKING_URI: str = "http://mlflow:5000"
    
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        case_sensitive = True

@lru_cache()
def get_settings() -> Settings:
    return Settings()
```

### Checklist de sÃ©curitÃ©

| Aspect | Statut | Fichier / Remarque |
|--------|--------|--------------------|
| âœ… Authentification API Key | ImplÃ©mentÃ© | `middleware/security.py` |
| âœ… Validation Pydantic | ImplÃ©mentÃ© | `schemas/*.py` |
| âœ… ORM (anti-injection SQL) | ImplÃ©mentÃ© | `core/db/` + SQLAlchemy |
| âœ… Secrets en env vars | ImplÃ©mentÃ© | `config.py` + `.env` |
| âœ… Logs d'audit | ImplÃ©mentÃ© | `monitoring/` |
| âš ï¸ CORS | Ã€ implÃ©menter | RÃ©seau Docker interne limite le risque |
| âš ï¸ Headers sÃ©curitÃ© HTTP | Ã€ implÃ©menter | RecommandÃ© pour production |
| âš ï¸ Rate limiting | Ã€ implÃ©menter | Protection contre DDoS |
| âš ï¸ JWT (si multi-users) | Ã€ implÃ©menter | Pour gestion utilisateurs |

---

\newpage

---

# 3. BLOC E3 : Mise Ã  Disposition de l'Intelligence Artificielle

> **CompÃ©tences visÃ©es : C9, C10, C11, C12, C13**

---

## 3.1 DÃ©veloppement de l'API exposant le modÃ¨le (C9)

> **C9** : *DÃ©velopper une API REST exposant un modÃ¨le d'intelligence artificielle en respectant ses spÃ©cifications fonctionnelles et techniques et les standards de qualitÃ© et de sÃ©curitÃ© du marchÃ© pour permettre l'interaction entre le modÃ¨le et les autres composants du projet.*

### ModÃ¨le de Machine Learning

**CaractÃ©ristiques du modÃ¨le :**

| PropriÃ©tÃ© | Valeur |
|-----------|--------|
| **Algorithme** | XGBoost Classifier |
| **Version en production** | v2 |
| **Accuracy** | 88.23% |
| **Precision** | 87.5% |
| **Recall** | 88.9% |
| **F1-Score** | 88.2% |
| **Features** | 42 (stats, types, avantages) |

**Comparaison des versions :**

| Version | ScÃ©nario | Accuracy | Remarque |
|---------|----------|----------|----------|
| v1 | `best_move` uniquement | 94.24% | Contexte simplifiÃ© |
| **v2** | `both_best_move` | **88.23%** | Plus rÃ©aliste, recommandÃ© |

### Architecture de l'API ML

```python
# api_pokemon/routes/predictions.py

from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel, Field
from typing import Optional
import joblib

router = APIRouter(prefix="/api/v1/predict", tags=["Predictions"])

class BattlePredictionRequest(BaseModel):
    """SchÃ©ma de requÃªte pour une prÃ©diction de combat."""
    pokemon_1_id: int = Field(..., description="ID du premier PokÃ©mon")
    pokemon_2_id: int = Field(..., description="ID du second PokÃ©mon")
    pokemon_1_move_id: Optional[int] = Field(None, description="Attaque du PokÃ©mon 1")
    pokemon_2_move_id: Optional[int] = Field(None, description="Attaque du PokÃ©mon 2")

class BattlePredictionResponse(BaseModel):
    """SchÃ©ma de rÃ©ponse pour une prÃ©diction."""
    winner: int = Field(..., description="PokÃ©mon gagnant (1 ou 2)")
    winner_name: str = Field(..., description="Nom du vainqueur")
    confidence: float = Field(..., description="Confiance de la prÃ©diction (0-1)")
    pokemon_1_win_probability: float
    pokemon_2_win_probability: float

@router.post("/battle", response_model=BattlePredictionResponse)
async def predict_battle(
    request: BattlePredictionRequest,
    db: Session = Depends(get_db)
):
    """
    PrÃ©dit le vainqueur d'un combat entre deux PokÃ©mon.
    
    Le modÃ¨le XGBoost v2 analyse les statistiques, types et attaques
    pour prÃ©dire le gagnant avec un taux de prÃ©cision de 88.23%.
    """
    # RÃ©cupÃ©ration des donnÃ©es des PokÃ©mon
    pokemon_1 = get_pokemon_by_id(db, request.pokemon_1_id)
    pokemon_2 = get_pokemon_by_id(db, request.pokemon_2_id)
    
    if not pokemon_1 or not pokemon_2:
        raise HTTPException(status_code=404, detail="PokÃ©mon non trouvÃ©")
    
    # PrÃ©paration des features
    features = prepare_battle_features(pokemon_1, pokemon_2, request)
    
    # PrÃ©diction
    model = load_model_from_mlflow()
    probabilities = model.predict_proba(features)[0]
    winner = int(model.predict(features)[0])
    
    return BattlePredictionResponse(
        winner=winner,
        winner_name=pokemon_1.name if winner == 1 else pokemon_2.name,
        confidence=max(probabilities),
        pokemon_1_win_probability=float(probabilities[0]),
        pokemon_2_win_probability=float(probabilities[1])
    )
```

### Chargement du modÃ¨le depuis MLflow

```python
# api_pokemon/services/ml_service.py

import mlflow
from functools import lru_cache

class MLService:
    """Service de gestion du modÃ¨le ML."""
    
    MODEL_NAME = "battle_winner_model"
    MODEL_VERSION = "Production"
    
    @lru_cache(maxsize=1)
    def load_model(self):
        """Charge le modÃ¨le depuis MLflow Registry (avec cache)."""
        mlflow.set_tracking_uri("http://mlflow:5000")
        
        model_uri = f"models:/{self.MODEL_NAME}/{self.MODEL_VERSION}"
        model = mlflow.pyfunc.load_model(model_uri)
        
        print(f"âœ… ModÃ¨le chargÃ© : {self.MODEL_NAME} ({self.MODEL_VERSION})")
        return model
    
    def predict(self, features: pd.DataFrame) -> dict:
        """Effectue une prÃ©diction."""
        model = self.load_model()
        prediction = model.predict(features)
        probas = model.predict_proba(features)
        
        return {
            "prediction": int(prediction[0]),
            "probabilities": probas[0].tolist()
        }
```

### SÃ©curisation selon OWASP

| Risque OWASP | Mesure appliquÃ©e |
|--------------|------------------|
| **Injection** | Validation Pydantic, requÃªtes paramÃ©trÃ©es |
| **Broken Authentication** | API Key obligatoire |
| **Sensitive Data Exposure** | HTTPS, pas de donnÃ©es sensibles |
| **Security Misconfiguration** | Headers sÃ©curisÃ©s, CORS configurÃ© |
| **Insufficient Logging** | Logs structurÃ©s, monitoring |

---

## 3.2 IntÃ©gration dans l'application Streamlit (C10)

> **C10** : *IntÃ©grer l'API d'un modÃ¨le ou d'un service d'intelligence artificielle dans une application, en respectant les spÃ©cifications du projet et les normes d'accessibilitÃ© en vigueur, Ã  l'aide de la documentation technique de l'API, afin de crÃ©er les fonctionnalitÃ©s d'intelligence artificielle de l'application.*

### Interface utilisateur Streamlit

```python
# interface/pages/battle_predictor.py

import streamlit as st
import requests

st.set_page_config(page_title="PredictionDex - Combat", page_icon="âš”ï¸")

st.title("âš”ï¸ PrÃ©dicteur de Combat PokÃ©mon")
st.markdown("SÃ©lectionnez deux PokÃ©mon pour prÃ©dire le vainqueur !")

# Colonnes pour la sÃ©lection
col1, col2 = st.columns(2)

with col1:
    st.subheader("ğŸ”´ PokÃ©mon 1")
    pokemon_1 = st.selectbox(
        "Choisir le premier PokÃ©mon",
        options=get_pokemon_list(),
        format_func=lambda x: x["name_fr"]
    )
    if pokemon_1:
        st.image(pokemon_1["sprite_url"], width=150)
        display_stats(pokemon_1)

with col2:
    st.subheader("ğŸ”µ PokÃ©mon 2")
    pokemon_2 = st.selectbox(
        "Choisir le second PokÃ©mon",
        options=get_pokemon_list(),
        format_func=lambda x: x["name_fr"]
    )
    if pokemon_2:
        st.image(pokemon_2["sprite_url"], width=150)
        display_stats(pokemon_2)

# Bouton de prÃ©diction
if st.button("âš¡ Lancer le combat !", type="primary"):
    with st.spinner("Analyse en cours..."):
        result = call_prediction_api(pokemon_1["id"], pokemon_2["id"])
        
        if result:
            winner_name = result["winner_name"]
            confidence = result["confidence"] * 100
            
            st.success(f"ğŸ† **{winner_name}** remporte le combat !")
            st.metric("Confiance", f"{confidence:.1f}%")
            
            # Graphique des probabilitÃ©s
            st.bar_chart({
                pokemon_1["name_fr"]: result["pokemon_1_win_probability"],
                pokemon_2["name_fr"]: result["pokemon_2_win_probability"]
            })
```

### Appel Ã  l'API depuis Streamlit

```python
# interface/services/api_client.py

import requests
from typing import Optional
import streamlit as st

API_BASE_URL = "http://api:8000/api/v1"

def call_prediction_api(pokemon_1_id: int, pokemon_2_id: int) -> Optional[dict]:
    """Appelle l'API de prÃ©diction."""
    try:
        response = requests.post(
            f"{API_BASE_URL}/predict/battle",
            json={
                "pokemon_1_id": pokemon_1_id,
                "pokemon_2_id": pokemon_2_id
            },
            headers={"X-API-Key": st.secrets["API_KEY"]},
            timeout=10
        )
        response.raise_for_status()
        return response.json()
    
    except requests.RequestException as e:
        st.error(f"Erreur API : {e}")
        return None
```

### Test d'intÃ©gration

```python
# tests/integration/test_streamlit_api.py

def test_prediction_integration():
    """Test d'intÃ©gration Streamlit -> API -> ModÃ¨le."""
    # Simulation d'un combat Pikachu vs Dracaufeu
    response = requests.post(
        f"{API_URL}/predict/battle",
        json={"pokemon_1_id": 25, "pokemon_2_id": 6},
        headers={"X-API-Key": TEST_API_KEY}
    )
    
    assert response.status_code == 200
    data = response.json()
    
    assert "winner" in data
    assert data["winner"] in [1, 2]
    assert 0 <= data["confidence"] <= 1
    assert "winner_name" in data
```

---

## 3.3 Monitoring du modÃ¨le et dÃ©tection de dÃ©rives (C11)

> **C11** : *Monitorer un modÃ¨le d'intelligence artificielle Ã  partir des mÃ©triques courantes et spÃ©cifiques au projet, en intÃ©grant les outils de collecte, d'alerte et de restitution des donnÃ©es du monitorage pour permettre l'amÃ©lioration du modÃ¨le de faÃ§on itÃ©rative.*

### Architecture du monitoring

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI    â”‚â”€â”€â”€â”€â–¶â”‚  Prometheus  â”‚â”€â”€â”€â”€â–¶â”‚   Grafana    â”‚
â”‚  (mÃ©triques) â”‚     â”‚  (collecte)  â”‚     â”‚  (dashboards)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Drift     â”‚
â”‚  Detection   â”‚
â”‚  (alertes)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### MÃ©triques exposÃ©es

```python
# api_pokemon/monitoring/metrics.py

from prometheus_client import Counter, Histogram, Gauge
import time

# Compteurs de requÃªtes
PREDICTION_REQUESTS = Counter(
    'prediction_requests_total',
    'Nombre total de prÃ©dictions',
    ['endpoint', 'status']
)

# Latence des prÃ©dictions
PREDICTION_LATENCY = Histogram(
    'prediction_latency_seconds',
    'Latence des prÃ©dictions en secondes',
    buckets=[0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0]
)

# Confiance moyenne des prÃ©dictions
PREDICTION_CONFIDENCE = Gauge(
    'prediction_confidence_avg',
    'Confiance moyenne des derniÃ¨res prÃ©dictions'
)

# Distribution des vainqueurs
WINNER_DISTRIBUTION = Counter(
    'prediction_winner_distribution',
    'Distribution des vainqueurs prÃ©dits',
    ['winner']
)

def track_prediction(winner: int, confidence: float, latency: float):
    """Enregistre les mÃ©triques d'une prÃ©diction."""
    PREDICTION_REQUESTS.labels(endpoint='/predict', status='success').inc()
    PREDICTION_LATENCY.observe(latency)
    PREDICTION_CONFIDENCE.set(confidence)
    WINNER_DISTRIBUTION.labels(winner=str(winner)).inc()
```

### DÃ©tection de drift

```python
# api_pokemon/monitoring/drift_detection.py

import numpy as np
from scipy import stats
from typing import Tuple
import json
from datetime import datetime

class DriftDetector:
    """DÃ©tecteur de dÃ©rive des donnÃ©es et du modÃ¨le."""
    
    KS_THRESHOLD = 0.1  # Seuil pour le test de Kolmogorov-Smirnov
    PSI_THRESHOLD = 0.2  # Seuil pour le Population Stability Index
    
    def __init__(self, reference_distribution: np.ndarray):
        self.reference = reference_distribution
        self.alerts = []
    
    def check_data_drift(self, current_data: np.ndarray) -> Tuple[bool, dict]:
        """
        DÃ©tecte une dÃ©rive des donnÃ©es d'entrÃ©e.
        
        Utilise le test de Kolmogorov-Smirnov pour comparer
        la distribution actuelle Ã  la distribution de rÃ©fÃ©rence.
        """
        ks_statistic, p_value = stats.ks_2samp(self.reference, current_data)
        
        is_drift = ks_statistic > self.KS_THRESHOLD
        
        result = {
            "test": "Kolmogorov-Smirnov",
            "statistic": float(ks_statistic),
            "p_value": float(p_value),
            "threshold": self.KS_THRESHOLD,
            "drift_detected": is_drift,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        if is_drift:
            self._raise_alert("DATA_DRIFT", result)
        
        return is_drift, result
    
    def check_prediction_drift(
        self, 
        reference_predictions: np.ndarray,
        current_predictions: np.ndarray
    ) -> Tuple[bool, dict]:
        """
        DÃ©tecte une dÃ©rive dans les prÃ©dictions (concept drift).
        
        Utilise le PSI (Population Stability Index).
        """
        psi = self._calculate_psi(reference_predictions, current_predictions)
        
        is_drift = psi > self.PSI_THRESHOLD
        
        result = {
            "test": "Population Stability Index",
            "psi": float(psi),
            "threshold": self.PSI_THRESHOLD,
            "drift_detected": is_drift,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        if is_drift:
            self._raise_alert("PREDICTION_DRIFT", result)
        
        return is_drift, result
    
    def _calculate_psi(self, expected: np.ndarray, actual: np.ndarray) -> float:
        """Calcule le Population Stability Index."""
        # CrÃ©ation des bins
        bins = np.linspace(0, 1, 11)
        expected_counts = np.histogram(expected, bins=bins)[0] / len(expected)
        actual_counts = np.histogram(actual, bins=bins)[0] / len(actual)
        
        # Ã‰viter division par zÃ©ro
        expected_counts = np.clip(expected_counts, 0.001, None)
        actual_counts = np.clip(actual_counts, 0.001, None)
        
        psi = np.sum((actual_counts - expected_counts) * 
                     np.log(actual_counts / expected_counts))
        return psi
    
    def _raise_alert(self, alert_type: str, details: dict):
        """Enregistre une alerte de drift."""
        alert = {
            "type": alert_type,
            "severity": "WARNING",
            "details": details
        }
        self.alerts.append(alert)
        # En production : envoyer vers Slack, email, PagerDuty, etc.
        print(f"âš ï¸ ALERTE {alert_type}: {json.dumps(details, indent=2)}")
```

### Dashboard Grafana

Le dashboard Grafana affiche :

1. **MÃ©triques temps rÃ©el**
   - Nombre de prÃ©dictions/minute
   - Latence moyenne et P99
   - Taux d'erreur

2. **Performance du modÃ¨le**
   - Distribution des confiances
   - RÃ©partition des vainqueurs prÃ©dits

3. **DÃ©tection de drift**
   - Graphique KS-statistic dans le temps
   - Alertes de dÃ©rive

---

## 3.4 Tests automatisÃ©s du modÃ¨le (C12)

> **C12** : *Programmer les tests automatisÃ©s d'un modÃ¨le d'intelligence artificielle en dÃ©finissant les rÃ¨gles de validation des jeux de donnÃ©es, des Ã©tapes de prÃ©paration des donnÃ©es, d'entraÃ®nement, d'Ã©valuation et de validation du modÃ¨le pour permettre son intÃ©gration en continu et garantir un niveau de qualitÃ© Ã©levÃ©.*

### StratÃ©gie de tests

| Type de test | Objectif | Outils |
|--------------|----------|--------|
| **Unitaires** | Fonctions individuelles | pytest |
| **IntÃ©gration** | API endpoints | pytest + TestClient |
| **ML Pipeline** | QualitÃ© donnÃ©es + modÃ¨le | pytest + Great Expectations |
| **Performance** | Latence, charge | locust |

### Tests de qualitÃ© des donnÃ©es

```python
# tests/ml/test_data_quality.py

import pytest
import pandas as pd
from machine_learning.features.feature_engineering import prepare_features

class TestDataQuality:
    """Tests de validation des donnÃ©es d'entraÃ®nement."""
    
    @pytest.fixture
    def training_data(self):
        """Charge le dataset d'entraÃ®nement."""
        return pd.read_csv("data/datasets/battles_train.csv")
    
    def test_no_missing_values(self, training_data):
        """VÃ©rifie l'absence de valeurs manquantes."""
        assert training_data.isnull().sum().sum() == 0, \
            "Le dataset contient des valeurs manquantes"
    
    def test_target_distribution(self, training_data):
        """VÃ©rifie l'Ã©quilibre des classes."""
        class_counts = training_data['winner'].value_counts()
        ratio = class_counts.min() / class_counts.max()
        
        assert ratio > 0.4, \
            f"Classes dÃ©sÃ©quilibrÃ©es (ratio: {ratio:.2f})"
    
    def test_feature_ranges(self, training_data):
        """VÃ©rifie que les features sont dans les ranges attendus."""
        stat_columns = ['p1_hp', 'p1_attack', 'p1_defense', 'p1_speed',
                        'p2_hp', 'p2_attack', 'p2_defense', 'p2_speed']
        
        for col in stat_columns:
            assert training_data[col].min() >= 0, f"{col} contient des valeurs nÃ©gatives"
            assert training_data[col].max() <= 255, f"{col} dÃ©passe 255"
    
    def test_no_data_leakage(self, training_data):
        """VÃ©rifie l'absence de fuite de donnÃ©es."""
        # Le winner ne doit pas Ãªtre corrÃ©lÃ© Ã  100% avec une feature
        for col in training_data.columns:
            if col != 'winner':
                corr = training_data[col].corr(training_data['winner'])
                assert abs(corr) < 0.99, f"Fuite potentielle : {col} (corr={corr})"
```

### Tests du modÃ¨le ML

```python
# tests/ml/test_model.py

import pytest
import numpy as np
from sklearn.metrics import accuracy_score, f1_score
import mlflow

class TestModelPerformance:
    """Tests de performance du modÃ¨le."""
    
    MINIMUM_ACCURACY = 0.85
    MINIMUM_F1 = 0.83
    
    @pytest.fixture
    def model(self):
        """Charge le modÃ¨le depuis MLflow."""
        mlflow.set_tracking_uri("http://localhost:5000")
        model = mlflow.pyfunc.load_model("models:/battle_winner_model/Production")
        return model
    
    @pytest.fixture
    def test_data(self):
        """Charge les donnÃ©es de test."""
        X_test = pd.read_csv("data/datasets/X_test.csv")
        y_test = pd.read_csv("data/datasets/y_test.csv")
        return X_test, y_test
    
    def test_model_accuracy(self, model, test_data):
        """VÃ©rifie que l'accuracy est au-dessus du seuil."""
        X_test, y_test = test_data
        predictions = model.predict(X_test)
        accuracy = accuracy_score(y_test, predictions)
        
        assert accuracy >= self.MINIMUM_ACCURACY, \
            f"Accuracy insuffisante : {accuracy:.2%} < {self.MINIMUM_ACCURACY:.2%}"
    
    def test_model_f1_score(self, model, test_data):
        """VÃ©rifie le F1-score."""
        X_test, y_test = test_data
        predictions = model.predict(X_test)
        f1 = f1_score(y_test, predictions, average='weighted')
        
        assert f1 >= self.MINIMUM_F1, \
            f"F1-score insuffisant : {f1:.2%} < {self.MINIMUM_F1:.2%}"
    
    def test_prediction_latency(self, model, test_data):
        """VÃ©rifie que les prÃ©dictions sont rapides."""
        X_test, _ = test_data
        single_sample = X_test.iloc[[0]]
        
        import time
        start = time.time()
        for _ in range(100):
            model.predict(single_sample)
        avg_latency = (time.time() - start) / 100
        
        assert avg_latency < 0.1, \
            f"Latence trop Ã©levÃ©e : {avg_latency:.3f}s > 0.1s"
    
    def test_model_reproducibility(self, model, test_data):
        """VÃ©rifie que les prÃ©dictions sont reproductibles."""
        X_test, _ = test_data
        sample = X_test.iloc[:10]
        
        pred1 = model.predict(sample)
        pred2 = model.predict(sample)
        
        assert np.array_equal(pred1, pred2), \
            "Les prÃ©dictions ne sont pas reproductibles"
```

### RÃ©sultats des tests

```
======================== test session starts ========================
platform linux -- Python 3.11.0, pytest-8.0.0
collected 252 items

tests/api/test_health.py ....                                   [  1%]
tests/api/test_pokemon_routes.py ............                   [  6%]
tests/api/test_prediction_routes.py ..........                  [ 10%]
tests/core/test_database.py ........                            [ 13%]
tests/etl/test_pipeline.py ................                     [ 20%]
tests/ml/test_data_quality.py ..........                        [ 24%]
tests/ml/test_model.py ............                             [ 29%]
tests/ml/test_feature_engineering.py ..........                 [ 33%]
tests/monitoring/test_drift_detection.py ........               [ 36%]
tests/integration/test_full_pipeline.py ............            [ 41%]
...

======================== 252 passed in 45.23s ========================

---------- coverage: platform linux, python 3.11.0 -----------
Name                                    Stmts   Miss  Cover
-----------------------------------------------------------
api_pokemon/                             1245    225    82%
core/                                     456     78    83%
etl_pokemon/                              678    134    80%
machine_learning/                         523     89    83%
-----------------------------------------------------------
TOTAL                                    2902    526    82%
```

---

## 3.5 ChaÃ®ne de livraison continue MLOps (C13)

> **C13** : *CrÃ©er une chaÃ®ne de livraison continue d'un modÃ¨le d'intelligence artificielle en installant les outils et en appliquant les configurations souhaitÃ©es, dans le respect du cadre imposÃ© par le projet et dans une approche MLOps, pour automatiser les Ã©tapes de validation, de test, de packaging et de dÃ©ploiement du modÃ¨le.*

### Architecture CI/CD

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Code     â”‚â”€â”€â”€â–¶â”‚     CI      â”‚â”€â”€â”€â–¶â”‚     CD      â”‚â”€â”€â”€â–¶â”‚   Deploy    â”‚
â”‚   (Push)    â”‚    â”‚   (Tests)   â”‚    â”‚   (Build)   â”‚    â”‚  (Staging)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚                  â”‚                  â”‚
                          â–¼                  â–¼                  â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Lint/Type  â”‚    â”‚   Docker    â”‚    â”‚  Production â”‚
                   â”‚  Unit Tests â”‚    â”‚   Images    â”‚    â”‚   (Manual)  â”‚
                   â”‚  ML Tests   â”‚    â”‚   Push      â”‚    â”‚             â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Workflows GitHub Actions

**6 workflows configurÃ©s :**

| Workflow | DÃ©clencheur | Actions |
|----------|-------------|---------|
| `ci.yml` | Push, PR | Lint, tests, coverage |
| `cd.yml` | Merge main | Build Docker, push registry |
| `ml-training.yml` | Manuel/Schedule | EntraÃ®nement modÃ¨le, MLflow |
| `ml-validation.yml` | Nouveau modÃ¨le | Tests ML, validation seuils |
| `deploy-staging.yml` | Merge main | DÃ©ploiement staging |
| `deploy-prod.yml` | Tag release | DÃ©ploiement production |

### Pipeline CI principal

```yaml
# .github/workflows/ci.yml

name: CI Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install ruff mypy
          pip install -r requirements.txt
      
      - name: Lint with Ruff
        run: ruff check .
      
      - name: Type check with MyPy
        run: mypy api_pokemon/ --ignore-missing-imports

  test:
    runs-on: ubuntu-latest
    needs: lint
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: testpass
          POSTGRES_DB: test_db
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: pip install -r requirements.txt -r tests/requirements.txt
      
      - name: Run tests with coverage
        run: |
          pytest tests/ -v --cov=. --cov-report=xml --cov-fail-under=80
        env:
          DATABASE_URL: postgresql://postgres:testpass@localhost:5432/test_db
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage.xml

  ml-tests:
    runs-on: ubuntu-latest
    needs: test
    steps:
      - uses: actions/checkout@v4
      
      - name: Run ML validation tests
        run: pytest tests/ml/ -v --tb=short
```

### Pipeline CD

```yaml
# .github/workflows/cd.yml

name: CD Pipeline

on:
  push:
    branches: [main]

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}
      
      - name: Build and push API image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: docker/Dockerfile.api
          push: true
          tags: |
            ${{ secrets.DOCKER_USERNAME }}/predictiondex-api:latest
            ${{ secrets.DOCKER_USERNAME }}/predictiondex-api:${{ github.sha }}
      
      - name: Build and push ML image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: docker/Dockerfile.ml
          push: true
          tags: ${{ secrets.DOCKER_USERNAME }}/predictiondex-ml:latest

  deploy-staging:
    needs: build-and-push
    runs-on: ubuntu-latest
    environment: staging
    steps:
      - name: Deploy to staging
        run: |
          echo "DÃ©ploiement vers l'environnement staging..."
          # SSH + docker-compose pull + up
```

### MLflow Registry

```python
# machine_learning/mlflow_integration.py

import mlflow
from mlflow.tracking import MlflowClient

def register_model_if_better(run_id: str, model_name: str, metric: str = "accuracy"):
    """
    Enregistre le modÃ¨le dans MLflow Registry si meilleur que le prÃ©cÃ©dent.
    """
    client = MlflowClient()
    
    # RÃ©cupÃ©rer la mÃ©trique du nouveau modÃ¨le
    run = client.get_run(run_id)
    new_metric = run.data.metrics[metric]
    
    # RÃ©cupÃ©rer le modÃ¨le en production actuel
    try:
        prod_version = client.get_latest_versions(model_name, stages=["Production"])[0]
        prod_run = client.get_run(prod_version.run_id)
        prod_metric = prod_run.data.metrics[metric]
    except IndexError:
        prod_metric = 0  # Pas de modÃ¨le en production
    
    # Comparer et promouvoir si meilleur
    if new_metric > prod_metric:
        # Enregistrer le nouveau modÃ¨le
        model_uri = f"runs:/{run_id}/model"
        mv = mlflow.register_model(model_uri, model_name)
        
        # Promouvoir en Production
        client.transition_model_version_stage(
            name=model_name,
            version=mv.version,
            stage="Production",
            archive_existing_versions=True
        )
        
        print(f"âœ… Nouveau modÃ¨le promu en Production (v{mv.version})")
        print(f"   {metric}: {prod_metric:.4f} â†’ {new_metric:.4f}")
        return True
    
    print(f"â„¹ï¸ ModÃ¨le non promu ({metric}: {new_metric:.4f} <= {prod_metric:.4f})")
    return False
```

---

\newpage

---

# 4. DÃ©monstration du Projet

> **Obligatoire pour E3** â€” Cette section dÃ©crit le scÃ©nario de dÃ©monstration Ã  rÃ©aliser lors de la soutenance.

## ScÃ©nario de dÃ©monstration (5-10 minutes)

### Ã‰tape 1 : Lancement de l'infrastructure (30s)

```bash
# DÃ©marrage de tous les services
docker-compose up -d

# VÃ©rification de la santÃ© des services
docker-compose ps
```

### Ã‰tape 2 : Interface utilisateur Streamlit (2 min)

1. Ouvrir l'interface : `http://localhost:8501`
2. Naviguer vers la page "PrÃ©dicteur de Combat"
3. SÃ©lectionner **Pikachu** (ID: 25) vs **Dracaufeu** (ID: 6)
4. Lancer la prÃ©diction
5. Montrer le rÃ©sultat avec les probabilitÃ©s

### Ã‰tape 3 : API REST (1 min)

1. Ouvrir Swagger UI : `http://localhost:8000/docs`
2. Tester l'endpoint `/api/v1/pokemon` (liste)
3. Tester l'endpoint `/api/v1/predict/battle` avec les mÃªmes PokÃ©mon

### Ã‰tape 4 : Monitoring Grafana (2 min)

1. Ouvrir Grafana : `http://localhost:3000`
2. Afficher le dashboard "PredictionDex Monitoring"
3. Montrer les mÃ©triques temps rÃ©el aprÃ¨s les prÃ©dictions
4. Expliquer les seuils d'alerte configurÃ©s

### Ã‰tape 5 : MLflow (1 min)

1. Ouvrir MLflow : `http://localhost:5001`
2. Montrer les expÃ©riences d'entraÃ®nement
3. Montrer le Model Registry avec les versions

### Ã‰tape 6 : CI/CD GitHub (1 min)

1. Montrer le repository GitHub
2. Afficher les workflows rÃ©cents
3. Montrer un exemple de pipeline rÃ©ussi

---

\newpage

---

# 5. SynthÃ¨se et Perspectives

## 5.1 Bilan technique

### Objectifs atteints

| Objectif | Statut | DÃ©tail |
|----------|--------|--------|
| Accuracy â‰¥ 85% | âœ… | 88.23% (v2) |
| Pipeline ETL automatisÃ© | âœ… | Docker + scripts Python |
| API REST documentÃ©e | âœ… | FastAPI + Swagger |
| Monitoring temps rÃ©el | âœ… | Prometheus + Grafana |
| CI/CD fonctionnel | âœ… | 6 workflows GitHub Actions |
| Tests automatisÃ©s | âœ… | 252 tests, 82% coverage |
| DÃ©tection de drift | âœ… | KS-test, PSI |

### MÃ©triques clÃ©s du projet

| MÃ©trique | Valeur |
|----------|--------|
| **Lignes de code** | ~5000 |
| **Services Docker** | 9 |
| **Endpoints API** | 15+ |
| **Tests automatisÃ©s** | 252 |
| **Couverture de code** | 82% |
| **Workflows CI/CD** | 6 |
| **Accuracy modÃ¨le** | 88.23% |

---

## 5.2 DifficultÃ©s rencontrÃ©es et solutions

| ProblÃ¨me | Impact | Solution appliquÃ©e |
|----------|--------|-------------------|
| **DonnÃ©es manquantes PokÃ©API** | Noms FR absents | Ajout scraping Pokepedia |
| **Performance modÃ¨le v1** | Contexte trop simple | Feature engineering v2 avec types |
| **Latence prÃ©dictions** | UX dÃ©gradÃ©e | Cache modÃ¨le avec `lru_cache` |
| **DÃ©sÃ©quilibre classes** | Biais du modÃ¨le | Stratified split + class_weight |
| **Tests flaky** | CI instable | Fixtures isolÃ©es, mocks |

---

## 5.3 Axes d'amÃ©lioration

### Court terme (< 3 mois)

- [ ] Ajouter l'explicabilitÃ© des prÃ©dictions (SHAP values)
- [ ] ImplÃ©menter un systÃ¨me de feedback utilisateur
- [ ] Ajouter plus de scÃ©narios de combat (mÃ©tÃ©o, terrain)

### Moyen terme (3-6 mois)

- [ ] A/B testing des versions de modÃ¨le
- [ ] DÃ©ploiement sur Kubernetes
- [ ] Ajout d'une API GraphQL

### Long terme (> 6 mois)

- [ ] Extension Ã  d'autres gÃ©nÃ©rations de PokÃ©mon
- [ ] ModÃ¨le de recommandation d'Ã©quipe
- [ ] Application mobile (React Native)

---

## 5.4 Conclusion

Le projet **PredictionDex** dÃ©montre la maÃ®trise complÃ¨te du cycle de vie d'un projet d'intelligence artificielle, de la collecte des donnÃ©es jusqu'au dÃ©ploiement et monitoring en production.

**Points forts du projet :**

- âœ… Architecture MLOps complÃ¨te et professionnelle
- âœ… Pipeline de donnÃ©es robuste et reproductible
- âœ… API REST sÃ©curisÃ©e et documentÃ©e
- âœ… Monitoring proactif avec dÃ©tection de dÃ©rives
- âœ… CI/CD entiÃ¨rement automatisÃ©
- âœ… Tests exhaustifs garantissant la qualitÃ©

Ce projet illustre les compÃ©tences attendues pour les blocs **E1** (C1-C5) et **E3** (C9-C13) de la certification RNCP "DÃ©veloppeur en Intelligence Artificielle".

---

\newpage

---

# 6. Annexes

## Annexe A : SchÃ©ma MCD/MPD

*[InsÃ©rer le diagramme entitÃ©-relation complet]*

---

## Annexe B : Architecture technique (9 services Docker)

### B.1 SchÃ©ma global de l'architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           PREDICTIONDEX ARCHITECTURE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Streamlitâ”‚   â”‚ FastAPI  â”‚   â”‚  MLflow  â”‚   â”‚PostgreSQLâ”‚   â”‚  pgAdmin â”‚  â”‚
â”‚  â”‚  :8501   â”‚â”€â”€â–¶â”‚  :8000   â”‚â”€â”€â–¶â”‚  :5001   â”‚   â”‚  :5432   â”‚   â”‚  :5050   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                      â”‚                             â”‚                        â”‚
â”‚                      â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                      â”‚         â”‚                                            â”‚
â”‚                      â–¼         â–¼                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚  â”‚   ETL    â”‚â”€â”€â–¶â”‚    ML    â”‚â”€â”€â–¶â”‚  Models  â”‚                                â”‚
â”‚  â”‚ Pipeline â”‚   â”‚ Training â”‚   â”‚  (data/) â”‚                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚Prometheusâ”‚â—€â”€â”€â”‚  Grafana â”‚                                               â”‚
â”‚  â”‚  :9090   â”‚   â”‚  :3000   â”‚                                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### B.2 DÃ©tail des 9 services Docker

| Service | Image | Port | RÃ´le | DÃ©pendances |
|---------|-------|------|------|-------------|
| **db** | `postgres:15` | 5432 | Base de donnÃ©es principale | - |
| **api** | `predictiondex-api` | 8000 | API REST (donnÃ©es + ML) | db, mlflow |
| **etl** | `predictiondex-etl` | - | Pipeline de collecte | db |
| **ml** | `predictiondex-ml` | - | EntraÃ®nement des modÃ¨les | db, mlflow |
| **mlflow** | `predictiondex-mlflow` | 5001 | Tracking & Model Registry | db |
| **streamlit** | `predictiondex-streamlit` | 8501 | Interface utilisateur | api |
| **prometheus** | `prom/prometheus:v2.47` | 9090 | Collecte de mÃ©triques | api |
| **grafana** | `grafana/grafana:10.1` | 3000 | Dashboards de monitoring | prometheus |
| **pgadmin** | `dpage/pgadmin4` | 5050 | Administration PostgreSQL | db |

### B.3 Flux de donnÃ©es entre services

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FLUX DE DONNÃ‰ES                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                    â”‚
â”‚  â‘  COLLECTE (ETL)                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ PokÃ©API  â”‚â”€â”€â”€â”€â–¶â”‚   ETL    â”‚â”€â”€â”€â”€â–¶â”‚ PostgreSQLâ”‚              â”‚
â”‚  â”‚Pokepedia â”‚     â”‚          â”‚     â”‚  (tables) â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜              â”‚
â”‚                                        â”‚                          â”‚
â”‚  â‘¡ ENTRAÃNEMENT (ML)                   â–¼                          â”‚
â”‚                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                                 â”‚    ML    â”‚â”€â”€â”€â”€â–¶â”‚  MLflow  â”‚  â”‚
â”‚                                 â”‚ Training â”‚     â”‚ (models) â”‚  â”‚
â”‚                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â”‚
â”‚                                                    â”‚            â”‚
â”‚  â‘¢ PRÃ‰DICTION (API)                               â–¼            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚Streamlit â”‚â”€â”€â”€â”€â–¶â”‚ FastAPI  â”‚â”€â”€â”€â”€â–¶â”‚  XGBoost â”‚              â”‚
â”‚  â”‚ (user)   â”‚     â”‚ /predict â”‚     â”‚  (model) â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                          â”‚                                      â”‚
â”‚  â‘£ MONITORING                 â–¼                                      â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚                    â”‚Prometheusâ”‚â”€â”€â”€â”€â–¶â”‚  Grafana â”‚              â”‚
â”‚                    â”‚ (metrics)â”‚     â”‚(dashboard)â”‚              â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### B.4 Arborescence du projet

```
predictiondex/
â”œâ”€â”€ ğŸ“ api_pokemon/              # API REST FastAPI
â”‚   â”œâ”€â”€ main.py                   # Point d'entrÃ©e de l'API
â”‚   â”œâ”€â”€ config.py                 # Configuration
â”‚   â”œâ”€â”€ routes/                   # Endpoints
â”‚   â”‚   â”œâ”€â”€ pokemon.py            # Routes PokÃ©mon
â”‚   â”‚   â”œâ”€â”€ predictions.py        # Routes ML
â”‚   â”‚   â””â”€â”€ health.py             # Health checks
â”‚   â”œâ”€â”€ services/                 # Logique mÃ©tier
â”‚   â”œâ”€â”€ middleware/               # SÃ©curitÃ©, CORS
â”‚   â””â”€â”€ monitoring/               # MÃ©triques, drift
â”‚
â”œâ”€â”€ ğŸ“ core/                      # Code partagÃ©
â”‚   â”œâ”€â”€ db/                       # Connexion BDD
â”‚   â”œâ”€â”€ models/                   # ModÃ¨les SQLAlchemy
â”‚   â””â”€â”€ schemas/                  # SchÃ©mas Pydantic
â”‚
â”œâ”€â”€ ğŸ“ etl_pokemon/               # Pipeline ETL
â”‚   â”œâ”€â”€ pipeline.py               # Orchestration ETL
â”‚   â”œâ”€â”€ pokepedia_scraper/        # Scraping web
â”‚   â””â”€â”€ utils/                    # Nettoyage donnÃ©es
â”‚
â”œâ”€â”€ ğŸ“ machine_learning/          # Module ML
â”‚   â”œâ”€â”€ train_model.py            # EntraÃ®nement
â”‚   â”œâ”€â”€ evaluation.py             # MÃ©triques
â”‚   â”œâ”€â”€ features/                 # Feature engineering
â”‚   â””â”€â”€ mlflow_integration.py     # Tracking MLflow
â”‚
â”œâ”€â”€ ğŸ“ interface/                 # Frontend Streamlit
â”‚   â”œâ”€â”€ app.py                    # Application principale
â”‚   â”œâ”€â”€ pages/                    # Pages multi-pages
â”‚   â””â”€â”€ services/                 # Appels API
â”‚
â”œâ”€â”€ ğŸ“ docker/                    # Configuration Docker
â”‚   â”œâ”€â”€ Dockerfile.api
â”‚   â”œâ”€â”€ Dockerfile.etl
â”‚   â”œâ”€â”€ Dockerfile.ml
â”‚   â”œâ”€â”€ Dockerfile.streamlit
â”‚   â”œâ”€â”€ grafana/                  # Config Grafana
â”‚   â””â”€â”€ prometheus/               # Config Prometheus
â”‚
â”œâ”€â”€ ğŸ“ tests/                     # Tests automatisÃ©s
â”‚   â”œâ”€â”€ api/                      # Tests API
â”‚   â”œâ”€â”€ ml/                       # Tests ML
â”‚   â”œâ”€â”€ integration/              # Tests intÃ©gration
â”‚   â””â”€â”€ conftest.py               # Fixtures pytest
â”‚
â”œâ”€â”€ ğŸ“ data/                      # DonnÃ©es
â”‚   â”œâ”€â”€ datasets/                 # CSV sources
â”‚   â””â”€â”€ ml/                       # ModÃ¨les exportÃ©s
â”‚
â”œâ”€â”€ ğŸ“ docs/                      # Documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â”œâ”€â”€ GUIDE_RAPPORT_E1_E3.md
â”‚   â””â”€â”€ RAPPORT_E1_E3_TEMPLATE.md
â”‚
â”œâ”€â”€ ğŸ“„ docker-compose.yml         # Orchestration
â”œâ”€â”€ ğŸ“„ pytest.ini                 # Config tests
â””â”€â”€ ğŸ“„ README.md                  # Documentation projet
```

---

## Annexe C : MÃ©triques du modÃ¨le v2

### Matrice de confusion

```
              PrÃ©diction
              1      2
RÃ©el   1   [856]   [112]
       2   [124]   [908]

Accuracy:  88.23%
Precision: 87.5%
Recall:    88.9%
F1-Score:  88.2%
```

### Courbe ROC

*[InsÃ©rer la courbe ROC]*

AUC = 0.94

---

## Annexe D : Captures d'Ã©cran dÃ©taillÃ©es

> **Instructions** : Remplacer chaque placeholder par une vraie capture d'Ã©cran annotÃ©e.

---

### D.1 Swagger UI (API Documentation)

**Ã€ capturer :** Page `/docs` de FastAPI

**Ã‰lÃ©ments Ã  montrer :**
- Liste complÃ¨te des endpoints
- DÃ©tail d'un endpoint `/api/v1/predict/battle`
- Exemple de requÃªte/rÃ©ponse

*[InsÃ©rer capture : swagger_ui.png]*

---

### D.2 Dashboard Grafana - Vue d'ensemble

**Ã€ capturer :** Dashboard principal `http://localhost:3000`

**Ã‰lÃ©ments Ã  montrer :**
1. **Panneau "Predictions/minute"** â€” Graphique temps rÃ©el
2. **Panneau "Latence API"** â€” Histogramme P50/P99
3. **Panneau "Accuracy en production"** â€” Gauge
4. **Panneau "Alertes actives"** â€” Liste des alertes

*[InsÃ©rer capture : grafana_dashboard_overview.png]*

---

### D.3 Grafana - MÃ©triques de latence

**Ã€ capturer :** Panneau de latence zoomÃ©

**Ã‰lÃ©ments Ã  montrer :**
- Courbe de latence sur 1 heure
- Seuil d'alerte (ligne rouge Ã  500ms)
- PÃ©riode de pic si visible

*[InsÃ©rer capture : grafana_latency.png]*

---

### D.4 Grafana - DÃ©tection de Drift

**Ã€ capturer :** Panneau de monitoring du drift

**Ã‰lÃ©ments Ã  montrer :**
- Graphique KS-statistic dans le temps
- Seuil de dÃ©clenchement (0.1)
- Ã‰volution du PSI

*[InsÃ©rer capture : grafana_drift.png]*

---

### D.5 MLflow - Liste des expÃ©riences

**Ã€ capturer :** Page d'accueil MLflow `http://localhost:5001`

**Ã‰lÃ©ments Ã  montrer :**
- Liste des expÃ©riences (battle_winner_v1, battle_winner_v2)
- Nombre de runs par expÃ©rience
- Date de derniÃ¨re modification

*[InsÃ©rer capture : mlflow_experiments.png]*

---

### D.6 MLflow - Comparaison de runs

**Ã€ capturer :** Vue comparaison de 2+ runs

**Ã‰lÃ©ments Ã  montrer :**
- Tableau comparatif des mÃ©triques (accuracy, F1, precision)
- DiffÃ©rence entre v1 et v2
- ParamÃ¨tres utilisÃ©s (n_estimators, max_depth, etc.)

*[InsÃ©rer capture : mlflow_compare_runs.png]*

---

### D.7 MLflow - Model Registry

**Ã€ capturer :** Page Models du Registry

**Ã‰lÃ©ments Ã  montrer :**
- ModÃ¨le `battle_winner_model`
- Versions (v1, v2)
- Stage de chaque version (Production, Archived)
- Lien vers le run source

*[InsÃ©rer capture : mlflow_model_registry.png]*

---

### D.8 MLflow - DÃ©tail d'un run

**Ã€ capturer :** Page dÃ©tail d'un run (le meilleur)

**Ã‰lÃ©ments Ã  montrer :**
- ParamÃ¨tres du modÃ¨le
- MÃ©triques finales
- Artifacts (modÃ¨le, requirements.txt)
- Tags

*[InsÃ©rer capture : mlflow_run_detail.png]*

---

### D.9 Interface Streamlit - SÃ©lection des PokÃ©mon

**Ã€ capturer :** Page de prÃ©diction de combat

**Ã‰lÃ©ments Ã  montrer :**
- SÃ©lecteurs de PokÃ©mon (avec sprites)
- Statistiques affichÃ©es
- Bouton "Lancer le combat"

*[InsÃ©rer capture : streamlit_selection.png]*

---

### D.10 Interface Streamlit - RÃ©sultat de prÃ©diction

**Ã€ capturer :** AprÃ¨s une prÃ©diction

**Ã‰lÃ©ments Ã  montrer :**
- Nom du vainqueur
- Pourcentage de confiance
- Graphique des probabilitÃ©s

*[InsÃ©rer capture : streamlit_prediction.png]*

---

### D.11 GitHub Actions - Workflows

**Ã€ capturer :** Page Actions du repository

**Ã‰lÃ©ments Ã  montrer :**
- Liste des workflows (ci.yml, cd.yml, etc.)
- Historique des runs rÃ©cents
- Au moins un âœ… (succÃ¨s)

*[InsÃ©rer capture : github_actions_list.png]*

---

### D.12 GitHub Actions - DÃ©tail d'un workflow

**Ã€ capturer :** DÃ©tail d'un run CI rÃ©ussi

**Ã‰lÃ©ments Ã  montrer :**
- Ã‰tapes du job (lint, test, ml-tests)
- DurÃ©e totale
- Logs d'une Ã©tape (tests)

*[InsÃ©rer capture : github_actions_detail.png]*

---

### D.13 pgAdmin - Tables peuplÃ©es

**Ã€ capturer :** Vue des tables dans pgAdmin

**Ã‰lÃ©ments Ã  montrer :**
- Arborescence des tables (pokemon, types, moves, battles)
- Exemple de donnÃ©es (SELECT * FROM pokemon LIMIT 10)

*[InsÃ©rer capture : pgadmin_tables.png]*

---

### D.14 Terminal - Docker Compose

**Ã€ capturer :** `docker-compose ps`

**Ã‰lÃ©ments Ã  montrer :**
- Tous les services en statut "Up"
- Ports mappÃ©s

```
$ docker-compose ps
NAME                  STATUS    PORTS
predictiondex-api     Up        0.0.0.0:8000->8000/tcp
predictiondex-db      Up        0.0.0.0:5432->5432/tcp
predictiondex-grafana Up        0.0.0.0:3000->3000/tcp
...
```

*[InsÃ©rer capture : docker_compose_ps.png]*

---

## Annexe E : Pipelines CI/CD (schÃ©ma)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        GITHUB ACTIONS WORKFLOWS                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Push/PR â”€â”€â”€â”€â–¶ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”€â”€â”€â”€â–¶ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”€â”€â”€â”€â–¶ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚                â”‚  Lint  â”‚       â”‚  Test  â”‚       â”‚ ML Tests   â”‚        â”‚
â”‚                â”‚  Ruff  â”‚       â”‚ pytest â”‚       â”‚ Validation â”‚        â”‚
â”‚                â”‚  MyPy  â”‚       â”‚ 82%cov â”‚       â”‚            â”‚        â”‚
â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                       â”‚                                 â”‚
â”‚                                       â–¼                                 â”‚
â”‚  Merge main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”€â”€â”€â”€â–¶ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                                 â”‚ Build/Push â”‚       â”‚  Deploy    â”‚   â”‚
â”‚                                 â”‚  Docker    â”‚       â”‚  Staging   â”‚   â”‚
â”‚                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚          â”‚
â”‚  Tag release â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â–¶  â”‚
â”‚                                                             â”‚          â”‚
â”‚                                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                                                      â”‚  Deploy    â”‚   â”‚
â”‚                                                      â”‚ Production â”‚   â”‚
â”‚                                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Annexe F : Glossaire technique

| Terme | DÃ©finition |
|-------|------------|
| **API REST** | Interface de programmation utilisant le protocole HTTP |
| **CI/CD** | Continuous Integration / Continuous Deployment |
| **Docker** | Plateforme de conteneurisation d'applications |
| **Drift** | DÃ©rive des donnÃ©es ou du modÃ¨le dans le temps |
| **ETL** | Extract, Transform, Load â€” Pipeline de donnÃ©es |
| **FastAPI** | Framework Python pour crÃ©er des APIs REST performantes |
| **Grafana** | Outil de visualisation de mÃ©triques |
| **KS-test** | Test de Kolmogorov-Smirnov pour comparer des distributions |
| **MLflow** | Plateforme open-source pour le cycle de vie ML |
| **MLOps** | Machine Learning Operations â€” DevOps pour le ML |
| **PostgreSQL** | SystÃ¨me de gestion de base de donnÃ©es relationnelle |
| **Prometheus** | SystÃ¨me de monitoring et d'alerting |
| **PSI** | Population Stability Index â€” MÃ©trique de drift |
| **Streamlit** | Framework Python pour crÃ©er des applications web |
| **XGBoost** | Algorithme de gradient boosting pour classification/rÃ©gression |

---

## Annexe G : RÃ©fÃ©rences

### Documentation officielle

- FastAPI : https://fastapi.tiangolo.com/
- MLflow : https://mlflow.org/docs/latest/
- XGBoost : https://xgboost.readthedocs.io/
- Prometheus : https://prometheus.io/docs/
- Grafana : https://grafana.com/docs/

### Sources de donnÃ©es

- PokÃ©API : https://pokeapi.co/
- Pokepedia : https://www.pokepedia.fr/

### Repository du projet

- GitHub : [InsÃ©rer l'URL du repository]

---

*Rapport gÃ©nÃ©rÃ© le 31 janvier 2026*

*Template basÃ© sur le guide GUIDE_RAPPORT_E1_E3.md*
